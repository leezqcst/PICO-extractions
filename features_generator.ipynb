{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Generator\n",
    "\n",
    "Features:\n",
    "\n",
    "   1. Features based on Genia Tagger:\n",
    "   \n",
    "       1. Inside parentheses or not\n",
    "       2. POS tag\n",
    "       3. The type of phrase\n",
    "       4. IOB tag\n",
    "       5. Whether it is the last token of phrase\n",
    "       6. Named entity\n",
    "       7. 1-6 above but with neighbors\n",
    "       8. Whether neighbor is in the same phrase as the token in question\n",
    "       \n",
    "   2. Features based on the word:\n",
    "   \n",
    "       1. One-hot encoding\n",
    "       2. Word2vec (http://bio.nlplab.org/#word-vectors)\n",
    "       3. Is all letters uppercase?\n",
    "       4. Is first letter uppercasea and the rest lowercase?\n",
    "       5. 1-4 above but with neighbors\n",
    "\n",
    "Additional features not yet implemented:\n",
    "\n",
    "1. Semantic tags tagged manually for words include people or measurements\n",
    "    1. People: *people, participants, subjects, men, women, children, patient*  \n",
    "    2. Meaurement: *length, volumen, weight, etc.\n",
    "    \n",
    "2. Semantic tags from Word-Net (https://wordnet.princeton.edu/)\n",
    "\n",
    "3. UMLS semantic type? (https://semanticnetwork.nlm.nih.gov/)\n",
    "\n",
    "4. Prefixes or suffixes?\n",
    "\n",
    "source: Automatic summarization of results from clinical trials; An Introduction to Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, pickle, sys\n",
    "from collections import defaultdict\n",
    "from preprocess_data import get_all_data_train, get_all_data_dev, get_all_data_test\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Directory for annotations\n",
    "directory = 'PICO-annotations/batch5k'\n",
    "\n",
    "# Suffixes for the generated files\n",
    "tokens_suffix = '_tokens.txt'\n",
    "genia_tags_suffix = '_genia.tag'\n",
    "\n",
    "# Names of word2vec models\n",
    "pubmed_w2v_name = 'PubMed-w2v.bin'\n",
    "pubmed_wiki_w2v_name = 'wikipedia-pubmed-and-PMC-w2v.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pubmed word2vec model (1-2 min)\n",
    "# pubmed_w2v = Word2Vec.load_word2vec_format(pubmed_w2v_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pubmed_wiki word2vec model (2-3 min)\n",
    "# pubmed_wiki_w2v = Word2Vec.load_word2vec_format(pubmed_wiki_w2v_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print elements of a list with spaces\n",
    "# Element l[i] is padded to have length space[i]\n",
    "def print_with_spaces(l, spaces):\n",
    "    # This pads strings to be of space length and aligned left\n",
    "    formatter = lambda space: '{:' + str(space) + '}'\n",
    "    \n",
    "    print ''.join([formatter(space).format(string) for string, space in zip(l, spaces)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Genia Tags\n",
    "Returns a list of lists of genia tags.  \n",
    "Inner list is per abstract. Inner list has tuples of the genia  \n",
    "tags per token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_genia_tags(data_set):\n",
    "    switcher = {\n",
    "        'train': 'PICO-annotations/train_abstracts.txt',\n",
    "        'dev': 'PICO-annotations/dev_abstracts.txt',\n",
    "        'test': 'PICO-annotations/test_abstracts.txt', \n",
    "    }\n",
    "    path = switcher[data_set];\n",
    "    abstract_file = open(path, 'r')\n",
    "    abstracts = abstract_file.readlines()\n",
    "    abstracts = [x.strip() for x in abstracts]\n",
    "    \n",
    "    genia_tags = []\n",
    "    \n",
    "    for abstract_path in abstracts:\n",
    "        pickle_path = abstract_path[:-4] + genia_tags_suffix\n",
    "        pickle_file = open(pickle_path, 'rb')\n",
    "        abstract_genia_tags = pickle.load(pickle_file)\n",
    "        \n",
    "        genia_tags.append(abstract_genia_tags)\n",
    "    return genia_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up genia tags\n",
    "\n",
    "_INPUT_: output of genia parser  \n",
    "Format: list of (word, base_form, pos, chunk, named_entity)\n",
    "\n",
    "_OUTPUT_: cleaned tags  \n",
    "Format: list of dictionaries with keys 'inside_paren', 'pos', 'chunk', 'iob', 'named_entity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def clean_tags(genia_tags):\n",
    "    cleaned_tags = []\n",
    "\n",
    "    # Keep track of whether word is inside parantheses\n",
    "    inside_paren = False\n",
    "\n",
    "    for word, base_form, pos, chunk, named_entity in genia_tags:\n",
    "        word_tags = dict()\n",
    "        \n",
    "        # Update parentheses\n",
    "        if pos == '(':\n",
    "            inside_paren = True\n",
    "        elif pos == ')':\n",
    "            inside_paren = False\n",
    "        \n",
    "        # Key: inside_paren\n",
    "        if pos == '(':\n",
    "            # '(' itself is not inside parentheses\n",
    "            word_tags['inside_paren'] = False\n",
    "        else:\n",
    "            word_tags['inside_paren'] = inside_paren\n",
    "        \n",
    "        # Key: POS\n",
    "        word_tags['pos'] = pos\n",
    "        \n",
    "        # Key: chunk\n",
    "        # Strip out IOB\n",
    "        if chunk == 'O':\n",
    "            word_tags['chunk'] = chunk\n",
    "        elif len(chunk) > 2:\n",
    "            word_tags['chunk'] = chunk[2:]\n",
    "        else:\n",
    "            raise ValueError('Unidentified chunk: ' + chunk)\n",
    "        \n",
    "        # Key: IOB\n",
    "        iob = chunk[0]\n",
    "        if iob != 'O' and iob != 'I' and iob != 'B':\n",
    "            raise ValueError('Unidentified chunk: ' + chunk)\n",
    "        word_tags['iob'] = iob\n",
    "        \n",
    "        # Key: named_entity\n",
    "        # Strip out IOB\n",
    "        if named_entity == 'O':\n",
    "            word_tags['named_entity'] = named_entity\n",
    "        elif len(named_entity) > 2:\n",
    "            word_tags['named_entity'] = named_entity[2:]\n",
    "        else:\n",
    "            raise ValueError('Unidentified named entity: ' + named_entity)\n",
    "        \n",
    "        cleaned_tags.append(word_tags)\n",
    "\n",
    "    return cleaned_tags\n",
    "\n",
    "if DEBUG:\n",
    "    f = open(directory + '/0b0153dd6caa41f79fdee74a09a9ba6e/24534270_genia.tag')\n",
    "    genia_tags = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    cleaned_tags = clean_tags(genia_tags)\n",
    "    \n",
    "    for word_tags in genia_tags:\n",
    "        print_with_spaces(word_tags, [20, 20, 5, 10, 5])\n",
    "    for word_tags in cleaned_tags:\n",
    "        tag_list = [str(word_tags['inside_paren']), word_tags['pos'], word_tags['chunk'], \n",
    "        word_tags['iob'], word_tags['named_entity']]\n",
    "        \n",
    "        print_with_spaces(tag_list, [10, 5, 5, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get options dictionary\n",
    "Transform options string into options dictionary\n",
    "\n",
    "Format: 'left_neighbors=3 right_neighbors=3 inside_paren w2v_model=wiki'  \n",
    "becomes {'inside_paren': True, 'w2v_model': 'wiki', 'left_neighbors': 3, 'right_neighbors': 3}\n",
    "\n",
    "Transform value from string to integer or boolean if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    " \n",
    "def get_options_dict(options_string):\n",
    "    # This means that any nonexisting key is mapped to False\n",
    "    options_dict = defaultdict(bool)\n",
    "    \n",
    "    options = options_string.split()\n",
    "    \n",
    "    for option in options:\n",
    "        if '=' in option:\n",
    "            name, value = option.split('=')\n",
    "            \n",
    "            # Transform into integer or boolean if possible\n",
    "            try:\n",
    "                value = int(value)\n",
    "            except:\n",
    "                if value == 'True':\n",
    "                    value = True\n",
    "                elif value == 'False':\n",
    "                    value = False\n",
    "        else:\n",
    "            name = option\n",
    "            value = True\n",
    "            \n",
    "        if name in options_dict.keys():\n",
    "            raise ValueError('Option ' + name + ' appears more than once')\n",
    "            \n",
    "        options_dict[name] = value\n",
    "    \n",
    "    return options_dict\n",
    "\n",
    "if DEBUG:\n",
    "    options_string = 'left_neighbors=3 right_neighbors=3 inside_paren w2v_model=wiki'\n",
    "    options_dict = get_options_dict(options_string)\n",
    "    print options_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get genia features\n",
    "for word at index word_index according to cleaned_tags, and update feature_dict\n",
    "\n",
    "_INPUT_:\n",
    "- word_index: index of the word that we want to extract features\n",
    "- cleaned_tags: the list of tuples from clean_tags\n",
    "- feature_dict: the dictionary that we want to store all the features for the word\n",
    "\n",
    "Relevant options:\n",
    "- left_neighbors=?, right_neighbors=?: number of left and right neighbors\n",
    "- inside_paren, pos, chunk, iob, named_entity: whether to use these features from cleaned tags\n",
    "- chunk_end: whether word is the last in its chunk\n",
    "- same_chunk: whether neighbor is in the same chunk as original word\n",
    "- concatenate these features with '_neighbors' to use them on neighbors\n",
    "\n",
    "_OUTPUT_:\n",
    "- feature_dict\n",
    "\n",
    "source: https://github.com/bwallace/Deep-PICO/blob/master/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def tags2genia_features(word_index, cleaned_tags, feature_dict, options_dict):\n",
    "    # Extract number of left and right neighbors\n",
    "    left_neighbors = options_dict['left_neighbors']\n",
    "    right_neighbors = options_dict['right_neighbors']\n",
    "    \n",
    "    # Now iterate over all possible offsets\n",
    "    for offset in range(-left_neighbors, right_neighbors+1):\n",
    "        # Index of word that we are extracting features\n",
    "        new_index = word_index + offset\n",
    "        \n",
    "        # Out of bound\n",
    "        if new_index < 0 or new_index >= len(cleaned_tags):\n",
    "            continue\n",
    "        \n",
    "        # Determine whether to include feature into dictionary\n",
    "        # If offset = 0, check using feature\n",
    "        # Otherwise, check using feature + '_neighbors'\n",
    "        def use(feature):\n",
    "            if offset == 0:\n",
    "                return options_dict[feature]\n",
    "            else:\n",
    "                return options_dict[feature + '_neighbors']\n",
    "        \n",
    "        # Get all the tags\n",
    "        word_tags = cleaned_tags[new_index]\n",
    "   \n",
    "        # Features: features from cleaned tags\n",
    "        for feature in ['inside_paren', 'pos', 'chunk', 'iob', 'named_entity']:\n",
    "            if use(feature):\n",
    "                feature_dict['{}[{}]'.format(feature, offset)] = word_tags[feature]\n",
    "        \n",
    "        # Features: whether word is the last in its chunk\n",
    "        if use('chunk_end'):\n",
    "            iob = word_tags['iob']\n",
    "            \n",
    "            # Check that IOB is I or B and the next IOB is not I\n",
    "            if (iob == 'I' or iob == 'B') and \\\n",
    "            (new_index == len(cleaned_tags)-1 or cleaned_tags[new_index+1]['iob'] != 'I'):\n",
    "                feature_dict['chunk_end[{}]'.format(offset)] = True\n",
    "            else:\n",
    "                feature_dict['chunk_end[{}]'.format(offset)] = False\n",
    "        \n",
    "        # Features: whether neighbor is in the same chunk as original word\n",
    "        if use('same_chunk'):\n",
    "            if offset == 0:\n",
    "                raise ValueError('Should not use same_chunk with the same word')\n",
    "                \n",
    "            start = min(word_index, new_index)\n",
    "            end = max(word_index, new_index)\n",
    "\n",
    "            feature_dict['same_chunk[{}]'.format(offset)] = True\n",
    "            \n",
    "            # Check that IOBs from start+1 to end are all I's\n",
    "            for b in range(start+1, end+1):\n",
    "                if cleaned_tags[b]['iob'] != 'I':\n",
    "                    feature_dict['same_chunk[{}]'.format(offset)] = False\n",
    "       \n",
    "    return feature_dict\n",
    "\n",
    "if DEBUG:\n",
    "    f = open(directory + '/0b0153dd6caa41f79fdee74a09a9ba6e/24534270_tokens.txt')\n",
    "    tokens = f.read().split()\n",
    "    f.close()\n",
    "    \n",
    "    f = open(directory + '/0b0153dd6caa41f79fdee74a09a9ba6e/24534270_genia.tag')\n",
    "    genia_tags = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    cleaned_tags = clean_tags(genia_tags)\n",
    "    \n",
    "    options_string = 'left_neighbors=3 right_neighbors=3 inside_paren pos chunk iob named_entity \\\n",
    "    inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "    chunk_end chunk_end_neighbors same_chunk_neighbors'\n",
    "    options_dict = get_options_dict(options_string)\n",
    "    \n",
    "    feature_dict ={}\n",
    "    feature_dict = tags2genia_features(4, cleaned_tags, feature_dict, options_dict)\n",
    "    print feature_dict\n",
    "    \n",
    "    for i in range(9):\n",
    "        word_tags = cleaned_tags[i]\n",
    "        \n",
    "        tag_list = [str(word_tags['inside_paren']), word_tags['pos'], word_tags['chunk'], \n",
    "        word_tags['iob'], word_tags['named_entity']]\n",
    "        \n",
    "        print_with_spaces([tokens[i]] + tag_list, [15, 10, 5, 5, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word features\n",
    "for word at index word_index and update feature_dict\n",
    "\n",
    "_INPUT_:\n",
    "- word_index: index of the word that we want to extract features\n",
    "- tokens: the word array for the abstract the word is in\n",
    "- cleaned_tags: the list of tuples from clean_tags\n",
    "- feature_dict: the dictionary that we want to store all the features for the word\n",
    "- w2v: word2vec we use, if any\n",
    "\n",
    "Relevant options:\n",
    "- left_neighbors=?, right_neighbors=?: number of left and right neighbors\n",
    "- one_hot: whether to use one-hot encodings\n",
    "- w2v: whether to use word2vec\n",
    "- w2vsize=?: the size of the word2vec\n",
    "- cosine_simil: whether to use cosine similarity with previous word\n",
    "- isupper: whether word is all capitalized\n",
    "- istitle: whether word starts with an uppercase letter followed by all lowercase letters\n",
    "- concatenate these features with '_neighbors' to use them on neighbors\n",
    "\n",
    "use base form?\n",
    "\n",
    "_OUTPUT_:\n",
    "- feature_dict\n",
    "\n",
    "source: https://github.com/bwallace/Deep-PICO/blob/master/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def tags2word_features(word_index, tokens, cleaned_tags, feature_dict, w2v, options_dict):   \n",
    "    # Extract options\n",
    "    left_neighbors = options_dict['left_neighbors']\n",
    "    right_neighbors = options_dict['right_neighbors']\n",
    "    \n",
    "    w2v_size = options_dict['w2v_size']\n",
    "        \n",
    "    # Now iterate over all possible offsets\n",
    "    for offset in range(-left_neighbors, right_neighbors+1):\n",
    "        # Index of word that we are extracting features\n",
    "        new_index = word_index + offset\n",
    "        \n",
    "        # Out of bound\n",
    "        if new_index < 0 or new_index >= len(cleaned_tags):\n",
    "            continue\n",
    "        \n",
    "        # Determine whether to include feature into dictionary\n",
    "        # If offset = 0, check using feature\n",
    "        # Otherwise, check using feature + '_neighbors'\n",
    "        def use(feature):\n",
    "            if offset == 0:\n",
    "                return options_dict[feature]\n",
    "            else:\n",
    "                return options_dict[feature + '_neighbors']\n",
    "        \n",
    "        word = tokens[new_index]\n",
    "        \n",
    "        if use('w2v'):\n",
    "            # Features: word2vec\n",
    "            try:\n",
    "                w2v_word = w2v[word]\n",
    "                found_word = True\n",
    "            except:\n",
    "                w2v_word = None\n",
    "                found_word = False\n",
    "\n",
    "            for n in range(w2v_size):\n",
    "                if found_word:\n",
    "                    feature_dict[\"w2v[{}][{}]\".format(offset, n)] = w2v_word[n]\n",
    "                else:\n",
    "                    feature_dict[\"w2v[{}][{}]\".format(offset, n)] = 0\n",
    "            \n",
    "            # Features: cosine similarity between the word and the previous word\n",
    "            if use('cosine_simil'):\n",
    "                if new_index > 0 and found_word:\n",
    "                    try:\n",
    "                        cosine_simil = w2v.similarity(tokens[new_index-1], tokens[new_index])\n",
    "                    except:\n",
    "                        cosine_simil = 0\n",
    "                    feature_dict['cosine_simil[{}]'.format(offset)] = cosine_simil\n",
    "        \n",
    "        # Features: one-hot encoding\n",
    "        if use('one_hot'):\n",
    "            feature_dict['word[{}]'.format(offset)] = word\n",
    "\n",
    "        # Features: whether the word is all capitalized\n",
    "        if use('isupper'):\n",
    "            feature_dict['isupper[{}]'.format(offset)] = word.isupper()\n",
    "        # Features: whether word starts with an uppercase letter followed by all lowercase letters\n",
    "        if use('istitle'):\n",
    "            feature_dict['istitle[{}]'.format(offset)] = word.istitle()\n",
    "       \n",
    "    return feature_dict\n",
    "\n",
    "if DEBUG:\n",
    "    f = open(directory + '/0b0153dd6caa41f79fdee74a09a9ba6e/24534270_tokens.txt')\n",
    "    tokens = f.read().split()\n",
    "    f.close()\n",
    "    \n",
    "    f = open(directory + '/0b0153dd6caa41f79fdee74a09a9ba6e/24534270_genia.tag')\n",
    "    genia_tags = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    cleaned_tags = clean_tags(genia_tags)\n",
    "    \n",
    "    options_string = 'left_neighbors=1 right_neighbors=1 one_hot one_hot_neighbors \\\n",
    "    w2v w2v_neighbors w2v_size=3 cosine_simil cosine_simil_neighbors isupper isupper_neighbors \\\n",
    "    istitle istitle_neighbors'\n",
    "    options_dict = get_options_dict(options_string)\n",
    "    \n",
    "    feature_dict ={}\n",
    "    feature_dict = tags2word_features(4, tokens, cleaned_tags, feature_dict, pubmed_w2v, options_dict)\n",
    "    print feature_dict\n",
    "    \n",
    "    for i in range(9):\n",
    "        word_tags = cleaned_tags[i]\n",
    "        \n",
    "        tag_list = [str(word_tags['inside_paren']), word_tags['pos'], word_tags['chunk'], \n",
    "        word_tags['iob'], word_tags['named_entity']]\n",
    "        \n",
    "        print_with_spaces([tokens[i]] + tag_list, [15, 10, 5, 5, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features for all abstracts\n",
    "and return an array X that contains the features for all abtracts\n",
    "\n",
    "_INPUT_:\n",
    "- tokens_list: word_array from prepocessing\n",
    "- genia_tags_list: list of genia tags\n",
    "- w2v: preloaded word2vec to use. If not None, this overrides the w2v_model option.\n",
    "\n",
    "Relevant options:\n",
    "- w2v_model: which word2vec to use. This is 'pubmed', 'pubmed_wiki', or 'False'\n",
    "\n",
    "_OUTPUT_:\n",
    "- X: [[list of features dictionary for each word in abstract 1], \n",
    "[list of features dictionary for each word in abstract 2 ], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "DISPLAY = True\n",
    "\n",
    "default_options_string = 'left_neighbors=4 right_neighbors=4 inside_paren pos chunk iob named_entity \\\n",
    "inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=100 cosine_simil cosine_simil_neighbors \\\n",
    "isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "def abstracts2features(tokens_list, genia_tags_list, w2v=None, options_string=default_options_string):\n",
    "    if len(tokens_list) != len(genia_tags_list):\n",
    "        raise ValueError('tokens_list has length {}, while genia_tags_list has length {}'\n",
    "                         .format(len(tokens_list), len(genia_tags_list)))\n",
    "    \n",
    "    # Transform options string into options dictionary\n",
    "    options_dict = get_options_dict(options_string)\n",
    "    \n",
    "    if w2v == None:\n",
    "        # Load word2vec model first\n",
    "        w2v_model = options_dict['w2v_model']\n",
    "    \n",
    "        if w2v_model == 'pubmed' or w2v_model == 'pubmed_wiki':\n",
    "            print 'Loading word2vec model...'\n",
    "            \n",
    "            if w2v_model == 'pubmed_wiki':\n",
    "                print 'Using pubmed_wiki word2vec...'\n",
    "                sys.stdout.flush()\n",
    "                word2vec_model = pubmed_wiki_w2v_name\n",
    "            else:\n",
    "                print 'Using pubmed word2vec...'\n",
    "                sys.stdout.flush()\n",
    "                word2vec_model = pubmed_w2v_name\n",
    "\n",
    "            w2v = Word2Vec.load_word2vec_format(word2vec_model, binary=True)\n",
    "            print 'Loaded word2vec model'\n",
    "        else:\n",
    "            w2v = None\n",
    "            \n",
    "    X = []\n",
    "    \n",
    "    for i in range(len(tokens_list)):\n",
    "        # Get tokens and genia tags\n",
    "        tokens = tokens_list[i]\n",
    "        genia_tags = genia_tags_list[i]\n",
    "        \n",
    "        if DISPLAY:\n",
    "            # Print progress\n",
    "            print '\\r{0}: {1}'.format(i, tokens[:3]),\n",
    "\n",
    "        '''Step 1: Clean up genia tags'''\n",
    "        \n",
    "        cleaned_tags = clean_tags(genia_tags)\n",
    "        \n",
    "        '''Step 2: Get features for abstract'''\n",
    "        \n",
    "        abstract_features = []\n",
    "        \n",
    "        for i, word in enumerate(tokens):\n",
    "            feature_dict = {}\n",
    "            \n",
    "            # Get genia features\n",
    "            feature_dict = tags2genia_features(i, cleaned_tags, feature_dict, options_dict)\n",
    "            # Get word features\n",
    "            feature_dict = tags2word_features(i, tokens, cleaned_tags, feature_dict, w2v, options_dict)\n",
    "            \n",
    "            abstract_features.append(feature_dict)\n",
    "        \n",
    "        X.append(abstract_features)\n",
    "        \n",
    "    return X\n",
    "\n",
    "if DEBUG:\n",
    "    f = open(directory + '/0b0153dd6caa41f79fdee74a09a9ba6e/24534270_tokens.txt')\n",
    "    tokens = f.read().split()\n",
    "    f.close()\n",
    "    \n",
    "    f = open(directory + '/0b0153dd6caa41f79fdee74a09a9ba6e/24534270_genia.tag')\n",
    "    genia_tags = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    options_string = 'left_neighbors=3 right_neighbors=3 inside_paren pos chunk iob named_entity \\\n",
    "    inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "    chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "    one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=3 cosine_simil cosine_simil_neighbors \\\n",
    "    isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "    X = abstracts2features([tokens], [genia_tags], w2v=pubmed_w2v, options_string=options_string)\n",
    "    print X[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing area\n",
    "This gets\n",
    "- all_tokens, all_genia_tags (45 sec)\n",
    "- train_tokens, train_genia_tags (30 sec)\n",
    "- dev_tokens, dev_genia_tags (10 sec)\n",
    "- test_tokens, test_genia_tags (5 sec)\n",
    "\n",
    "Time to generate features for all 5000 abstracts (4 neighbors + full features):\n",
    "- without word2vec: around 3-4 min\n",
    "- with word2vec of size ~10 or less: around 12 min\n",
    "- with word2vec of size ~30: around 16 min\n",
    "- with word2vec of size ~100: around 30 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DISPLAY = True\n",
    "\n",
    "# all_tokens = []\n",
    "# all_genia_tags = []\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# # For each subdirectory\n",
    "# for subdir in os.listdir(directory):\n",
    "#     subdir_path = directory + '/' + subdir\n",
    "    \n",
    "#     # Not a directory\n",
    "#     if not os.path.isdir(subdir_path):\n",
    "#         continue\n",
    "    \n",
    "#     # For each abstract in subdirectory\n",
    "#     for abstract in os.listdir(subdir_path):\n",
    "#         if abstract[-4:] == '.txt' and tokens_suffix not in abstract:\n",
    "#             abstract_index = abstract[:-4]\n",
    "            \n",
    "#             # First get the tokens\n",
    "#             f = open(subdir_path + '/' + abstract_index + tokens_suffix)\n",
    "#             tokens = f.read().split()\n",
    "#             f.close()\n",
    "            \n",
    "#             all_tokens.append(tokens)\n",
    "            \n",
    "#             # Now get genia tags\n",
    "#             f = open(subdir_path + '/' + abstract_index + genia_tags_suffix)\n",
    "#             genia_tags = pickle.load(f)\n",
    "#             f.close()\n",
    "            \n",
    "#             all_genia_tags.append(genia_tags)\n",
    "            \n",
    "#             count += 1\n",
    "            \n",
    "#             if DISPLAY and count % 10 == 0:\n",
    "#                 # Print progress\n",
    "#                 print '\\r{0}: {1}'.format(count, tokens[:3]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_tokens, tag_array = get_all_data_train()\n",
    "# train_genia_tags = get_genia_tags('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dev_tokens, tag_array = get_all_data_dev()\n",
    "# dev_genia_tags = get_genia_tags('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_tokens, tag_array = get_all_data_test()\n",
    "# test_genia_tags = get_genia_tags('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# options_string = 'left_neighbors=4 right_neighbors=4 inside_paren pos chunk iob named_entity \\\n",
    "# inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "# chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "# one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=100 cosine_simil cosine_simil_neighbors \\\n",
    "# isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "# X = abstracts2features(train_tokens, train_genia_tags, pubmed_w2v, options_string)\n",
    "# #X = abstracts2features(test_tokens, test_genia_tags, pubmed_w2v, options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sanity_check(features):\n",
    "    num_abstracts = len(features)\n",
    "    num_tokens = sum(len(abstract_features) for abstract_features in features)\n",
    "    \n",
    "    num_features = 0\n",
    "    max_features = 0\n",
    "    min_features = float('inf')\n",
    "    \n",
    "    for abstract_features in features:\n",
    "        for token_features in abstract_features:\n",
    "            len_features = len(token_features)\n",
    "            \n",
    "            num_features += len_features\n",
    "            max_features = max(max_features, len_features)\n",
    "            min_features = min(min_features, len_features)\n",
    "            \n",
    "    print 'Number of abstracts:', num_abstracts\n",
    "    print 'Number of tokens:   ', num_tokens\n",
    "    print 'Number of features: ', num_features, '\\n'\n",
    "    \n",
    "    print 'Avg tokens per abstract:', num_tokens/num_abstracts\n",
    "    print 'Avg features per token: ', num_features/num_tokens, '\\n'\n",
    "    \n",
    "    print 'Max features per token: ', max_features\n",
    "    print 'Min features per token: ', min_features\n",
    "\n",
    "# sanity_check(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
