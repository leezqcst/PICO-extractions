{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Chain CRF\n",
    "\n",
    "pycrfsuite version \n",
    "source: https://github.com/bwallace/Deep-PICO/blob/3152ab3690cad1b6e369be8a8aac27393811341c/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator, sys, time, pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from preprocess_data import get_all_data_train, get_all_data_dev, get_all_data_test\n",
    "from features_generator import abstracts2features, get_genia_tags, sanity_check\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pycrfsuite\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CRF\n",
    "_INPUT_:\n",
    "- features_list: list of list of features dictionaries\n",
    "- tags_list: list of list of tags\n",
    "- num_iters: number of iterations\n",
    "- l1, l2: regularization parameters\n",
    "- file_name: file name to write model out; '.model' added automatically\n",
    "\n",
    "_OUTPUT_:\n",
    "- The trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_crf(features_list, tags_list, num_iters, l1, l2, file_name=''):\n",
    "    # Set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose=False)\n",
    "    model.set_params({\n",
    "        'c1': l1,  # Coefficient for L1 penalty\n",
    "        'c2': l2,  # Coefficient for L2 penalty\n",
    "        'max_iterations': num_iters,\n",
    "\n",
    "        # Include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    \n",
    "    if len(features_list) != len(tags_list):\n",
    "        raise ValueError('features_list has length {}, while tags_list has length {}'\\\n",
    "                         .format(len(features_list), len(tags_list)))\n",
    "    \n",
    "    print 'Adding data...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for i in range(len(tags_list)):\n",
    "        features = features_list[i]\n",
    "        tags = tags_list[i]\n",
    "        \n",
    "        if len(features) != len(tags):\n",
    "            raise ValueError('features_list[{}] has length {}, while tags_list[{}] has length {}'\\\n",
    "                             .format(i, len(features), i, len(tags)))\n",
    "        \n",
    "        model.append(features, tags)\n",
    "\n",
    "    print 'Training model...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    model.train(file_name + '.model')\n",
    "    print 'Done!'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tagger\n",
    "Get tagger which opens file_name ('.model' added automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tagger(file_name):\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(file_name + '.model')\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print model info\n",
    "_INPUT_:\n",
    "- tagger: pycrfsuite.Tagger class (need to open model with it first)\n",
    "- num_items: number of top positive/negative state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_model_info(tagger, num_items=20):\n",
    "    # A quick peak of the model\n",
    "    info = tagger.info()\n",
    "\n",
    "    def print_transitions(trans_features):\n",
    "        for (label_from, label_to), weight in trans_features:\n",
    "            print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "    print(\"Top likely transitions:\")\n",
    "    print_transitions(Counter(info.transitions).most_common())\n",
    "\n",
    "    def print_state_features(state_features):\n",
    "        for (attr, label), weight in state_features:\n",
    "            print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "    print(\"\\nTop positive:\")\n",
    "    print_state_features(Counter(info.state_features).most_common(num_items))\n",
    "\n",
    "    print(\"\\nTop negative:\")\n",
    "    print_state_features(Counter(info.state_features).most_common()[-num_items:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict tags\n",
    "_INPUT_:\n",
    "- tagger: pycrfsuite.Tagger class (need to open model with it first)\n",
    "- features_list: list of list of features dictionaries\n",
    "\n",
    "_OUTPUT_:\n",
    "- List of list of predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_tags(tagger, features_list):\n",
    "    # Make predictions \n",
    "    pred_tags_list = []\n",
    "\n",
    "    for features in features_list:\n",
    "        pred_tags = tagger.tag(features)\n",
    "        pred_tags_list.append(pred_tags)\n",
    "    \n",
    "    return pred_tags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count tags\n",
    "_INPUT_:\n",
    "- pred_tags_list: list of list of predicted tags\n",
    "- gold_tags_list: list of list of gold tags\n",
    "- tag_name: tag name to count (e.g. 'P')\n",
    "\n",
    "_OUTPUT_:\n",
    "- Number of tags with tag name in predicted tags, gold tags, and intersection of both, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def count_tags(pred_tags_list, gold_tags_list, tag_name):\n",
    "    num_pred_tags = 0\n",
    "    num_gold_tags = 0\n",
    "    num_both_tags = 0\n",
    "    \n",
    "    if len(pred_tags_list) != len(gold_tags_list):\n",
    "        raise ValueError('pred_tags_list has length ' + str(len(pred_tags_list)) + \\\n",
    "                         ', while gold_tags_list has length ' + str(len(gold_tags_list)))\n",
    "    \n",
    "    for i in range(len(gold_tags_list)):\n",
    "        pred_tags = pred_tags_list[i]\n",
    "        gold_tags = gold_tags_list[i]\n",
    "        \n",
    "        if len(pred_tags) != len(gold_tags):\n",
    "            raise ValueError('pred_tags_list[{}] has length {}, while gold_tags_list[{}] has length {}'\\\n",
    "                             .format(i, len(pred_tags), i, len(gold_tags)))\n",
    "        \n",
    "        for j in range(len(gold_tags)):\n",
    "            if gold_tags[j] == tag_name:\n",
    "                num_gold_tags += 1\n",
    "                \n",
    "                if pred_tags[j] == tag_name:\n",
    "                    num_both_tags += 1\n",
    "                    num_pred_tags += 1\n",
    "            elif pred_tags[j] == tag_name:\n",
    "                num_pred_tags += 1\n",
    "\n",
    "    return num_pred_tags, num_gold_tags, num_both_tags\n",
    "\n",
    "if DEBUG:\n",
    "    gold_tags_list = [['None', 'P', 'None'], ['P', 'P', 'None', 'None']]\n",
    "    pred_tags_list = [['P', 'P', 'None'], ['P', 'None', 'None', 'P']]\n",
    "    \n",
    "    print count_tags(pred_tags_list, gold_tags_list, 'P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "_INPUT_:\n",
    "- Number of predicted tags, num of gold tags, number of tags predicted correctly\n",
    "\n",
    "_OUTPUT_:\n",
    "- Precision, recall, f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def metrics(num_pred_tags, num_gold_tags, num_both_tags):\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    if num_both_tags > num_pred_tags:\n",
    "        raise ValueError('num_both_tags = {} is greater than num_pred_tags = {}'\\\n",
    "                         .format(num_both_tags, num_pred_tags))\n",
    "    if num_both_tags > num_gold_tags:\n",
    "        raise ValueError('num_both_tags = {} is greater than num_gold_tags = {}'\\\n",
    "                         .format(num_both_tags, num_gold_tags))\n",
    "    \n",
    "    if num_pred_tags != 0:\n",
    "        precision = float(num_both_tags)/num_pred_tags\n",
    "        \n",
    "    if num_gold_tags != 0:\n",
    "        recall = float(num_both_tags)/num_gold_tags\n",
    "    \n",
    "    if precision != 0 and recall != 0:\n",
    "        f1 = 2/(1/precision + 1/recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "if DEBUG:\n",
    "    print metrics(3,4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate prediction\n",
    "_INPUT_:\n",
    "- pred_tags_list: list of list of predicted tags\n",
    "- gold_tags_list: list of list of gold tags\n",
    "- eval_tags: list of tags to evaluate on, e.g. 'P'\n",
    "\n",
    "_OUTPUT_:  \n",
    "- Dictionary of format {tag: (precision, recall, f1), ...} for each tag in eval_tags. Also have key 'Overall' for precision, recall, f1 of all tags considered in aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(pred_tags_list, gold_tags_list, eval_tags):\n",
    "    # Compute evaluation metrics\n",
    "    num_pred_all = 0\n",
    "    num_gold_all = 0\n",
    "    num_both_all = 0\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Metrics for each tag\n",
    "    for tag in eval_tags:\n",
    "        num_pred, num_gold, num_both = count_tags(pred_tags_list, gold_tags_list, tag)\n",
    "\n",
    "        p, r, f1 = metrics(num_pred, num_gold, num_both)\n",
    "        result[tag] = (p, r, f1)\n",
    "\n",
    "        num_pred_all += num_pred\n",
    "        num_gold_all += num_gold\n",
    "        num_both_all += num_both\n",
    "\n",
    "    # Overall metrics\n",
    "    p_overall, r_overall, f1_overall = metrics(num_pred_all, num_gold_all, num_both_all)\n",
    "    result['Overall'] = (p_overall, r_overall, f1_overall)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to and read from files\n",
    "'.result' added to file name automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_result(result, file_name):\n",
    "    f = open(file_name + '.result', 'w')\n",
    "    pickle.dump(result, f)\n",
    "    f.close()\n",
    "\n",
    "def read_result(file_name):\n",
    "    f = open(file_name + '.result', 'r')\n",
    "    result = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get CRF results\n",
    "Quick run of CRF as 1 function call\n",
    "\n",
    "_INPUT_:\n",
    "- train_features: list of list of train features dictionaries\n",
    "- train_tags: list of list of train tags\n",
    "- test_features: list of list of test features dictionaries\n",
    "- test_tags: list of list of test tags\n",
    "- num_iters: number of iterations\n",
    "- l1, l2: regularization parameters\n",
    "- eval_tags: list of tags to evaluate on, e.g. 'P'\n",
    "- file_name: file name to write model out; '.model' added automatically\n",
    "- save: whether to save result to file, named (file_name + '.result')\n",
    "\n",
    "_OUTPUT_:\n",
    "- Result as computed by evaluate_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_crf_results(train_features, train_tags, test_features, test_tags, num_iters, l1, l2, eval_tags,\n",
    "                    file_name='', save=False):\n",
    "    # Train model\n",
    "    model = train_crf(train_features, train_tags, num_iters, l1, l2, file_name)\n",
    "\n",
    "    # Get tagger\n",
    "    tagger = get_tagger(file_name)\n",
    "\n",
    "    # Make predictions\n",
    "    pred_test_tags = predict_tags(tagger, test_features)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    result = evaluate_prediction(pred_test_tags, test_tags, eval_tags)\n",
    "    \n",
    "    if save:\n",
    "        write_result(result, file_name)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get k-fold results\n",
    "_INPUT_:\n",
    "- features_list: list of list of features dictionaries\n",
    "- tags_list: list of list of tags\n",
    "- num_iters: number of iterations\n",
    "- l1, l2: regularization parameters\n",
    "- eval_tags: list of tags we are evaluating on, e.g. 'P'\n",
    "- file_name: file name to write model out; '.model' added automatically\n",
    "- save: whether to save result to file, named (file_name + '.result')\n",
    "- n_folds: number of folds\n",
    "\n",
    "_OUTPUT_:\n",
    "- List of dictionaries for the each fold result, as computed by evaluate_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kfold_results(features_list, tags_list, num_iters, l1, l2, eval_tags, file_name='', save=False, n_folds=5):\n",
    "    # Set up the KFold\n",
    "    num_abstracts = len(tags_list)\n",
    "    \n",
    "    if len(features_list) != len(tags_list):\n",
    "        raise ValueError('features_list has length {}, while tags_list has length {}'\\\n",
    "                         .format(len(features_list), len(tags_list)))\n",
    "\n",
    "    kf = KFold(num_abstracts, random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "    \n",
    "    # Store result of each fold\n",
    "    fold_result_list = []\n",
    "    \n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf):\n",
    "        print 'On fold %s' % fold_idx\n",
    "\n",
    "        train_features = [features_list[i] for i in train_indices]\n",
    "        train_tags = [tags_list[i] for i in train_indices]\n",
    "\n",
    "        test_features = [features_list[i] for i in test_indices]\n",
    "        test_tags = [tags_list[i] for i in test_indices]\n",
    "        \n",
    "        # Get result of this fold\n",
    "        fold_result = get_crf_results(train_features, train_tags, test_features, test_tags,\\\n",
    "                                      num_iters, l1, l2, eval_tags, file_name=file_name)\n",
    "        \n",
    "        fold_result_list.append(fold_result)\n",
    "    \n",
    "    if save:\n",
    "        write_result(fold_result_list, file_name)\n",
    "    \n",
    "    return fold_result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average scores\n",
    "Compute average scores from result outputted from get_kfold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_scores(result):\n",
    "    if type(result) is not list:\n",
    "        raise ValueError('result must be of type list')\n",
    "    \n",
    "    eval_tags = result[0].keys()\n",
    "    \n",
    "    avg_dict = dict()\n",
    "    \n",
    "    for tag in eval_tags:\n",
    "        avg_dict[tag] = tuple(np.mean([fold_result[tag][i] for fold_result in result]) for i in range(3))\n",
    "    \n",
    "    return avg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print result\n",
    "Can print result of\n",
    "- evaluate_prediction, get_crf_results, average_scores: a single dictionary\n",
    "- get_kfold_results: list of dictionaries\n",
    "- grid_search: dictionary of dictionaries\n",
    "- sort_by_metric: list of (tuple, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    if type(result) is dict:\n",
    "        value = result.values()[0]\n",
    "        \n",
    "        if type(value) is tuple:\n",
    "            # result is a single dictionary\n",
    "            for tag, value in result.iteritems():\n",
    "                print '{}: {}'.format(tag, value)\n",
    "        elif type(value) is dict:\n",
    "            # result is a dictionary of dictionaries\n",
    "            for (l1, l2), params_result in result.iteritems():\n",
    "                print 'L1: {}, L2: {}'.format(l1, l2)\n",
    "                print_result(params_result)\n",
    "        else:\n",
    "            raise ValueError('result must be dictionary of tuples or dicts')\n",
    "    elif type(result) is list:\n",
    "        item = result[0]\n",
    "        \n",
    "        if type(item) is dict:\n",
    "            # result is a list of dictionaries\n",
    "            for i in range(len(result)):\n",
    "                print 'Fold {}'.format(i)\n",
    "                print_result(result[i])\n",
    "\n",
    "            # Also print out average\n",
    "            print 'Average'\n",
    "            avg_dict = average_scores(result)\n",
    "            print_result(avg_dict)\n",
    "        elif type(item) is tuple:\n",
    "            # result is a list of (tuple, dictionary)\n",
    "            for (l1, l2), params_result in result:\n",
    "                print 'L1: {}, L2: {}'.format(l1, l2)\n",
    "                print_result(params_result)\n",
    "        else:\n",
    "            raise ValueRror('result must be list of tuples or dicts')\n",
    "    else:\n",
    "        raise ValueError('result must be of type dict or list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "\n",
    "_INPUT_:\n",
    "- train_features: list of list of train features dictionaries\n",
    "- train_tags: list of list of train tags\n",
    "- test_features: list of list of test features dictionaries\n",
    "- test_tags: list of list of test tags\n",
    "- num_iters: number of iterations\n",
    "- l1_list, l2_list: lists of regularization parameters to try\n",
    "- eval_tags: list of tags to evaluate on, e.g. 'P'\n",
    "- file_name: file name to write model out; '.model' added automatically\n",
    "- save: whether to save result to file, named (file_name + '.result')\n",
    "\n",
    "_OUTPUT_:\n",
    "- Dictionary mapping (l1, l2) to associated result from get_crf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(train_features, train_tags, test_features, test_tags, num_iters, l1_list, l2_list, eval_tags,\n",
    "                file_name='', save=False):\n",
    "    grid_search_result = dict()\n",
    "    \n",
    "    for l1 in l1_list:\n",
    "        for l2 in l2_list:\n",
    "            # Run CRF\n",
    "            result = get_crf_results(train_features, train_tags, test_features, test_tags,\\\n",
    "                                     num_iters, l1, l2, eval_tags, file_name=file_name)\n",
    "            \n",
    "            print 'L1: {}, L2: {}, scores: {}'.format(l1, l2, result)\n",
    "            \n",
    "            # Store result\n",
    "            grid_search_result[l1, l2] = result\n",
    "    \n",
    "    if save:\n",
    "        write_result(grid_search_result, file_name)\n",
    "\n",
    "    return grid_search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by metric\n",
    "Sort result of grid search\n",
    "\n",
    "_INPUT_:\n",
    "- grid_search_result: result of grid_search\n",
    "- tag: tag to sort with\n",
    "- metric: metric to sort with\n",
    "\n",
    "_OUTPUT_:\n",
    "- List of ((l1, l2), result), sorted descending by the specified metric of the specified tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sort_by_metric(grid_search_result, tag='Overall', metric='f1'):\n",
    "    metric2index = {\n",
    "        'p': 0,\n",
    "        'precision': 0,\n",
    "        'r': 1,\n",
    "        'recall': 1,\n",
    "        'f': 2,\n",
    "        'f1': 2\n",
    "    }\n",
    "    \n",
    "    # Gives index corresponding to metric\n",
    "    metric_index = metric2index[metric.lower()]\n",
    "    \n",
    "    # Get tag's metric\n",
    "    get_tag_metric = lambda x: x[1][tag][metric_index]\n",
    "    \n",
    "    # Sort result\n",
    "    sorted_result = sorted(grid_search_result.items(), key=get_tag_metric, reverse=True)\n",
    "    \n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get train data\n",
    "train_tokens, train_tags = get_all_data_train()\n",
    "train_genia_tags = get_genia_tags('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dev data\n",
    "dev_tokens, dev_tags = get_all_data_dev()\n",
    "dev_genia_tags = get_genia_tags('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get test data\n",
    "test_tokens, test_tags = get_all_data_test()\n",
    "test_genia_tags = get_genia_tags('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set options\n",
    "big_options_string = 'left_neighbors=1 right_neighbors=0 inside_paren pos chunk iob named_entity \\\n",
    "inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=10 cosine_simil cosine_simil_neighbors \\\n",
    "isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "options_string = 'left_neighbors=1 right_neighbors=1 one_hot one_hot_neighbors'\n",
    "\n",
    "w2v=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute features for train\n",
    "train_features = abstracts2features(train_tokens, train_genia_tags, w2v=w2v, options_string=options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute features for dev\n",
    "dev_features = abstracts2features(dev_tokens, dev_genia_tags, w2v=w2v, options_string=options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute features for test\n",
    "test_features = abstracts2features(test_tokens, test_genia_tags, w2v=w2v, options_string=options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "sanity_check(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick run :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set options\n",
    "num_iters = 100\n",
    "l1 = 1\n",
    "l2 = 0.01\n",
    "eval_tags = ['P']\n",
    "file_name = 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run CRF\n",
    "start_time = time.time()\n",
    "crf_result = get_crf_results(train_features, train_tags, dev_features, dev_tags, num_iters, l1, l2, eval_tags,\n",
    "                             file_name=file_name)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print result\n",
    "print_result(crf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "model = train_crf(train_features, train_tags, num_iters, l1, l2, file_name)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get model from file\n",
    "tagger = get_tagger(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "print_model_info(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict dev tags\n",
    "pred_dev_tags = predict_tags(tagger, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate dev tags\n",
    "dev_result = evaluate_prediction(pred_dev_tags, dev_tags, eval_tags)\n",
    "print_result(dev_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict train tags\n",
    "pred_train_tags = predict_tags(tagger, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate train tags\n",
    "train_result = evaluate_prediction(pred_train_tags, train_tags, eval_tags)\n",
    "print_result(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict test tags\n",
    "pred_test_tags = predict_tags(tagger, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate test tags\n",
    "test_result = evaluate_prediction(pred_test_tags, test_tags, eval_tags)\n",
    "print_result(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run K-fold\n",
    "kfold_file_name = 'kfold'\n",
    "\n",
    "start_time = time.time()\n",
    "kfold_result = get_kfold_results(train_features, train_tags, num_iters, l1, l2, eval_tags, file_name=kfold_file_name)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all results\n",
    "print_result(kfold_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print just the average scores\n",
    "print_result(average_scores(kfold_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "grid_file_name = 'grid_search'\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search_result = grid_search(train_features, train_tags, dev_features, dev_tags,\\\n",
    "                                 num_iters, [0,0.05,0.1,0.15,0.2], [0,0.05,0.1], eval_tags, file_name=grid_file_name)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort result\n",
    "sorted_result = sort_by_metric(grid_search_result, tag='Overall', metric='f1')\n",
    "print_result(sorted_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
