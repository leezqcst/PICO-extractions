{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Chain CRF\n",
    "\n",
    "pycrfsuite version \n",
    "source: https://github.com/bwallace/Deep-PICO/blob/3152ab3690cad1b6e369be8a8aac27393811341c/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, time, pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from preprocess_data import get_all_data_train, get_all_data_dev, get_all_data_test\n",
    "from features_generator import abstracts2features, get_genia_tags, sanity_check\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pycrfsuite\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CRF\n",
    "_INPUT_:\n",
    "- features_list: list of list of features dictionaries\n",
    "- tags_list: list of list of tags\n",
    "- num_iters: number of iterations\n",
    "- l1, l2: regularization parameters\n",
    "- file_name: file name to write model out; '.model' added automatically\n",
    "\n",
    "_OUTPUT_:\n",
    "- The trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_crf(features_list, tags_list, num_iters, l1, l2, file_name=''):\n",
    "    # Set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose=False)\n",
    "    model.set_params({\n",
    "        'c1': l1,  # Coefficient for L1 penalty\n",
    "        'c2': l2,  # Coefficient for L2 penalty\n",
    "        'max_iterations': num_iters,\n",
    "\n",
    "        # Include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    \n",
    "    if len(features_list) != len(tags_list):\n",
    "        raise ValueError('features_list has length {}, while tags_list has length {}'\\\n",
    "                         .format(len(features_list), len(tags_list)))\n",
    "    \n",
    "    print 'Adding data...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for i in range(len(tags_list)):\n",
    "        features = features_list[i]\n",
    "        tags = tags_list[i]\n",
    "        \n",
    "        if len(features) != len(tags):\n",
    "            raise ValueError('features_list[{}] has length {}, while tags_list[{}] has length {}'\\\n",
    "                             .format(i, len(features), i, len(tags)))\n",
    "        \n",
    "        model.append(features, tags)\n",
    "\n",
    "    print 'Training model...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    model.train(file_name + '.model')\n",
    "    print 'Done!'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tagger\n",
    "Get tagger which opens file_name ('.model' added automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tagger(file_name):\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(file_name + '.model')\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print model info\n",
    "_INPUT_:\n",
    "- tagger: pycrfsuite.Tagger class (need to open model with it first)\n",
    "- num_items: number of top positive/negative state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_model_info(tagger, num_items=20):\n",
    "    # A quick peak of the model\n",
    "    info = tagger.info()\n",
    "\n",
    "    def print_transitions(trans_features):\n",
    "        for (label_from, label_to), weight in trans_features:\n",
    "            print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "    print(\"Top likely transitions:\")\n",
    "    print_transitions(Counter(info.transitions).most_common())\n",
    "\n",
    "    def print_state_features(state_features):\n",
    "        for (attr, label), weight in state_features:\n",
    "            print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "    print(\"\\nTop positive:\")\n",
    "    print_state_features(Counter(info.state_features).most_common(num_items))\n",
    "\n",
    "    print(\"\\nTop negative:\")\n",
    "    print_state_features(Counter(info.state_features).most_common()[-num_items:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict tags\n",
    "_INPUT_:\n",
    "- tagger: pycrfsuite.Tagger class (need to open model with it first)\n",
    "- features_list: list of list of features dictionaries\n",
    "\n",
    "_OUTPUT_:\n",
    "- List of list of predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_tags(tagger, features_list):\n",
    "    # Make predictions \n",
    "    pred_tags_list = []\n",
    "\n",
    "    for features in features_list:\n",
    "        pred_tags = tagger.tag(features)\n",
    "        pred_tags_list.append(pred_tags)\n",
    "    \n",
    "    return pred_tags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count tags\n",
    "_INPUT_:\n",
    "- pred_tags_list: list of list of predicted tags\n",
    "- gold_tags_list: list of list of gold tags\n",
    "- tag_name: tag name to count (e.g. 'P')\n",
    "\n",
    "_OUTPUT_:\n",
    "- Number of tags with tag name in predicted tags, gold tags, and intersection of both, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def count_tags(pred_tags_list, gold_tags_list, tag_name):\n",
    "    num_pred_tags = 0\n",
    "    num_gold_tags = 0\n",
    "    num_both_tags = 0\n",
    "    \n",
    "    if len(pred_tags_list) != len(gold_tags_list):\n",
    "        raise ValueError('pred_tags_list has length ' + str(len(pred_tags_list)) + \\\n",
    "                         ', while gold_tags_list has length ' + str(len(gold_tags_list)))\n",
    "    \n",
    "    for i in range(len(gold_tags_list)):\n",
    "        pred_tags = pred_tags_list[i]\n",
    "        gold_tags = gold_tags_list[i]\n",
    "        \n",
    "        if len(pred_tags) != len(gold_tags):\n",
    "            raise ValueError('pred_tags_list[{}] has length {}, while gold_tags_list[{}] has length {}'\\\n",
    "                             .format(i, len(pred_tags), i, len(gold_tags)))\n",
    "        \n",
    "        for j in range(len(gold_tags)):\n",
    "            if gold_tags[j] == tag_name:\n",
    "                num_gold_tags += 1\n",
    "                \n",
    "                if pred_tags[j] == tag_name:\n",
    "                    num_both_tags += 1\n",
    "                    num_pred_tags += 1\n",
    "            elif pred_tags[j] == tag_name:\n",
    "                num_pred_tags += 1\n",
    "\n",
    "    return num_pred_tags, num_gold_tags, num_both_tags\n",
    "\n",
    "if DEBUG:\n",
    "    gold_tags_list = [['None', 'P', 'None'], ['P', 'P', 'None', 'None']]\n",
    "    pred_tags_list = [['P', 'P', 'None'], ['P', 'None', 'None', 'P']]\n",
    "    \n",
    "    print count_tags(pred_tags_list, gold_tags_list, 'P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "_INPUT_:\n",
    "- Number of predicted tags, num of gold tags, number of tags predicted correctly\n",
    "\n",
    "_OUTPUT_:\n",
    "- Precision, recall, f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def metrics(num_pred_tags, num_gold_tags, num_both_tags):\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    if num_both_tags > num_pred_tags:\n",
    "        raise ValueError('num_both_tags = {} is greater than num_pred_tags = {}'\\\n",
    "                         .format(num_both_tags, num_pred_tags))\n",
    "    if num_both_tags > num_gold_tags:\n",
    "        raise ValueError('num_both_tags = {} is greater than num_gold_tags = {}'\\\n",
    "                         .format(num_both_tags, num_gold_tags))\n",
    "    \n",
    "    if num_pred_tags != 0:\n",
    "        precision = float(num_both_tags)/num_pred_tags\n",
    "        \n",
    "    if num_gold_tags != 0:\n",
    "        recall = float(num_both_tags)/num_gold_tags\n",
    "    \n",
    "    if precision != 0 and recall != 0:\n",
    "        f1 = 2/(1/precision + 1/recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "if DEBUG:\n",
    "    print metrics(3,4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate prediction\n",
    "_INPUT_:\n",
    "- pred_tags_list: list of list of predicted tags\n",
    "- gold_tags_list: list of list of gold tags\n",
    "- eval_tags: list of tags to evaluate on\n",
    "\n",
    "_OUTPUT_:  \n",
    "- Dictionary of format {tag: (precision, recall, f1), ...} for each tag in eval_tags. Also have key 'Overall' for precision, recall, f1 of all tags considered in aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(pred_tags_list, gold_tags_list, eval_tags):\n",
    "    # Compute evaluation metrics\n",
    "    num_pred_all = 0\n",
    "    num_gold_all = 0\n",
    "    num_both_all = 0\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Metrics for each tag\n",
    "    for tag in eval_tags:\n",
    "        num_pred, num_gold, num_both = count_tags(pred_tags_list, gold_tags_list, tag)\n",
    "\n",
    "        p, r, f1 = metrics(num_pred, num_gold, num_both)\n",
    "        result[tag] = (p, r, f1)\n",
    "\n",
    "        num_pred_all += num_pred\n",
    "        num_gold_all += num_gold\n",
    "        num_both_all += num_both\n",
    "\n",
    "    # Overall metrics\n",
    "    p_overall, r_overall, f1_overall = metrics(num_pred_all, num_gold_all, num_both_all)\n",
    "    result['Overall'] = (p_overall, r_overall, f1_overall)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get k-fold results\n",
    "_INPUT_:\n",
    "- features_list: list of list of features dictionaries\n",
    "- tags_list: list of list of tags\n",
    "- num_iters: number of iterations\n",
    "- l1, l2: regularization parameters\n",
    "- eval_tags: list of tags we are evaluating on, e.g. 'P'\n",
    "- file_name: file name to write model out; '.model' added automatically\n",
    "- save: whether to save result to file, named (file_name + '.result')\n",
    "- n_folds: number of folds\n",
    "\n",
    "_OUTPUT_:\n",
    "- List of dictionaries for the each fold result, as computed by evaluate_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kfold_results(features_list, tags_list, num_iters, l1, l2, eval_tags, file_name='', save=False, n_folds=5):\n",
    "    # Set up the KFold\n",
    "    num_abstracts = len(tags_list)\n",
    "    \n",
    "    if len(features_list) != len(tags_list):\n",
    "        raise ValueError('features_list has length {}, while tags_list has length {}'\\\n",
    "                         .format(len(features_list), len(tags_list)))\n",
    "\n",
    "    kf = KFold(num_abstracts, random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "    \n",
    "    # Store result of each fold\n",
    "    fold_result_list = []\n",
    "    \n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf):\n",
    "        print 'On fold %s' % fold_idx\n",
    "\n",
    "        train_features = [features_list[i] for i in train_indices]\n",
    "        train_tags = [tags_list[i] for i in train_indices]\n",
    "\n",
    "        test_features = [features_list[i] for i in test_indices]\n",
    "        test_tags = [tags_list[i] for i in test_indices]\n",
    "\n",
    "        # Train model\n",
    "        model = train_crf(train_features, train_tags, num_iters, l1, l2, file_name)\n",
    "\n",
    "        # Get tagger\n",
    "        tagger = get_tagger(file_name)\n",
    "\n",
    "        # Make predictions\n",
    "        pred_test_tags = predict_tags(tagger, test_features)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        fold_result = evaluate_prediction(pred_test_tags, test_tags, eval_tags)\n",
    "\n",
    "        fold_result_list.append(fold_result)\n",
    "    \n",
    "    if save:\n",
    "        f = open(file_name + '.result', 'w')\n",
    "        pickle.dump(fold_result_list, f)\n",
    "        f.close()\n",
    "    \n",
    "    return fold_result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average scores\n",
    "Compute average scores from result outputted from get_kfold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_scores(result):\n",
    "    if type(result) is not list:\n",
    "        raise ValueError('result must be of type list')\n",
    "    \n",
    "    eval_tags = result[0].keys()\n",
    "    \n",
    "    avg_dict = dict()\n",
    "    \n",
    "    for tag in eval_tags:\n",
    "        avg_dict[tag] = tuple(np.mean([fold_result[tag][i] for fold_result in result]) for i in range(3))\n",
    "    \n",
    "    return avg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print result\n",
    "Can print result of either evaluate_prediction, average_scores (a single dictionary) or get_kfold_results (list of dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    if type(result) is dict:\n",
    "        for tag, value in result.iteritems():\n",
    "            print '{}: {}'.format(tag, value)\n",
    "    elif type(result) is list:\n",
    "        for i in range(len(result)):\n",
    "            print 'Fold {}'.format(i)\n",
    "            print_result(result[i])\n",
    "        \n",
    "        # Also print out average\n",
    "        print 'Average'\n",
    "        avg_dict = average_scores(result)\n",
    "        print_result(avg_dict)\n",
    "    else:\n",
    "        raise ValueError('result must be of type dict or list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(features_list, tags_list, num_iters, l1_list, l2_list, eval_tags, n_folds=5):\n",
    "    grid_search_result = {}\n",
    "    for l1 in l1_list:\n",
    "        for l2 in l2_list:\n",
    "            # Run k-fold\n",
    "            result = get_kfold_results(features_list, tags_list, num_iters, l1, l2, eval_tags, n_folds=n_folds)\n",
    "            \n",
    "            # Keep the average scores\n",
    "            åavg_scores = average_scores(result)\n",
    "            \n",
    "            print 'L1: {}, L2: {}, scores: {}'.format(l1, l2, avg_scores)\n",
    "            grid_search_result[l1, l2] = avg_scores\n",
    "\n",
    "    return grid_search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train data\n",
    "train_tokens, train_tags = get_all_data_train()\n",
    "train_genia_tags = get_genia_tags('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dev data\n",
    "dev_tokens, dev_tags = get_all_data_dev()\n",
    "dev_genia_tags = get_genia_tags('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set options\n",
    "big_options_string = 'left_neighbors=1 right_neighbors=0 inside_paren pos chunk iob named_entity \\\n",
    "inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=10 cosine_simil cosine_simil_neighbors \\\n",
    "isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "small_options_string = 'left_neighbors=0 right_neighbors=0 one_hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute features for train\n",
    "train_features = abstracts2features(train_tokens, train_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute features for dev\n",
    "dev_features = abstracts2features(dev_tokens, dev_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "sanity_check(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_crf(train_features, train_tags, 100, 0.1, 0.1, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get model from file\n",
    "tagger = get_tagger('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "print_model_info(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_dev_tags = predict_tags(tagger, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate a single tag\n",
    "num_pred, num_gold, num_both = count_tags(pred_dev_tags, dev_tags, 'P')\n",
    "p, r, f1 = metrics(num_pred, num_gold, num_both)\n",
    "print num_pred, num_gold, num_both\n",
    "print \"Precision:\", p, \"Recall:\", r, \"F1:\", f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate all tags at once\n",
    "result = evaluate_prediction(pred_dev_tags, dev_tags, ['P'])\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run K-fold\n",
    "start_time = time.time()\n",
    "kfold_result = get_kfold_results(train_features, train_tags, 100, 0.1, 0.1, ['P'], 'base')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all results\n",
    "print_result(kfold_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print just the average scores\n",
    "print_result(average_scores(kfold_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0, L2: 0, scores: {'P': (0.69074569376883965, 0.33279688828277443, 0.4483076485862994), 'Overall': (0.69074569376883965, 0.33279688828277443, 0.4483076485862994)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0, L2: 0.1, scores: {'P': (0.67840548284887525, 0.36793663268027083, 0.47660282738274251), 'Overall': (0.67840548284887525, 0.36793663268027083, 0.47660282738274251)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0, L2: 0.2, scores: {'P': (0.67443233429015259, 0.34814018930422863, 0.45826956504366512), 'Overall': (0.67443233429015259, 0.34814018930422863, 0.45826956504366512)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0.1, L2: 0, scores: {'P': (0.63981752128399116, 0.43025100983333725, 0.51434361622346536), 'Overall': (0.63981752128399116, 0.43025100983333725, 0.51434361622346536)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0.1, L2: 0.1, scores: {'P': (0.66840635704718032, 0.41287717337282004, 0.51032647244391049), 'Overall': (0.66840635704718032, 0.41287717337282004, 0.51032647244391049)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0.1, L2: 0.2, scores: {'P': (0.67912409058685408, 0.40156458228089686, 0.50456310428978135), 'Overall': (0.67912409058685408, 0.40156458228089686, 0.50456310428978135)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0.2, L2: 0, scores: {'P': (0.65414343746159775, 0.42159722252676612, 0.51259895206026651), 'Overall': (0.65414343746159775, 0.42159722252676612, 0.51259895206026651)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0.2, L2: 0.1, scores: {'P': (0.67542108391370292, 0.40599626271257661, 0.50694325154864717), 'Overall': (0.67542108391370292, 0.40599626271257661, 0.50694325154864717)}\n",
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "L1: 0.2, L2: 0.2, scores: {'P': (0.68258567222657474, 0.39866616165125801, 0.50326106167846274), 'Overall': (0.68258567222657474, 0.39866616165125801, 0.50326106167846274)}\n",
      "--- 972.534544945 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "grid_search_result = grid_search(train_features, train_tags, 100, [0,0.1,0.2], [0,0.1,0.2], ['P'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0.2, 0.1): {'P': (0.67542108391370292, 0.40599626271257661, 0.50694325154864717), 'Overall': (0.67542108391370292, 0.40599626271257661, 0.50694325154864717)}, (0, 0): {'P': (0.69074569376883965, 0.33279688828277443, 0.4483076485862994), 'Overall': (0.69074569376883965, 0.33279688828277443, 0.4483076485862994)}, (0.1, 0.1): {'P': (0.66840635704718032, 0.41287717337282004, 0.51032647244391049), 'Overall': (0.66840635704718032, 0.41287717337282004, 0.51032647244391049)}, (0, 0.2): {'P': (0.67443233429015259, 0.34814018930422863, 0.45826956504366512), 'Overall': (0.67443233429015259, 0.34814018930422863, 0.45826956504366512)}, (0.1, 0.2): {'P': (0.67912409058685408, 0.40156458228089686, 0.50456310428978135), 'Overall': (0.67912409058685408, 0.40156458228089686, 0.50456310428978135)}, (0.1, 0): {'P': (0.63981752128399116, 0.43025100983333725, 0.51434361622346536), 'Overall': (0.63981752128399116, 0.43025100983333725, 0.51434361622346536)}, (0.2, 0): {'P': (0.65414343746159775, 0.42159722252676612, 0.51259895206026651), 'Overall': (0.65414343746159775, 0.42159722252676612, 0.51259895206026651)}, (0, 0.1): {'P': (0.67840548284887525, 0.36793663268027083, 0.47660282738274251), 'Overall': (0.67840548284887525, 0.36793663268027083, 0.47660282738274251)}, (0.2, 0.2): {'P': (0.68258567222657474, 0.39866616165125801, 0.50326106167846274), 'Overall': (0.68258567222657474, 0.39866616165125801, 0.50326106167846274)}}\n"
     ]
    }
   ],
   "source": [
    "print grid_search_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
