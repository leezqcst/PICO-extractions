{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-Chain CRF\n",
    "\n",
    "pycrfsuite version \n",
    "source: https://github.com/bwallace/Deep-PICO/blob/3152ab3690cad1b6e369be8a8aac27393811341c/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "from preprocess_data import get_all_data_train, get_all_data_dev, get_all_data_test\n",
    "from features_generator import abstracts2features, get_genia_tags, sanity_check\n",
    "from evaluation import eval_abstracts, eval_abstracts_avg\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn_crfsuite import metrics\n",
    "import pycrfsuite\n",
    "import sklearn_crfsuite\n",
    "import scipy\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_crf(features_list, tags_list, num_iters, l1, l2, file_name=''):\n",
    "    # Set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose=False)\n",
    "    model.set_params({\n",
    "        'c1': l1,  # Coefficient for L1 penalty\n",
    "        'c2': l2,  # Coefficient for L2 penalty\n",
    "        'max_iterations': num_iters,  # Stop earlier\n",
    "\n",
    "        # Include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    \n",
    "    print 'Adding data...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for i in range(len(tags_list)):\n",
    "        model.append(features_list[i], tags_list[i])\n",
    "\n",
    "    print 'Training model...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    model.train(file_name)\n",
    "    print 'Done!'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_model_info(tagger, num_items=20):\n",
    "    # A quick peak of the model\n",
    "    info = tagger.info()\n",
    "\n",
    "    def print_transitions(trans_features):\n",
    "        for (label_from, label_to), weight in trans_features:\n",
    "            print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "    print(\"Top likely transitions:\")\n",
    "    print_transitions(Counter(info.transitions).most_common())\n",
    "\n",
    "    def print_state_features(state_features):\n",
    "        for (attr, label), weight in state_features:\n",
    "            print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "    print(\"\\nTop positive:\")\n",
    "    print_state_features(Counter(info.state_features).most_common(num_items))\n",
    "\n",
    "    print(\"\\nTop negative:\")\n",
    "    print_state_features(Counter(info.state_features).most_common()[-num_items:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_tags(tagger, features_list):\n",
    "    # Make predictions \n",
    "    pred_tags_list = []\n",
    "\n",
    "    for features in features_list:\n",
    "        pred_tags = tagger.tag(features)\n",
    "        pred_tags_list.append(pred_tags)\n",
    "    \n",
    "    return pred_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_crf_kfold(features_list, tags_list, num_iters, l1, l2, grid_search, file_name='', n_folds=5):\n",
    "    \n",
    "    # Set up the KFold\n",
    "    num_abstracts = len(tags_list)\n",
    "\n",
    "    kf = KFold(num_abstracts, random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "    \n",
    "    recall_scores=[]\n",
    "    precision_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    labels = set(tags_list[0])\n",
    "    \n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf):\n",
    "        print 'on fold %s' % fold_idx\n",
    "        print 'loading data...'\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        train_x = [features_list[i] for i in train_indices]\n",
    "        train_y = [tags_list[i] for i in train_indices]\n",
    "        \n",
    "        test_x = [features_list[i] for i in test_indices]\n",
    "        test_y = [tags_list[i] for i in test_indices]\n",
    "        \n",
    "        print 'Adding data'\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        model = pycrfsuite.Trainer(verbose=False)\n",
    "        model.set_params({'c1': l1,'c2': l2,'max_iterations': num_iters,'feature.possible_transitions': True})\n",
    "        \n",
    "        for i in train_indices:\n",
    "            model.append(features_list[i], tags_list[i])\n",
    "        \n",
    "        #train the model\n",
    "        if grid_search:\n",
    "            crf = sklearn_crfsuite.CRF(algorithm='lbfgs',c1=l1,c2=l2,max_iterations=num_iters,all_possible_transitions=False)\n",
    "            \n",
    "            params_space = {\n",
    "                'c1': scipy.stats.expon(scale=0.5),\n",
    "                'c2': scipy.stats.expon(scale=0.05),\n",
    "            }\n",
    "            \n",
    "            # use the same metric for evaluation\n",
    "            f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                                    average='weighted', labels=labels)\n",
    "\n",
    "\n",
    "            # search\n",
    "            rs = RandomizedSearchCV(crf, params_space,\n",
    "                                    cv=3,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=-1,\n",
    "                                    n_iter=50,\n",
    "                                    scoring=f1_scorer)\n",
    "            rs.fit(train_x, train_y)\n",
    "            tagger = rs.best_estimator_.tagger_\n",
    "        else:\n",
    "            model_name = file_name + '_model {}'.format(fold_idx)\n",
    "            print('training model...')\n",
    "            model.train(model_name)\n",
    "            print('done')\n",
    "            tagger = pycrfsuite.Tagger()\n",
    "            tagger.open(model_name)\n",
    "\n",
    "        \n",
    "        #make predictions\n",
    "        test_pred = predict_tags(tagger, test_x)\n",
    "            \n",
    "        # compute evaluation metrics    \n",
    "        fold_precision,fold_recall, fold_f1 = eval_abstracts(test_y, test_pred)\n",
    "        \n",
    "        recall_scores.append(fold_recall)\n",
    "        precision_scores.append(fold_precision)\n",
    "        f1_scores.append(fold_f1_score)\n",
    "\n",
    "        fold_recall_results = \"Fold recall: {}\".format(fold_recall)\n",
    "        fold_precision_results = \"Fold precision: {}\".format(fold_precision)\n",
    "        fold_f1_results = \"Fold F1 Score: {}\".format(fold_f1_score)\n",
    "        print fold_recall_results\n",
    "        print fold_precision_results\n",
    "        print fold_f1_results\n",
    "\n",
    "        file = open(modelname + '_results.txt', 'w+')\n",
    "\n",
    "        file.write(fold_recall_results + '\\n')\n",
    "        file.write(fold_precision_results + '\\n')\n",
    "        file.write(fold_f1_results + '\\n')\n",
    "\n",
    "       \n",
    "    recall_average = np.mean(recall_scores)\n",
    "    precision_average = np.mean(precision_scores)\n",
    "    f1_scores = np.mean(f1_scores)\n",
    "\n",
    "    print \"Recall Average: {}\".format(recall_average)\n",
    "    print \"Precision Average: {}\".format(precision_average)\n",
    "    print \"F1 Average: {}\".format(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crf(l2,l1,iters,grid_search,modelname,train_features,train_tag_array):\n",
    "    \n",
    "    # set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose = False)\n",
    "    n = len(train_tag_array)\n",
    "    n_folds= 5\n",
    "    kf = KFold(n ,random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "    \n",
    "    recall_scores=[]\n",
    "    precision_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    labels = set(train_tag_array[0])\n",
    "    \n",
    "    for fold_idx, (train,test) in enumerate(kf):\n",
    "        print(\"on fold %s\" % fold_idx)\n",
    "        print('loading data...')\n",
    "        train_x =[train_features[i] for i in train]\n",
    "        train_y = [train_tag_array[i] for i in train]\n",
    "        \n",
    "        test_x =[train_features[i] for i in test]\n",
    "        test_y = [train_tag_array[i] for i in test]\n",
    "        \n",
    "        for x, y in zip(train_x,train_y):\n",
    "            model.append(x,y)\n",
    "        \n",
    "        #train the model\n",
    "        if grid_search:\n",
    "            model.set_params({'c1': l1,'c2': l2,'max_iterations': iters,'feature.possible_transitions': True})\n",
    "                \n",
    "                \n",
    "            crf = sklearn_crfsuite.CRF(algorithm='lbfgs',c1=l1,c2=l2,max_iterations=iters,all_possible_transitions=False)\n",
    "            \n",
    "            params_space = {\n",
    "                'c1': scipy.stats.expon(scale=0.5),\n",
    "                'c2': scipy.stats.expon(scale=0.05),\n",
    "            }\n",
    "            \n",
    "            # use the same metric for evaluation\n",
    "            f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                                    average='weighted', labels=labels)\n",
    "\n",
    "\n",
    "            # search\n",
    "            rs = RandomizedSearchCV(crf, params_space,\n",
    "                                    cv=3,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=-1,\n",
    "                                    n_iter=50,\n",
    "                                    scoring=f1_scorer)\n",
    "            rs.fit(train_x, train_y)\n",
    "            info = rs.best_estimator_.tagger_.info()\n",
    "            tagger = rs.best_estimator_.tagger_\n",
    "        else:\n",
    "            model.set_params({\n",
    "                'c1': l1,   # coefficient for L1 penalty\n",
    "                'c2': l2,  # coefficient for L2 penalty\n",
    "                'max_iterations': iters,  # stop earlier\n",
    "\n",
    "                # include transitions that are possible, but not observed\n",
    "                'feature.possible_transitions': True\n",
    "            })\n",
    "            model_name = modelname + '_model {}'.format(fold_idx)\n",
    "            print('training model...')\n",
    "            model.train(model_name)\n",
    "            print('done...')\n",
    "            tagger = pycrfsuite.Tagger()\n",
    "            tagger.open(model_name)\n",
    "\n",
    "            info = tagger.info()\n",
    "    \n",
    "        # a quick peak of the model \n",
    "        def print_transitions(trans_features):\n",
    "            for (label_from, label_to), weight in trans_features:\n",
    "                print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "        print(\"Top likely transitions:\")\n",
    "        print_transitions(Counter(info.transitions).most_common(80))\n",
    "\n",
    "        print(\"\\nTop unlikely transitions:\")\n",
    "        print_transitions(Counter(info.transitions).most_common()[-80:])\n",
    "\n",
    "        def print_state_features(state_features):\n",
    "            for (attr, label), weight in state_features:\n",
    "                print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "        print(\"Top positive:\")\n",
    "        print_state_features(Counter(info.state_features).most_common(80))\n",
    "\n",
    "        print(\"\\nTop negative:\")\n",
    "        print_state_features(Counter(info.state_features).most_common()[-80:])\n",
    "        \n",
    "        print(\"parameters\")\n",
    "        model.get_params()\n",
    "\n",
    "        \n",
    "        #make predictions \n",
    "        test_pred = []\n",
    "        \n",
    "\n",
    "        for i,  (x, y) in enumerate(zip(test_x, test_y)):\n",
    "            \n",
    "            pred_labels = tagger.tag(x)\n",
    "            test_pred.append(pred_labels)\n",
    "\n",
    "            \n",
    "        # compute evaluation metrics    \n",
    "        fold_precision,fold_recall, fold_f1 = eval_abstracts(test_y, test_pred)\n",
    "        \n",
    "        recall_scores.append(fold_recall)\n",
    "        precision_scores.append(fold_precision)\n",
    "        f1_scores.append(fold_f1_score)\n",
    "\n",
    "        fold_recall_results = \"Fold recall: {}\".format(fold_recall)\n",
    "        fold_precision_results = \"Fold precision: {}\".format(fold_precision)\n",
    "        fold_f1_results = \"Fold F1 Score: {}\".format(fold_f1_score)\n",
    "        print fold_recall_results\n",
    "        print fold_precision_results\n",
    "        print fold_f1_results\n",
    "\n",
    "        file = open(modelname + '_results.txt', 'w+')\n",
    "\n",
    "        file.write(fold_recall_results + '\\n')\n",
    "        file.write(fold_precision_results + '\\n')\n",
    "        file.write(fold_f1_results + '\\n')\n",
    "\n",
    "       \n",
    "    recall_average = np.mean(recall_scores)\n",
    "    precision_average = np.mean(precision_scores)\n",
    "    f1_scores = np.mean(f1_scores)\n",
    "\n",
    "    print \"Recall Average: {}\".format(recall_average)\n",
    "    print \"Precision Average: {}\".format(precision_average)\n",
    "    print \"F1 Average: {}\".format(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train data\n",
    "train_tokens, train_tags = get_all_data_train()\n",
    "train_genia_tags = get_genia_tags('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dev data\n",
    "dev_tokens, dev_tags = get_all_data_dev()\n",
    "dev_genia_tags = get_genia_tags('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_options_string = 'left_neighbors=1 right_neighbors=0 inside_paren pos chunk iob named_entity \\\n",
    "inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=10 cosine_simil cosine_simil_neighbors \\\n",
    "isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "small_options_string = 'left_neighbors=0 right_neighbors=0 one_hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = abstracts2features(train_tokens, train_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_features = abstracts2features(dev_tokens, dev_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "sanity_check(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_crf(train_features, train_tags, 100, 0.1, 0.1, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get model from file\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "print_model_info(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_dev_tags = predict_tags(tagger, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'curr_pred_tags' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f8f925e9092b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_abstracts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_dev_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/natsothanaphan/Desktop/Courses/6.806/project/6.864-project/evaluation.py\u001b[0m in \u001b[0;36meval_abstracts\u001b[0;34m(all_gold_tags, all_pred_tags)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_gold_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mcurr_gold_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_gold_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mcurr_pred_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_pred_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mp_tokens_extracted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tokens_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_true_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_abstract_token_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_gold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_pred_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'curr_pred_tags' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Problem with this somehow\n",
    "eval_abstracts(pred_dev_tags, dev_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on fold 0\n",
      "loading data...\n",
      "Adding data\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 58.4min finished\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'curr_pred_tags' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-0f5751ec4817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_crf_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Base_crf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-c7436a8e0c4d>\u001b[0m in \u001b[0;36mtrain_crf_kfold\u001b[0;34m(features_list, tags_list, num_iters, l1, l2, grid_search, file_name, n_folds)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# compute evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mfold_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_abstracts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mrecall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_recall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natsothanaphan/Desktop/Courses/6.806/project/6.864-project/evaluation.py\u001b[0m in \u001b[0;36meval_abstracts\u001b[0;34m(all_gold_tags, all_pred_tags)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_gold_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mcurr_gold_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_gold_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mcurr_pred_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_pred_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mp_tokens_extracted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tokens_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_true_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_abstract_token_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_gold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_pred_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'curr_pred_tags' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "train_crf_kfold(train_features, train_tags, 100, 0.1, 0.1, True, 'Base_crf')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
