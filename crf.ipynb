{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-Chain CRF\n",
    "\n",
    "pycrfsuite version \n",
    "source: https://github.com/bwallace/Deep-PICO/blob/3152ab3690cad1b6e369be8a8aac27393811341c/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from features_generator import abstracts2features\n",
    "\n",
    "from preprocess_data import get_all_data_train, get_all_data_dev, get_all_data_test\n",
    "from gensim.models import Word2Vec\n",
    "from features_generator import abstracts2features\n",
    "from features_generator import get_genia_tags\n",
    "from sklearn_crfsuite import metrics\n",
    "import pycrfsuite\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_options_string = 'left_neighbors=3 right_neighbors=3 inside_paren pos chunk iob named_entity \\\n",
    "inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "one_hot one_hot_neighbors w2v=False cosine_simil cosine_simil_neighbors \\\n",
    "isupper isupper_neighbors istitle istitle_neighbors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tokens, tag_array = get_all_data_test()\n",
    "train_genia_tags = get_genia_tags('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['for', 'Management', 'Severe', 'Chronic', 'Low', 'Back', 'Pain'], ['monotherapy', 'versus', 'tapentadol', 'PR/pregabalin', 'combination', 'therapy', 'for', 'severe', 'chronic', 'low', 'back', 'pain', 'with', 'neuropathic', 'component', '.', 'METHODS', 'Eligible', 'patients', 'painDETECT', '``', 'unclear', \"''\", 'or', '``', 'positive', \"''\", 'ratings'], ['PR/pregabalin', '(', '27.0', '%', ';', 'P\\xc2\\xa0=\\xc2\\xa00.0302', ')', '.', 'CONCLUSIONS']]\n",
      "[('None', 'Effectiveness'), ('None', 'and'), ('None', 'Safety'), ('None', 'of'), ('None', 'Tapentadol'), ('None', 'Prolonged'), ('None', 'Release'), ('None', '('), ('None', 'PR'), ('None', ')'), ('None', 'Versus'), ('None', 'a'), ('None', 'Combination'), ('None', 'of'), ('None', 'Tapentadol'), ('None', 'PR'), ('None', 'and'), ('None', 'Pregabalin'), ('Pb', 'for'), ('Pm', 'the'), ('Pm', 'Management'), ('Pm', 'of'), ('Pm', 'Severe'), ('Pm', ','), ('Pm', 'Chronic'), ('Pm', 'Low'), ('Pm', 'Back'), ('Pm', 'Pain'), ('None', 'With'), ('None', 'a'), ('None', 'Neuropathic'), ('None', 'Component'), ('None', ':'), ('None', 'A'), ('None', 'Randomized'), ('None', ','), ('None', 'Double-blind'), ('None', ','), ('None', 'Phase'), ('None', '3b'), ('None', 'Study'), ('None', '.'), ('None', 'OBJECTIVE'), ('None', 'To'), ('None', 'evaluate'), ('None', 'the'), ('None', 'effectiveness'), ('None', 'and'), ('None', 'tolerability'), ('None', 'of'), ('None', 'tapentadol'), ('None', 'PR'), ('Pb', 'monotherapy'), ('Pm', 'versus'), ('Pm', 'tapentadol'), ('Pm', 'PR/pregabalin'), ('Pm', 'combination'), ('Pm', 'therapy'), ('Pb', 'for'), ('Pm', 'severe'), ('Pm', ','), ('Pm', 'chronic'), ('Pm', 'low'), ('Pm', 'back'), ('Pm', 'pain'), ('Pm', 'with'), ('Pm', 'a'), ('Pm', 'neuropathic'), ('Pm', 'component'), ('Pm', '.'), ('Pm', 'METHODS'), ('Pm', 'Eligible'), ('Pm', 'patients'), ('Pm', 'had'), ('Pm', 'painDETECT'), ('Pm', '``'), ('Pm', 'unclear'), ('Pm', \"''\"), ('Pm', 'or'), ('Pm', '``'), ('Pm', 'positive'), ('Pm', \"''\"), ('Pm', 'ratings'), ('None', 'and'), ('None', 'average'), ('None', 'pain'), ('None', 'intensity'), ('None', '\\xe2\\x89\\xa5\\xc2\\xa06'), ('None', '('), ('None', '11-point'), ('None', 'NRS-3'), ('None', '['), ('None', 'average'), ('None', '3-day'), ('None', 'pain'), ('None', 'intensity'), ('None', ']'), ('None', ')'), ('None', 'at'), ('None', 'baseline.'), ('None', 'Patients'), ('None', 'were'), ('None', 'titrated'), ('None', 'to'), ('None', 'tapentadol'), ('None', 'PR'), ('None', '300\\xc2\\xa0mg/day'), ('None', 'over'), ('None', '3\\xc2\\xa0weeks.'), ('None', 'Patients'), ('None', 'with'), ('None', '\\xe2\\x89\\xa5\\xc2\\xa01-point'), ('None', 'decrease'), ('None', 'in'), ('None', 'pain'), ('None', 'intensity'), ('None', 'and'), ('None', 'average'), ('None', 'pain'), ('None', 'intensity'), ('None', '\\xe2\\x89\\xa5\\xc2\\xa04'), ('None', 'were'), ('None', 'randomized'), ('None', 'to'), ('None', 'tapentadol'), ('None', 'PR'), ('None', '('), ('None', '500\\xc2\\xa0mg/day'), ('None', ')'), ('None', 'or'), ('None', 'tapentadol'), ('None', 'PR'), ('None', '('), ('None', '300\\xc2\\xa0mg/day'), ('None', ')'), ('None', '/pregabalin'), ('None', '('), ('None', '300\\xc2\\xa0mg/day'), ('None', ')'), ('None', 'during'), ('None', 'an'), ('None', '8-week'), ('None', 'comparative'), ('None', 'period'), ('None', '.'), ('None', 'RESULTS'), ('None', 'In'), ('None', 'the'), ('None', 'per-protocol'), ('None', 'population'), ('None', '('), ('None', 'n\\xc2\\xa0=\\xc2\\xa0288'), ('None', ')'), ('None', ','), ('None', 'the'), ('None', 'effectiveness'), ('None', 'of'), ('None', 'tapentadol'), ('None', 'PR'), ('None', 'was'), ('None', 'clinically'), ('None', 'and'), ('None', 'statistically'), ('None', 'comparable'), ('None', 'to'), ('None', 'tapentadol'), ('None', 'PR/pregabalin'), ('None', 'based'), ('None', 'on'), ('None', 'the'), ('None', 'change'), ('None', 'in'), ('None', 'pain'), ('None', 'intensity'), ('None', 'from'), ('None', 'randomization'), ('None', 'to'), ('None', 'final'), ('None', 'evaluation'), ('None', '('), ('None', 'LOCF'), ('None', ';'), ('None', 'LSMD'), ('None', '['), ('None', '95'), ('None', '%'), ('None', 'CI'), ('None', ']'), ('None', ','), ('None', '-0.066'), ('None', '['), ('None', '-0.57'), ('None', ','), ('None', '0.43'), ('None', ']'), ('None', ';'), ('None', 'P\\xc2\\xa0'), ('None', '<'), ('None', '\\xc2\\xa00.0001'), ('None', 'for'), ('None', 'noninferiority'), ('None', ')'), ('None', '.'), ('None', 'Neuropathic'), ('None', 'pain'), ('None', 'and'), ('None', 'quality-of-life'), ('None', 'measures'), ('None', 'improved'), ('None', 'significantly'), ('None', 'in'), ('None', 'both'), ('None', 'groups.'), ('None', 'Tolerability'), ('None', 'was'), ('None', 'good'), ('None', 'in'), ('None', 'both'), ('None', 'groups'), ('None', ','), ('None', 'in'), ('None', 'line'), ('None', 'with'), ('None', 'prior'), ('None', 'trials'), ('None', 'in'), ('None', 'the'), ('None', 'high'), ('None', 'dose'), ('None', 'range'), ('None', 'of'), ('None', '500\\xc2\\xa0mg/day'), ('None', 'for'), ('None', 'tapentadol'), ('None', 'PR'), ('None', 'monotherapy'), ('None', ','), ('None', 'and'), ('None', 'favorable'), ('None', 'compared'), ('None', 'with'), ('None', 'historical'), ('None', 'combination'), ('None', 'trials'), ('None', 'of'), ('None', 'strong'), ('None', 'opioids'), ('None', 'and'), ('None', 'anticonvulsants'), ('None', 'for'), ('None', 'combination'), ('None', 'therapy.'), ('None', 'The'), ('None', 'incidence'), ('None', 'of'), ('None', 'the'), ('None', 'composite'), ('None', 'of'), ('None', 'dizziness'), ('None', 'and/or'), ('None', 'somnolence'), ('None', 'was'), ('None', 'significantly'), ('None', 'lower'), ('None', 'with'), ('None', 'tapentadol'), ('None', 'PR'), ('None', '('), ('None', '16.9'), ('None', '%'), ('None', ')'), ('None', 'than'), ('None', 'tapentadol'), ('Pb', 'PR/pregabalin'), ('Pm', '('), ('Pm', '27.0'), ('Pm', '%'), ('Pm', ';'), ('Pm', 'P\\xc2\\xa0=\\xc2\\xa00.0302'), ('Pm', ')'), ('Pm', '.'), ('Pm', 'CONCLUSIONS'), ('None', 'Tapentadol'), ('None', 'PR'), ('None', '500\\xc2\\xa0mg'), ('None', 'is'), ('None', 'associated'), ('None', 'with'), ('None', 'comparable'), ('None', 'improvements'), ('None', 'in'), ('None', 'pain'), ('None', 'intensity'), ('None', 'and'), ('None', 'quality-of-life'), ('None', 'measures'), ('None', 'to'), ('None', 'tapentadol'), ('None', 'PR'), ('None', '300\\xc2\\xa0mg/pregabalin'), ('None', '300\\xc2\\xa0mg'), ('None', ','), ('None', 'with'), ('None', 'improved'), ('None', 'central'), ('None', 'nervous'), ('None', 'system'), ('None', 'tolerability'), ('None', ','), ('None', 'suggesting'), ('None', 'that'), ('None', 'tapentadol'), ('None', 'PR'), ('None', 'monotherapy'), ('None', 'may'), ('None', 'offer'), ('None', 'a'), ('None', 'favorable'), ('None', 'treatment'), ('None', 'option'), ('None', 'for'), ('None', 'severe'), ('None', 'low'), ('None', 'back'), ('None', 'pain'), ('None', 'with'), ('None', 'a'), ('None', 'neuropathic'), ('None', 'component'), ('None', '.')]\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "\n",
    "\"\"\"\n",
    "Evaluate at label type level. \n",
    "For each abstrast,\n",
    "return a list of list of words that have the\n",
    "label in interest\n",
    "\"\"\"\n",
    "\n",
    "def output2words(labels, words,label_type='P'):\n",
    "    \n",
    "    predicted_mention = []\n",
    "    predicted_mentions = []\n",
    "    \n",
    "    \"\"\" Do we need to add or remove any stop_words\"\"\"\n",
    "    stop_words = ['a', 'an', 'the', 'of', 'had', 'group', 'groups', 'arm', ',']\n",
    "\n",
    "    mention = True\n",
    "    for label, word in zip(labels, words):\n",
    "        if label_type in label:\n",
    "            if word not in stop_words:\n",
    "                predicted_mention.append(word)\n",
    "                mention = True\n",
    "        else:\n",
    "            if mention:\n",
    "                mention = False\n",
    "                if len(predicted_mention) == 0:\n",
    "                    continue\n",
    "                predicted_mentions.append(predicted_mention)\n",
    "                predicted_mention = []\n",
    "\n",
    "    return predicted_mentions\n",
    "\n",
    "if DEBUG:\n",
    "    print output2words(tag_array[3],train_tokens[3],label_type='P')\n",
    "    print zip(tag_array[3],train_tokens[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3969849246231156, 0.5771689497716895, 0.4704131001116487)\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "\n",
    "\"\"\"\n",
    "Need to think about the logic to \n",
    "compare two lists of lists \n",
    "\"\"\"\n",
    "\n",
    "def evaluate_scores(predicted_mentions, true_mentions):\n",
    "    false_positives = 0\n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "    mentions = {}\n",
    "    overlap = False\n",
    "\n",
    "    for abs_pred, true_pred in zip(predicted_mentions, true_mentions):\n",
    "\n",
    "        for mention in abs_pred:\n",
    "            already_overlapped = False\n",
    "\n",
    "            for true_mention in true_pred:\n",
    "                intersection = list(set(mention) & set(true_mention))\n",
    "\n",
    "                # Annotated mentions that do not match detected mentions are considered to be false negatives.\n",
    "                if len(intersection) > 0:\n",
    "                    # A detected mention is considered a match for an annotated mention if they consist of the same set\n",
    "                    # of words or if the detected mention\n",
    "                    #  overlaps the annotated one and the overlap is not a symbol or stop word\n",
    "                    # If a detected mention overlaps multiple annotated mentions, it is considered to be a false positive\n",
    "\n",
    "                    if already_overlapped:\n",
    "                        false_positives += 1\n",
    "                    else:\n",
    "\n",
    "                        true_positives += 1\n",
    "\n",
    "                    already_overlapped = True\n",
    "                else:\n",
    "                    false_negatives += 1\n",
    "\n",
    "\n",
    "    #print \"false negatives: {}\".format(false_negatives)\n",
    "    #print \"true postitives: {}\".format(true_positives)\n",
    "    if not (true_positives + false_negatives) == 0:\n",
    "        recall = float(true_positives)/float((true_positives + false_negatives))\n",
    "    else:\n",
    "        recall = 0\n",
    "        print 'Error: divide by zero default to 0 for recall '\n",
    "    if not true_positives + false_positives == 0:\n",
    "        precision = float(true_positives) / float(true_positives + false_positives)\n",
    "    else:\n",
    "        precision = 0\n",
    "        print 'Error: divide by zero default to 0 for precision'\n",
    "\n",
    "    if not precision + recall == 0:\n",
    "        f1_score = float(2 * precision * recall) / float(precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "        print 'Error: divide by zero default to 0 for f1'\n",
    "\n",
    "\n",
    "    return recall, precision, f1_score\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    pred_mentions = []\n",
    "    actual_mentions = []\n",
    "    for i,(label, token) in enumerate(zip(tag_array,train_tokens)):\n",
    "        pred = output2words(label,token,label_type='P')\n",
    "        actual = output2words(label,token,label_type='P')\n",
    "        pred_mentions.append(pred)\n",
    "        actual_mentions.append(actual)\n",
    "        \n",
    "    \n",
    "    print evaluate_scores(pred_mentions, actual_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crf(l2,l1,iters,grid_search,modelname,train_tokens,train_tag_array, train_genia_tags,default_options_string):\n",
    "    \n",
    "    #get training data\n",
    "    train_features = abstracts2features(train_tokens, train_genia_tags, default_options_string)\n",
    "    \n",
    "    # set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose = False)\n",
    "    n = len(train_tokens)\n",
    "    n_fold= 5\n",
    "    kf = KFold(n ,random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "    \n",
    "    recall_scores=[]\n",
    "    precision_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for fold_idx, (train,test) in enumerate(kf):\n",
    "        print(\"on fold %s\" % fold_idx)\n",
    "        print('loading data...')\n",
    "        train_x =[train_features[i] for i in train]\n",
    "        train_y = [train_tag_array[i] for i in train]\n",
    "        \n",
    "        test_x =[train_features[i] for i in test]\n",
    "        test_y = [train_tag_array[i] for i in test]\n",
    "        \n",
    "        for x, y in zip(train_x,train_y):\n",
    "            model.append(x,y)\n",
    "        \n",
    "        #train the model\n",
    "        if grid_search:\n",
    "            model.set_params({'c1': l1,'c2': l2,'max_iterations': iters,'feature.possible_transitions': True})\n",
    "                \n",
    "                \n",
    "            crf = sklearn_crfsuite.CRF(algorithm='lbfgs',c1=l1,c2=l2,max_iterations=iters,all_possible_transitions=False)\n",
    "            \n",
    "            params_space = {\n",
    "                'c1': scipy.stats.expon(scale=0.5),\n",
    "                'c2': scipy.stats.expon(scale=0.05),\n",
    "            }\n",
    "\n",
    "            # use the same metric for evaluation\n",
    "            f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                                    average='weighted', labels=test_y)\n",
    "\n",
    "\n",
    "            # search\n",
    "            rs = RandomizedSearchCV(crf, params_space,\n",
    "                                    cv=3,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=-1,\n",
    "                                    n_iter=50,\n",
    "                                    scoring=f1_scorer)\n",
    "            rs.fit(train_x, train_y)\n",
    "            info = rs.best_estimator_.tagger_.info()\n",
    "            tagger = rs.best_estimator_.tagger_\n",
    "        else:\n",
    "            model.set_params({\n",
    "                'c1': l1,   # coefficient for L1 penalty\n",
    "                'c2': l2,  # coefficient for L2 penalty\n",
    "                'max_iterations': iters,  # stop earlier\n",
    "\n",
    "                # include transitions that are possible, but not observed\n",
    "                'feature.possible_transitions': True\n",
    "            })\n",
    "            model_name = name + '_model {}'.format(fold_idx)\n",
    "            print('training model...')\n",
    "            model.train(model_name)\n",
    "            print('done...')\n",
    "            tagger = pycrfsuite.Tagger()\n",
    "            tagger.open(model_name)\n",
    "\n",
    "            info = tagger.info()\n",
    "    \n",
    "        # a quick peak of the model \n",
    "        def print_transitions(trans_features):\n",
    "            for (label_from, label_to), weight in trans_features:\n",
    "                print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "        print(\"Top likely transitions:\")\n",
    "        print_transitions(Counter(info.transitions).most_common(80))\n",
    "\n",
    "        print(\"\\nTop unlikely transitions:\")\n",
    "        print_transitions(Counter(info.transitions).most_common()[-80:])\n",
    "\n",
    "        def print_state_features(state_features):\n",
    "            for (attr, label), weight in state_features:\n",
    "                print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "        print(\"Top positive:\")\n",
    "        print_state_features(Counter(info.state_features).most_common(80))\n",
    "\n",
    "        print(\"\\nTop negative:\")\n",
    "        print_state_features(Counter(info.state_features).most_common()[-80:])\n",
    "\n",
    "        \n",
    "        #make predictions \n",
    "        abstract_predicted_mentions, true_abstract_mentions = [], []\n",
    "        \n",
    "\n",
    "        for i,  (x, y) in enumerate(zip(test_x, test_y)):\n",
    "            \n",
    "            # get the idx of the abstract \n",
    "            abstract_id = test[i]\n",
    "            abstract_tokens =  train_tokens[abstract_id]\n",
    "\n",
    "            pred_labels = tagger.tag(x)\n",
    "            pred_mentions = output2words(pred_labels, abstract_tokens)\n",
    "            true_mentions = output2words(y, abstract_tokens)\n",
    "\n",
    "            print \"Predicted: {}\".format(pred_mentions)\n",
    "            print \"True: {}\".format(true_mentions)\n",
    "            print '\\n'\n",
    "            abstract_predicted_mentions.append(pred_mentions)\n",
    "            true_abstract_mentions.append(true_mentions)\n",
    "            \n",
    "        # compute evaluation metrics    \n",
    "        fold_recall, fold_precision, fold_f1_score = evaluate_scores(abstract_predicted_mentions, true_abstract_mentions)\n",
    "        recall_scores.append(fold_recall)\n",
    "        precision_scores.append(fold_precision)\n",
    "        f1_scores.append(fold_f1_score)\n",
    "\n",
    "        fold_recall_results = \"Fold recall: {}\".format(fold_recall)\n",
    "        fold_precision_results = \"Fold precision: {}\".format(fold_precision)\n",
    "        fold_f1_results = \"Fold F1 Score: {}\".format(fold_f1_score)\n",
    "        print fold_recall_results\n",
    "        print fold_precision_results\n",
    "        print fold_f1_results\n",
    "\n",
    "        file = open(model_name + '_results.txt', 'w+')\n",
    "\n",
    "        file.write(fold_recall_results + '\\n')\n",
    "        file.write(fold_precision_results + '\\n')\n",
    "        file.write(fold_f1_results + '\\n')\n",
    "\n",
    "       \n",
    "    recall_average = np.mean(recall_scores)\n",
    "    precision_average = np.mean(precision_scores.mean)\n",
    "    f1_scores = np.mean(f1_scores)\n",
    "\n",
    "    print \"Recall Average: {}\".format(recall_average)\n",
    "    print \"Precision Average: {}\".format(precision_average)\n",
    "    print \"F1 Average: {}\".format(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " crf(0,0,10,True,'Init',train_tokens,tag_array, train_genia_tags,default_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
