{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-Chain CRF\n",
    "\n",
    "pycrfsuite version \n",
    "source: https://github.com/bwallace/Deep-PICO/blob/3152ab3690cad1b6e369be8a8aac27393811341c/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "from preprocess_data import get_all_data_train, get_all_data_dev, get_all_data_test\n",
    "from features_generator import abstracts2features, get_genia_tags, sanity_check\n",
    "from evaluation import eval_abstracts, eval_abstracts_avg\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn_crfsuite import metrics\n",
    "import pycrfsuite\n",
    "import sklearn_crfsuite\n",
    "import scipy\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CRF\n",
    "_INPUT_:\n",
    "- features_list: list of list of features dictionaries\n",
    "- tags_list: list of list of tags\n",
    "- num_iters: number of iterations\n",
    "- l1, l2: regularization parameters\n",
    "- file_name: file name to write model out; '.model' added automatically\n",
    "\n",
    "_OUTPUT_:\n",
    "- The trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_crf(features_list, tags_list, num_iters, l1, l2, file_name=''):\n",
    "    # Set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose=False)\n",
    "    model.set_params({\n",
    "        'c1': l1,  # Coefficient for L1 penalty\n",
    "        'c2': l2,  # Coefficient for L2 penalty\n",
    "        'max_iterations': num_iters,\n",
    "\n",
    "        # Include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    \n",
    "    print 'Adding data...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for i in range(len(tags_list)):\n",
    "        model.append(features_list[i], tags_list[i])\n",
    "\n",
    "    print 'Training model...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    model.train(file_name + '.model')\n",
    "    print 'Done!'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tagger\n",
    "Get tagger which opens file_name ('.model' added automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tagger(file_name):\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(file_name + '.model')\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print model info\n",
    "_INPUT_:\n",
    "- tagger: pycrfsuite.Tagger class (need to open model with it first)\n",
    "- num_items: number of top positive/negative state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_model_info(tagger, num_items=20):\n",
    "    # A quick peak of the model\n",
    "    info = tagger.info()\n",
    "\n",
    "    def print_transitions(trans_features):\n",
    "        for (label_from, label_to), weight in trans_features:\n",
    "            print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "    print(\"Top likely transitions:\")\n",
    "    print_transitions(Counter(info.transitions).most_common())\n",
    "\n",
    "    def print_state_features(state_features):\n",
    "        for (attr, label), weight in state_features:\n",
    "            print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "    print(\"\\nTop positive:\")\n",
    "    print_state_features(Counter(info.state_features).most_common(num_items))\n",
    "\n",
    "    print(\"\\nTop negative:\")\n",
    "    print_state_features(Counter(info.state_features).most_common()[-num_items:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict tags\n",
    "_INPUT_:\n",
    "- tagger: pycrfsuite.Tagger class (need to open model with it first)\n",
    "- features_list: list of list of features dictionaries\n",
    "\n",
    "_OUTPUT_:\n",
    "- List of list of predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_tags(tagger, features_list):\n",
    "    # Make predictions \n",
    "    pred_tags_list = []\n",
    "\n",
    "    for features in features_list:\n",
    "        pred_tags = tagger.tag(features)\n",
    "        pred_tags_list.append(pred_tags)\n",
    "    \n",
    "    return pred_tags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count tags\n",
    "_INPUT_:\n",
    "- pred_tags_list: list of list of predicted tags\n",
    "- gold_tags_list: list of list of gold tags\n",
    "- tag_name: tag name to count (e.g. 'P')\n",
    "\n",
    "_OUTPUT_:\n",
    "- Number of tags with tag name in predicted tags, gold tags, and intersection of both, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def count_tags(pred_tags_list, gold_tags_list, tag_name):\n",
    "    num_pred_tags = 0\n",
    "    num_gold_tags = 0\n",
    "    num_both_tags = 0\n",
    "    \n",
    "    if len(pred_tags_list) != len(gold_tags_list):\n",
    "        raise ValueError('pred_tags_list has length ' + str(len(pred_tags_list)) + \\\n",
    "                         ', while gold_tags_list has length ' + str(len(gold_tags_list)))\n",
    "    \n",
    "    for i in range(len(gold_tags_list)):\n",
    "        pred_tags = pred_tags_list[i]\n",
    "        gold_tags = gold_tags_list[i]\n",
    "        \n",
    "        if len(pred_tags) != len(gold_tags):\n",
    "            raise ValueError('pred_tags_list[{}] has length {}, while gold_tags_list[{}] has length {}'\\\n",
    "                             .format(i, len(pred_tags), i, len(gold_tags)))\n",
    "        \n",
    "        for j in range(len(gold_tags)):\n",
    "            if gold_tags[j] == tag_name:\n",
    "                num_gold_tags += 1\n",
    "                \n",
    "                if pred_tags[j] == tag_name:\n",
    "                    num_both_tags += 1\n",
    "                    num_pred_tags += 1\n",
    "            elif pred_tags[j] == tag_name:\n",
    "                num_pred_tags += 1\n",
    "\n",
    "    return num_pred_tags, num_gold_tags, num_both_tags\n",
    "\n",
    "if DEBUG:\n",
    "    gold_tags_list = [['None', 'P', 'None'], ['P', 'P', 'None', 'None']]\n",
    "    pred_tags_list = [['P', 'P', 'None'], ['P', 'None', 'None', 'P']]\n",
    "    \n",
    "    print count_tags(pred_tags_list, gold_tags_list, 'P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "_INPUT_:\n",
    "- Number of predicted tags, num of gold tags, number of tags predicted correctly\n",
    "\n",
    "_OUTPUT_:\n",
    "- Precision, recall, f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def metrics(num_pred_tags, num_gold_tags, num_both_tags):\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    if num_both_tags > num_pred_tags:\n",
    "        raise ValueError('num_both_tags = {} is greater than num_pred_tags = {}'\\\n",
    "                         .format(num_both_tags, num_pred_tags))\n",
    "    if num_both_tags > num_gold_tags:\n",
    "        raise ValueError('num_both_tags = {} is greater than num_gold_tags = {}'\\\n",
    "                         .format(num_both_tags, num_gold_tags))\n",
    "    \n",
    "    if num_pred_tags != 0:\n",
    "        precision = float(num_both_tags)/num_pred_tags\n",
    "        \n",
    "    if num_gold_tags != 0:\n",
    "        recall = float(num_both_tags)/num_gold_tags\n",
    "    \n",
    "    if precision != 0 and recall != 0:\n",
    "        f1 = 2/(1/precision + 1/recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "if DEBUG:\n",
    "    print metrics(3,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_crf_kfold(features_list, tags_list, num_iters, l1, l2, file_name='', save=False, n_folds=5):\n",
    "    # Set up the KFold\n",
    "    num_abstracts = len(tags_list)\n",
    "\n",
    "    kf = KFold(num_abstracts, random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "\n",
    "    recall_scores=[]\n",
    "    precision_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    labels = set(tags_list[0])\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf):\n",
    "        print 'On fold %s' % fold_idx\n",
    "\n",
    "        train_x = [features_list[i] for i in train_indices]\n",
    "        train_y = [tags_list[i] for i in train_indices]\n",
    "\n",
    "        test_x = [features_list[i] for i in test_indices]\n",
    "        test_y = [tags_list[i] for i in test_indices]\n",
    "\n",
    "        # Train model\n",
    "        model_name = file_name + '_fold{}'.format(fold_idx)\n",
    "        model = train_crf(train_x, train_y, num_iters, l1, l2, model_name)\n",
    "\n",
    "        # Get tagger\n",
    "        tagger = get_tagger(model_name)\n",
    "\n",
    "        # Make predictions\n",
    "        test_pred = predict_tags(tagger, test_x)\n",
    "\n",
    "        # compute evaluation metrics\n",
    "        num_pred_all = 0\n",
    "        num_gold_all = 0\n",
    "        num_both_all = 0\n",
    "\n",
    "        fold_result = {}\n",
    "\n",
    "        #label level metrics\n",
    "        for l in labels:\n",
    "            if l != 'None':\n",
    "                num_pred, num_gold, num_both = count_tags(test_pred, test_y, l)\n",
    "\n",
    "                num_pred_all += num_pred\n",
    "                num_gold_all += num_gold\n",
    "                num_both_all += num_both\n",
    "\n",
    "                fold_p, fold_r, fold_f1 = metrics(num_pred, num_gold, num_both)\n",
    "                fold_result[l] = [fold_p, fold_r, fold_f1]\n",
    "\n",
    "        # overal metrics\n",
    "        fold_p_overall, fold_r_overall, fold_f1_overall = metrics(num_pred_all, num_gold_all, num_both_all)\n",
    "        fold_result['Overall'] = [fold_p_overall, fold_r_overall, fold_f1_overall]\n",
    "\n",
    "        precision_scores.append(fold_p_overall)\n",
    "        recall_scores.append(fold_r_overall)\n",
    "        f1_scores.append(fold_f1_overall)\n",
    "\n",
    "\n",
    "        fold_precision_results = \"Fold precision: {}\".format(fold_p_overall)\n",
    "        fold_recall_results = \"Fold recall: {}\".format(fold_r_overall)\n",
    "        fold_f1_results = \"Fold F1 Score: {}\".format(fold_f1_overall)\n",
    "        print fold_recall_results\n",
    "        print fold_precision_results\n",
    "        print fold_f1_results\n",
    "\n",
    "        if save:\n",
    "            f = open(file_name + '_results.txt', 'w+')\n",
    "            pickle.dump(fold_result, f)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "    precision_average = np.mean(precision_scores)\n",
    "    recall_average = np.mean(recall_scores)\n",
    "    f1_scores = np.mean(f1_scores)\n",
    "\n",
    "\n",
    "    print \"Recall Average: {}\".format(recall_average)\n",
    "    print \"Precision Average: {}\".format(precision_average)\n",
    "    print \"F1 Average: {}\".format(f1_scores)\n",
    "\n",
    "    return precision_average,recall_average,f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(features_list, tags_list, num_iters, l1_list, l2_list, file_name='',save=False,n_folds=5):\n",
    "    result = {}\n",
    "    for l1 in l1_list:\n",
    "        for l2 in l2_list:\n",
    "            p, r, f1 = train_crf_kfold(features_list, tags_list, num_iters, l1, l2, file_name,save=save,n_folds=n_folds)\n",
    "            result[l1,l2] = [p, r, f1]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train data\n",
    "train_tokens, train_tags = get_all_data_train()\n",
    "train_genia_tags = get_genia_tags('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dev data\n",
    "dev_tokens, dev_tags = get_all_data_dev()\n",
    "dev_genia_tags = get_genia_tags('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_options_string = 'left_neighbors=1 right_neighbors=0 inside_paren pos chunk iob named_entity \\\n",
    "inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=10 cosine_simil cosine_simil_neighbors \\\n",
    "isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "small_options_string = 'left_neighbors=0 right_neighbors=0 one_hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499: ['Pulsed', 'azithromycin', 'treatment']\n"
     ]
    }
   ],
   "source": [
    "train_features = abstracts2features(train_tokens, train_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999: ['Serum', 'bactericidal', 'activities']\n"
     ]
    }
   ],
   "source": [
    "dev_features = abstracts2features(dev_tokens, dev_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "sanity_check(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_crf(train_features, train_tags, 100, 0.1, 0.1, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get model from file\n",
    "tagger = get_tagger('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "print_model_info(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_dev_tags = predict_tags(tagger, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_pred, num_gold, num_both = count_tags(pred_dev_tags, dev_tags, 'P')\n",
    "p, r, f1 = metrics(num_pred, num_gold, num_both)\n",
    "print num_pred, num_gold, num_both\n",
    "print \"Precision:\", p, \"Recall:\", r, \"F1:\", f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.402249983648\n",
      "Fold precision: 0.65488233415\n",
      "Fold F1 Score: 0.498379254457\n",
      "On fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.421791648783\n",
      "Fold precision: 0.662844478705\n",
      "Fold F1 Score: 0.515531906921\n",
      "On fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.394653403792\n",
      "Fold precision: 0.674601487779\n",
      "Fold F1 Score: 0.497979996078\n",
      "On fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.428396871945\n",
      "Fold precision: 0.670043000478\n",
      "Fold F1 Score: 0.522640032795\n",
      "On fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.417293958695\n",
      "Fold precision: 0.679660484124\n",
      "Fold F1 Score: 0.517101171968\n",
      "Recall Average: 0.412877173373\n",
      "Precision Average: 0.668406357047\n",
      "F1 Average: 0.510326472444\n",
      "--- 87.6969969273 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "train_crf_kfold(train_features, train_tags, 100, 0.1, 0.1, 'Base_crf')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a['a','b']=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b'): 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
