{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-Chain CRF\n",
    "\n",
    "pycrfsuite version \n",
    "source: https://github.com/bwallace/Deep-PICO/blob/3152ab3690cad1b6e369be8a8aac27393811341c/crf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "from preprocess_data import get_all_data_train, get_all_data_dev, get_all_data_test\n",
    "from features_generator import abstracts2features, get_genia_tags, sanity_check\n",
    "from evaluation import eval_abstracts, eval_abstracts_avg\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn_crfsuite import metrics\n",
    "import pycrfsuite\n",
    "import sklearn_crfsuite\n",
    "import scipy\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_crf(features_list, tags_list, num_iters, l1, l2, file_name=''):\n",
    "    # Set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose=False)\n",
    "    model.set_params({\n",
    "        'c1': l1,  # Coefficient for L1 penalty\n",
    "        'c2': l2,  # Coefficient for L2 penalty\n",
    "        'max_iterations': num_iters,  # Stop earlier\n",
    "\n",
    "        # Include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    \n",
    "    print 'Adding data...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for i in range(len(tags_list)):\n",
    "        model.append(features_list[i], tags_list[i])\n",
    "\n",
    "    print 'Training model...'\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    model.train(file_name)\n",
    "    print 'Done!'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_model_info(tagger, num_items=20):\n",
    "    # A quick peak of the model\n",
    "    info = tagger.info()\n",
    "\n",
    "    def print_transitions(trans_features):\n",
    "        for (label_from, label_to), weight in trans_features:\n",
    "            print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "    print(\"Top likely transitions:\")\n",
    "    print_transitions(Counter(info.transitions).most_common())\n",
    "\n",
    "    def print_state_features(state_features):\n",
    "        for (attr, label), weight in state_features:\n",
    "            print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "    print(\"\\nTop positive:\")\n",
    "    print_state_features(Counter(info.state_features).most_common(num_items))\n",
    "\n",
    "    print(\"\\nTop negative:\")\n",
    "    print_state_features(Counter(info.state_features).most_common()[-num_items:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_tags(tagger, features_list):\n",
    "    # Make predictions \n",
    "    pred_tags_list = []\n",
    "\n",
    "    for features in features_list:\n",
    "        pred_tags = tagger.tag(features)\n",
    "        pred_tags_list.append(pred_tags)\n",
    "    \n",
    "    return pred_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "def evaluate(pred_tags_list, gold_tags_list, tag_name):\n",
    "    num_pred_tags = 0\n",
    "    num_gold_tags = 0\n",
    "    num_both_tags = 0\n",
    "    \n",
    "    for i in range(len(gold_tags_list)):\n",
    "        gold_tags = gold_tags_list[i]\n",
    "        pred_tags = pred_tags_list[i]\n",
    "        \n",
    "        for j in range(len(gold_tags)):\n",
    "            if gold_tags[j] == tag_name:\n",
    "                num_gold_tags += 1\n",
    "                \n",
    "                if pred_tags[j] == tag_name:\n",
    "                    num_both_tags += 1\n",
    "                    num_pred_tags += 1\n",
    "            elif pred_tags[j] == tag_name:\n",
    "                num_pred_tags += 1\n",
    "\n",
    "    return num_pred_tags, num_gold_tags, num_both_tags\n",
    "\n",
    "if DEBUG:\n",
    "    gold_tags_list = [['None', 'P', 'None'], ['P', 'P', 'None', 'None']]\n",
    "    pred_tags_list = [['P', 'P', 'None'], ['P', 'None', 'None', 'P']]\n",
    "    \n",
    "    print evaluate(pred_tags_list, gold_tags_list, tag_name='P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "def metrics(num_pred_tags, num_gold_tags, num_both_tags):\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    if num_pred_tags != 0:\n",
    "        precision = float(num_both_tags)/num_pred_tags\n",
    "        \n",
    "    if num_gold_tags != 0:\n",
    "        recall = float(num_both_tags)/num_gold_tags\n",
    "    \n",
    "    if precision != 0 and recall != 0:\n",
    "        f1 = 2/(1/precision + 1/recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "if DEBUG:\n",
    "    print metrics(3,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_crf_kfold(features_list, tags_list, num_iters, l1, l2, tag_name, file_name='', n_folds=5):\n",
    "    # Set up the KFold\n",
    "    num_abstracts = len(tags_list)\n",
    "\n",
    "    kf = KFold(num_abstracts, random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "\n",
    "    recall_scores=[]\n",
    "    precision_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    labels = set(tags_list[0])\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf):\n",
    "        print 'on fold %s' % fold_idx\n",
    "\n",
    "        train_x = [features_list[i] for i in train_indices]\n",
    "        train_y = [tags_list[i] for i in train_indices]\n",
    "\n",
    "        test_x = [features_list[i] for i in test_indices]\n",
    "        test_y = [tags_list[i] for i in test_indices]\n",
    "\n",
    "        #train models\n",
    "        model_name = file_name + '_model {}'.format(fold_idx)\n",
    "        model =train_crf(train_x, train_y, num_iters, l1, l2, model_name)\n",
    "        \n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(model_name)\n",
    "\n",
    "        #make predictions\n",
    "        test_pred = predict_tags(tagger, test_x)\n",
    "\n",
    "        # compute evaluation metrics\n",
    "        num_pred, num_gold, num_both = evaluate(test_pred, test_y, 'P')\n",
    "        fold_precision,fold_recall, fold_f1_score = metrics(num_pred, num_gold, num_both)\n",
    "\n",
    "        recall_scores.append(fold_recall)\n",
    "        precision_scores.append(fold_precision)\n",
    "        f1_scores.append(fold_f1_score)\n",
    "\n",
    "        fold_recall_results = \"Fold recall: {}\".format(fold_recall)\n",
    "        fold_precision_results = \"Fold precision: {}\".format(fold_precision)\n",
    "        fold_f1_results = \"Fold F1 Score: {}\".format(fold_f1_score)\n",
    "        print fold_recall_results\n",
    "        print fold_precision_results\n",
    "        print fold_f1_results\n",
    "\n",
    "        file = open(file_name + '_results.txt', 'w+')\n",
    "\n",
    "        file.write(fold_recall_results + '\\n')\n",
    "        file.write(fold_precision_results + '\\n')\n",
    "        file.write(fold_f1_results + '\\n')\n",
    "\n",
    "\n",
    "    recall_average = np.mean(recall_scores)\n",
    "    precision_average = np.mean(precision_scores)\n",
    "    f1_scores = np.mean(f1_scores)\n",
    "\n",
    "    print \"Recall Average: {}\".format(recall_average)\n",
    "    print \"Precision Average: {}\".format(precision_average)\n",
    "    print \"F1 Average: {}\".format(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crf(l2,l1,iters,grid_search,modelname,train_features,train_tag_array):\n",
    "    \n",
    "    # set up the model parameters \n",
    "    model = pycrfsuite.Trainer(verbose = False)\n",
    "    n = len(train_tag_array)\n",
    "    n_folds= 5\n",
    "    kf = KFold(n ,random_state=1234, shuffle=True, n_folds=n_folds)\n",
    "    \n",
    "    recall_scores=[]\n",
    "    precision_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    labels = set(train_tag_array[0])\n",
    "    \n",
    "    for fold_idx, (train,test) in enumerate(kf):\n",
    "        print(\"on fold %s\" % fold_idx)\n",
    "        print('loading data...')\n",
    "        train_x =[train_features[i] for i in train]\n",
    "        train_y = [train_tag_array[i] for i in train]\n",
    "        \n",
    "        test_x =[train_features[i] for i in test]\n",
    "        test_y = [train_tag_array[i] for i in test]\n",
    "        \n",
    "        for x, y in zip(train_x,train_y):\n",
    "            model.append(x,y)\n",
    "        \n",
    "        #train the model\n",
    "        if grid_search:\n",
    "            model.set_params({'c1': l1,'c2': l2,'max_iterations': iters,'feature.possible_transitions': True})\n",
    "                \n",
    "                \n",
    "            crf = sklearn_crfsuite.CRF(algorithm='lbfgs',c1=l1,c2=l2,max_iterations=iters,all_possible_transitions=False)\n",
    "            \n",
    "            params_space = {\n",
    "                'c1': scipy.stats.expon(scale=0.5),\n",
    "                'c2': scipy.stats.expon(scale=0.05),\n",
    "            }\n",
    "            \n",
    "            # use the same metric for evaluation\n",
    "            f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                                    average='weighted', labels=labels)\n",
    "\n",
    "\n",
    "            # search\n",
    "            rs = RandomizedSearchCV(crf, params_space,\n",
    "                                    cv=3,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=-1,\n",
    "                                    n_iter=50,\n",
    "                                    scoring=f1_scorer)\n",
    "            rs.fit(train_x, train_y)\n",
    "            info = rs.best_estimator_.tagger_.info()\n",
    "            tagger = rs.best_estimator_.tagger_\n",
    "        else:\n",
    "            model.set_params({\n",
    "                'c1': l1,   # coefficient for L1 penalty\n",
    "                'c2': l2,  # coefficient for L2 penalty\n",
    "                'max_iterations': iters,  # stop earlier\n",
    "\n",
    "                # include transitions that are possible, but not observed\n",
    "                'feature.possible_transitions': True\n",
    "            })\n",
    "            model_name = modelname + '_model {}'.format(fold_idx)\n",
    "            print('training model...')\n",
    "            model.train(model_name)\n",
    "            print('done...')\n",
    "            tagger = pycrfsuite.Tagger()\n",
    "            tagger.open(model_name)\n",
    "\n",
    "            info = tagger.info()\n",
    "    \n",
    "        # a quick peak of the model \n",
    "        def print_transitions(trans_features):\n",
    "            for (label_from, label_to), weight in trans_features:\n",
    "                print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "        print(\"Top likely transitions:\")\n",
    "        print_transitions(Counter(info.transitions).most_common(80))\n",
    "\n",
    "        print(\"\\nTop unlikely transitions:\")\n",
    "        print_transitions(Counter(info.transitions).most_common()[-80:])\n",
    "\n",
    "        def print_state_features(state_features):\n",
    "            for (attr, label), weight in state_features:\n",
    "                print(\"%0.6f %-6s %s\" % (weight, label, attr))\n",
    "\n",
    "        print(\"Top positive:\")\n",
    "        print_state_features(Counter(info.state_features).most_common(80))\n",
    "\n",
    "        print(\"\\nTop negative:\")\n",
    "        print_state_features(Counter(info.state_features).most_common()[-80:])\n",
    "        \n",
    "        print(\"parameters\")\n",
    "        model.get_params()\n",
    "\n",
    "        \n",
    "        #make predictions \n",
    "        test_pred = []\n",
    "        \n",
    "\n",
    "        for i,  (x, y) in enumerate(zip(test_x, test_y)):\n",
    "            \n",
    "            pred_labels = tagger.tag(x)\n",
    "            test_pred.append(pred_labels)\n",
    "\n",
    "            \n",
    "        # compute evaluation metrics    \n",
    "        fold_precision,fold_recall, fold_f1 = eval_abstracts(test_y, test_pred)\n",
    "        \n",
    "        recall_scores.append(fold_recall)\n",
    "        precision_scores.append(fold_precision)\n",
    "        f1_scores.append(fold_f1_score)\n",
    "\n",
    "        fold_recall_results = \"Fold recall: {}\".format(fold_recall)\n",
    "        fold_precision_results = \"Fold precision: {}\".format(fold_precision)\n",
    "        fold_f1_results = \"Fold F1 Score: {}\".format(fold_f1_score)\n",
    "        print fold_recall_results\n",
    "        print fold_precision_results\n",
    "        print fold_f1_results\n",
    "\n",
    "        file = open(modelname + '_results.txt', 'w+')\n",
    "\n",
    "        file.write(fold_recall_results + '\\n')\n",
    "        file.write(fold_precision_results + '\\n')\n",
    "        file.write(fold_f1_results + '\\n')\n",
    "\n",
    "       \n",
    "    recall_average = np.mean(recall_scores)\n",
    "    precision_average = np.mean(precision_scores)\n",
    "    f1_scores = np.mean(f1_scores)\n",
    "\n",
    "    print \"Recall Average: {}\".format(recall_average)\n",
    "    print \"Precision Average: {}\".format(precision_average)\n",
    "    print \"F1 Average: {}\".format(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train data\n",
    "train_tokens, train_tags = get_all_data_train()\n",
    "train_genia_tags = get_genia_tags('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dev data\n",
    "dev_tokens, dev_tags = get_all_data_dev()\n",
    "dev_genia_tags = get_genia_tags('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_options_string = 'left_neighbors=1 right_neighbors=0 inside_paren pos chunk iob named_entity \\\n",
    "inside_paren_neighbors pos_neighbors chunk_neighbors iob_neighbors named_entity_neighbors \\\n",
    "chunk_end chunk_end_neighbors same_chunk_neighbors \\\n",
    "one_hot one_hot_neighbors w2v_model=pubmed w2v w2v_neighbors w2v_size=10 cosine_simil cosine_simil_neighbors \\\n",
    "isupper isupper_neighbors istitle istitle_neighbors'\n",
    "\n",
    "small_options_string = 'left_neighbors=0 right_neighbors=0 one_hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = abstracts2features(train_tokens, train_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_features = abstracts2features(dev_tokens, dev_genia_tags, w2v=None, options_string=small_options_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "sanity_check(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_crf(train_features, train_tags, 100, 0.1, 0.1, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get model from file\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For debug\n",
    "print_model_info(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_dev_tags = predict_tags(tagger, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_pred, num_gold, num_both = evaluate(pred_dev_tags, dev_tags, 'P')\n",
    "p, r, f1 = metrics(num_pred, num_gold, num_both)\n",
    "print num_pred, num_gold, num_both\n",
    "print \"Precision:\", p, \"Recall:\", r, \"F1:\", f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on fold 0\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.402249983648\n",
      "Fold precision: 0.65488233415\n",
      "Fold F1 Score: 0.498379254457\n",
      "on fold 1\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.421791648783\n",
      "Fold precision: 0.662844478705\n",
      "Fold F1 Score: 0.515531906921\n",
      "on fold 2\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.394653403792\n",
      "Fold precision: 0.674601487779\n",
      "Fold F1 Score: 0.497979996078\n",
      "on fold 3\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.428396871945\n",
      "Fold precision: 0.670043000478\n",
      "Fold F1 Score: 0.522640032795\n",
      "on fold 4\n",
      "Adding data...\n",
      "Training model...\n",
      "Done!\n",
      "Fold recall: 0.417293958695\n",
      "Fold precision: 0.679660484124\n",
      "Fold F1 Score: 0.517101171968\n",
      "Recall Average: 0.412877173373\n",
      "Precision Average: 0.668406357047\n",
      "F1 Average: 0.510326472444\n",
      "--- 80.889747858 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "train_crf_kfold(train_features, train_tags, 100, 0.1, 0.1, 'Base_crf')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
