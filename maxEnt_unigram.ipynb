{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from preprocess_data import get_all_data_train\n",
    "from preprocess_data import get_all_data_dev\n",
    "from preprocess_data import get_all_data_test\n",
    "from evaluation import eval_abstracts_avg\n",
    "from evaluation import eval_abstracts\n",
    "from evaluation import evaluate_abstract_PRF1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Hot features: Function to return X and Y from words and tags.\n",
    "X only contains W_t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf1_1hot_get_X_Y(words, tags, dict_vectorizer=None):\n",
    "    dict_list = []\n",
    "    Y = []\n",
    "    for sentance_index in range(0, len(words)):\n",
    "        sentance = words[sentance_index]\n",
    "        tag_list = tags[sentance_index]\n",
    "        for word_ind in range(0, len(sentance)):\n",
    "            d = {}\n",
    "            d['word'] = sentance[word_ind];\n",
    "#             d.update(get_dict_extra_features(sentance[word_ind]))\n",
    "            dict_list.append(d)\n",
    "            Y.append(tag_list[word_ind])\n",
    "            \n",
    "    if dict_vectorizer == None:\n",
    "        dict_vectorizer = DictVectorizer()\n",
    "        X = dict_vectorizer.fit_transform(dict_list)\n",
    "        return [X, Y, dict_vectorizer]\n",
    "    else:\n",
    "#         print dict_list\n",
    "        X = dict_vectorizer.transform(dict_list)\n",
    "        return [X, Y, dict_vectorizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier clf1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_clf1(X_train, Y_train, c=1.0):\n",
    "    clf1 = LogisticRegression(random_state=123, C=c)\n",
    "    clf1.fit(X_train,Y_train)\n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict tags using clf1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_tags_clf1(clf1, X):\n",
    "    Y_pred = clf1.predict(X)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.253475292703\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "words_tr, tags_tr = get_all_data_train()\n",
    "words_dev, tags_dev = get_all_data_dev()\n",
    "max_f1 = 0.0\n",
    "best_reg = None\n",
    "for reg_param in [1.0, 3.0, 5.0, 7.0, 10.0]:\n",
    "    [X_train, Y_train, dict_vectorizer] = clf1_1hot_get_X_Y(words_tr, tags_tr)\n",
    "    clf1 = train_clf1(X_train, Y_train, reg_param)\n",
    "    [X_dev, Y_dev, dict_vectorizer] = clf1_1hot_get_X_Y(words_dev, tags_dev, dict_vectorizer)\n",
    "    Y_pred_dev = predict_tags_clf1(clf1, X_dev)\n",
    "    P, R, F1 = evaluate_abstract_PRF1(Y_dev, Y_pred_dev)\n",
    "    if (F1 > max_f1):\n",
    "        max_f1 = F1\n",
    "        best_reg = reg_param\n",
    "print max_f1\n",
    "print best_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "Pre 0.706711,  rec 0.209978,  f1 0.323760\n",
      "dev: \n",
      "Pre 0.580807,  rec 0.162112,  f1 0.253475\n",
      "test: \n",
      "Pre 0.579103,  rec 0.160742,  f1 0.251637\n"
     ]
    }
   ],
   "source": [
    "words_tr, tags_tr = get_all_data_train()\n",
    "[X_train, Y_train, dict_vectorizer] = clf1_1hot_get_X_Y(words_tr, tags_tr)\n",
    "clf1 = train_clf1(X_train, Y_train, 7.0)\n",
    "Y_pred_tr = predict_tags_clf1(clf1, X_train)\n",
    "print \"train: \"\n",
    "P, R, F1 = evaluate_abstract_PRF1(Y_train, Y_pred_tr)\n",
    "print \"Pre {:f},  rec {:f},  f1 {:f}\".format(P, R, F1)\n",
    "\n",
    "words_dev, tags_dev = get_all_data_dev()\n",
    "[X_dev, Y_dev, dict_vectorizer] = clf1_1hot_get_X_Y(words_dev, tags_dev, dict_vectorizer)\n",
    "Y_pred_dev = predict_tags_clf1(clf1, X_dev)\n",
    "print \"dev: \"\n",
    "P, R, F1 = evaluate_abstract_PRF1(Y_dev, Y_pred_dev)\n",
    "print \"Pre {:f},  rec {:f},  f1 {:f}\".format(P, R, F1)\n",
    "\n",
    "words_test, tags_test = get_all_data_test()\n",
    "[X_test, Y_test, dict_vectorizer] = clf1_1hot_get_X_Y(words_test, tags_test, dict_vectorizer)\n",
    "Y_pred_test = predict_tags_clf1(clf1, X_test)\n",
    "print \"test: \"\n",
    "P, R, F1 = evaluate_abstract_PRF1(Y_test, Y_pred_test)\n",
    "print \"Pre {:f},  rec {:f},  f1 {:f}\".format(P, R, F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
