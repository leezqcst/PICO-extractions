{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feautres:\n",
    "   1. Features based on the token itself:\n",
    "   \n",
    "       1. actual token (one-hot-encoding or word2vec (http://bio.nlplab.org/#word-vectors)\n",
    "       2. POS tag (geniatagger or ntlk)\n",
    "       3. inside parentheses or not\n",
    "       4. Named Entity by geniatagger\n",
    "       5. Prefixes or suffixes?\n",
    "      \n",
    "   2. Features based on the phrase containing the token: (geniatagger -'chunker')\n",
    "   \n",
    "       1. the type of phrase\n",
    "       2. whether it is the first or last token in the phrase\n",
    "       3. UMLS semantic type ? (https://semanticnetwork.nlm.nih.gov/)\n",
    "       \n",
    "   3. Features based on the four nearest tokens on each side of the token in question:\n",
    "   \n",
    "      1. tokens themselves\n",
    "      2. their POS\n",
    "      3. whether each token is in the same phrase as the token in question\n",
    "      4. semantic tags ??\n",
    "   \n",
    "Additional features after tokenization:\n",
    "\n",
    "1. Semantic tags tagged manually for words include people or measurements\n",
    "    1. People: *people, participants, subjects, men, women, children, patient*  \n",
    "    2. Meaurement: *length, volumen, weight, etc.\n",
    "    \n",
    "2. Semantic tags from Word-Net (https://wordnet.princeton.edu/)\n",
    "\n",
    "    \n",
    "\n",
    "source: Automatic summarization of results from clinical trials; An Introduction to Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geniatagger import GeniaTagger\n",
    "from preprocess_data import get_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory for geniatagger\n",
    "genia_directory = 'geniatagger-3.0.2/geniatagger'\n",
    "\n",
    "tagger = GeniaTagger(genia_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all data\n",
    "# Format of word_array: [[word1, word2, ...], ...]\n",
    "word_array, tag_array = get_all_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print elements of a list with spaces\n",
    "# Element l[i] is padded to have length space[i]\n",
    "def print_with_spaces(l, spaces):\n",
    "    # This pads strings to be of space length and aligned left\n",
    "    formatter = lambda space: '{:' + str(space) + '}'\n",
    "    \n",
    "    print ''.join([formatter(space).format(string) for string, space in zip(l, spaces)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "'''\n",
    "Clean up genia tags\n",
    "\n",
    "INPUT: \n",
    "- abstract (word array from prepocessing)\n",
    "- genia_tags (output of genia parser)\n",
    "- label (tag array from preprocessing)\n",
    "\n",
    "OUTPUT:\n",
    "- abstract_mod (word array that match with the output of genia parser)\n",
    "- clean tags\n",
    "    - a list of tuples\n",
    "    - Features: pos,chunk_clean,iob,named_entity, whether word inside parentheses.\n",
    "- label_mod (label array that match with the output of genia parser)\n",
    "'''\n",
    "\n",
    "\n",
    "def clean_tags(abstract, genia_tags, label):\n",
    "    cleaned_tags = []\n",
    "    abstract_mod = []\n",
    "    label_mod =[]\n",
    "    miss_match=False\n",
    "    # Keep track of whether word is inside parantheses\n",
    "    inside_paren = False\n",
    "    \n",
    "    idx=0\n",
    "\n",
    "    for word, base_form, pos, chunk, named_entity in genia_tags:\n",
    "        # ';' has POS ':'\n",
    "        if word == pos or pos == ':' or pos == '(' or pos == ')' or word == '%':\n",
    "            # This means the word is puctuation, parentheses, etc.,\n",
    "            # so we do not make features for it.\n",
    "            if pos == '(':\n",
    "                inside_paren = True\n",
    "            elif pos == ')':\n",
    "                inside_paren = False\n",
    "            elif len(word) > 1 and word not in ['``',  '\\'\\'', '--', 'TO', '...']:\n",
    "                # This shouldn't happen\n",
    "                raise ValueError('Unidentified word: ' + word)\n",
    "            \n",
    "        # Strip out IOB from chunk\n",
    "        if chunk == 'O':\n",
    "            chunk_clean=chunk\n",
    "        elif len(chunk) > 2:\n",
    "            chunk_clean=chunk[2:]\n",
    "        else:\n",
    "            raise ValueError('Unidentified chunk: ' + chunk)\n",
    "        \n",
    "        # Get IOB\n",
    "        iob = chunk[0]\n",
    "        if iob != 'O' and iob != 'I' and iob != 'B':\n",
    "            raise ValueError('Unidentified chunk:s ' + chunk)\n",
    "        \n",
    "        # Strip out IOB from named_entity\n",
    "        if named_entity == 'O':\n",
    "            ne_clean=named_entity\n",
    "        elif len(named_entity) > 2:\n",
    "            ne_clean=named_entity[2:]\n",
    "        else:\n",
    "            raise ValueError('Unidentified named entity: ' + named_entity)\n",
    "\n",
    "        \n",
    "        cleaned_tags.append((pos,chunk_clean,iob,named_entity,inside_paren))\n",
    "\n",
    "        #recreate the abstract based on the parser\n",
    "        abstract_mod.append(word)\n",
    "        \n",
    "        #recreate label for each word\n",
    "        org_word = abstract[idx]\n",
    "        # update index of the word in old word list if match\n",
    "        if word == org_word:\n",
    "            label_mod.append(label[idx])\n",
    "            idx +=1\n",
    "            miss_match=False\n",
    "        #if the previous word is not matched\n",
    "        elif miss_match:\n",
    "            if word[-1] == org_word[-1]:\n",
    "                label_mod.append(label[idx])\n",
    "                idx+=1\n",
    "                miss_match=False\n",
    "            elif word in org_word:\n",
    "                label_mod.append(label[idx])\n",
    "            else:\n",
    "                print word \n",
    "                print org_word\n",
    "                print \"Error at word\"+ str(idx)\n",
    "        #check if the word match partially\n",
    "        elif word == org_word[:len(word)]: \n",
    "            label_mod.append(label[idx])\n",
    "            miss_match=True\n",
    "        else:\n",
    "            print word \n",
    "            print org_word\n",
    "            print \"Error at word\"+ str(idx)\n",
    "\n",
    "    return abstract_mod, cleaned_tags, label_mod\n",
    "\n",
    "if DEBUG:\n",
    "    test = ' '. join (word_array[4])\n",
    "    genia_tags=tagger.parse(test)\n",
    "    abstract_mod,cleaned_tags,label_mod = clean_tags(word_array[4], genia_tags, tag_array[4])\n",
    "    print zip(word_array[1][68:90],tag_array[1][68:90] )    \n",
    "    print zip(abstract_mod[78:103],label_mod[78:103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "'''\n",
    "Get features for a word at index word_i,\n",
    "where d indicates the distance between word_i\n",
    " and the word that we want to create features for,\n",
    " i.e. the owner of feature_dict\n",
    "\n",
    "INPUT:\n",
    "- word_i: index of the word that we want to extract its own features\n",
    "- d: distance from the word that we want to create feature dictionary for \n",
    "- abstract: the word array for the abstract the word is in\n",
    "- tagged_abs: the list of tuples from clean_tags\n",
    "- feature_dict: the dictionary that we want to store all the features for the word\n",
    "- direction: whether the word_i is before or after the word\n",
    "- w2v: whether we want to use word2vect\n",
    "- w2v_size: the size of the w2v\n",
    "\n",
    "OUTPUT:\n",
    "- feature_dict\n",
    "\n",
    "source:\n",
    "https://github.com/bwallace/Deep-PICO/blob/master/crf.py\n",
    "'''\n",
    "\n",
    "def tags2features(word_i, d, abstract, tagged_abs, feature_dict, direction, w2v, w2v_size=100):\n",
    "     \n",
    "    if direction == 'before':\n",
    "        position = '-'\n",
    "    elif direction == 'after':\n",
    "        position = '+'\n",
    "    else:\n",
    "        position = ''\n",
    "    \n",
    "    word = abstract[word_i]\n",
    "    pos, chunk, iob , named_entity, inside_paren = tagged_abs[word_i]\n",
    "    \n",
    "    #add features to the feature dict\n",
    "    feature_dict['word[{}{}]'.format(position, d)] = word\n",
    "    #pos tag\n",
    "    feature_dict['pos[{}{}]'.format(position, d)] = pos\n",
    "    # type of phrase\n",
    "    feature_dict['chunk[{}{}]'.format(position, d)] = chunk\n",
    "    # location of the word in a phrase\n",
    "    feature_dict['chunkiob[{}{}]'.format(position, d)] = iob    \n",
    "    #Named Entity\n",
    "    feature_dict['ne[{}{}]'.format(position, d)] = named_entity\n",
    "    #Inside parentheses\n",
    "    feature_dict['inside_paren[{}{}]'.format(position, d)] = str(inside_paren)\n",
    "    \n",
    "    #Whether the word is all capitalized\n",
    "    feature_dict['isupper[{}{}]'.format(position, d)] = word.isupper()\n",
    "    feature_dict['istitle[{}{}]'.format(position, d)] = word.istitle()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print feature_dict\n",
    "        print word, tagged_abs[word_i]\n",
    "    \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that extracts features of neigbor words\n",
    "with a certain window size\n",
    "add more features to the existing feature dictionary\n",
    "\n",
    "INPUT:\n",
    "- n_before, n_after: the size of the window that we are interested in\n",
    "- word_i: index of the word that we want to create feature dictionary for \n",
    "- abstract: the word array for the abstract the word is in\n",
    "- tagged_abs: the list of tuples from clean_tags\n",
    "- feature_dict: the dictionary that we want to store all the features for the word\n",
    "- w2v: whether we want to use word2vect\n",
    "- w2v_size: the size of the w2v\n",
    "\n",
    "OUTPUT:\n",
    "- feature_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def tags2features_window(n_before,n_after,word_i,abstract, tagged_abs, feature_dict, w2v, w2v_size=100):\n",
    "    for d in range(1,n_before+1):\n",
    "        #for the word that do not have enough words before, \n",
    "        #they will just do not have as many features\n",
    "        if word_i-d >=0:\n",
    "            feature_dict = tags2features(word_i-d, d, abstract, tagged_abs, feature_dict, 'before', w2v, w2v_size=w2v_size)\n",
    "    for d in range(1,n_after+1):\n",
    "        #for the word that do not have enough words before, \n",
    "        #they will just do not have as many features\n",
    "        if word_i+d <(len(abstract)-1):\n",
    "            feature_dict = tags2features(word_i+d, d, abstract, tagged_abs, feature_dict, 'after', w2v, w2v_size=w2v_size)\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "'''\n",
    "Get features for all abstracts and return\n",
    "an array X that contains the features for all abtracts\n",
    "and array Y for the corresponding labels\n",
    "\n",
    "INPUT:\n",
    "- abstract_list: word_array from prepocessing\n",
    "- tag_list: labels from prepocessing\n",
    "- window: (n_before,n_after) the number of neighbors that we want to consider\n",
    "- w2v: whether we want to use word2vect\n",
    "\n",
    "OUTPUT:\n",
    "- X: [[list of features dictionary for each word in abstract 1], \n",
    "[list of features dictionary for each word in abstract 2 ], ...]\n",
    "- Y: [[labels for abstract 1], [lables for abstract 2],...]\n",
    "\n",
    "'''\n",
    "def abstracts2features(abstract_list,tag_list,window,w2v, w2v_size=100):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    n_before = window[0]\n",
    "    n_after= window[1]\n",
    "    for i in range(len(abstract_list)):\n",
    "        abstract_txt = ' '. join (abstract_list[i])\n",
    "        label = tag_list[i]\n",
    "        \n",
    "        if DEBUG:\n",
    "            if not abstract_txt.startswith('Association of efavirenz'):\n",
    "                continue\n",
    "            print abstract\n",
    "        \n",
    "        if DISPLAY:\n",
    "            # Print progress\n",
    "            print '\\r{0}: {1}'.format(i, abstract_txt[:30]),\n",
    "    \n",
    "        # Get tags from genia tagger\n",
    "        # Format: [(Association, Association, NN, B-NP, O), ...]\n",
    "        # Visualization here: http://nactem7.mib.man.ac.uk/geniatagger/\n",
    "        genia_tags = tagger.parse(abstract_txt)\n",
    "\n",
    "        '''Step 1: Clean up genia tags'''\n",
    "\n",
    "        abstract,tagged_abs,labels = clean_tags(abstract_list[i], genia_tags, label)\n",
    "\n",
    "        '''Step 2: Get features the abstract'''\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for i, word in enumerate(abstract):\n",
    "            feature_dict = {}\n",
    "            \n",
    "            #get features for the current word\n",
    "            feature_dict = tags2features(i, 0,abstract ,tagged_abs, feature_dict, '', w2v,w2v_size=w2v_size)\n",
    "            #get features for the neighbor words\n",
    "            feature_dict = tags2features_window(n_before,n_after,i,abstract, tagged_abs, feature_dict, w2v, w2v_size=w2v_size)\n",
    "            \n",
    "            features.append(feature_dict)    \n",
    "        X.append(features)\n",
    "        Y.append(labels)\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "if DEBUG:\n",
    "    X,Y = abstracts2features(word_array[1:10],tag_array[1:10],(1,1),False, w2v_size=100)\n",
    "    print X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8: [A randomized trial of PVB, VA\n"
     ]
    }
   ],
   "source": [
    "X,Y = abstracts2features(word_array[1:10],tag_array[1:10],(1,1),False, w2v_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
