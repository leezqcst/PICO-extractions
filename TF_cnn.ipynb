{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess_data import get_all_data_train\n",
    "from TF_preprocess_data import get_1_hot_abstract_encodings\n",
    "from text_cnn import TextCNN\n",
    "import datetime\n",
    "# import data_helpers\n",
    "import time\n",
    "import os\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# dennybritz's Sentance classification using cnn\n",
    "# https://github.com/dennybritz/cnn-text-classification-tf\n",
    "# njl's Sentiment Analysis example \n",
    "# https://github.com/nicholaslocascio/tensorflow-nlp-tutorial/blob/master/sentiment-analysis/Sentiment-RNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "tf.InteractiveSession \n",
    "with tf.Session() as sess:\n",
    "    matrix1 = tf.constant(2.0)\n",
    "    print(sess.run(matrix1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_array, tag_array = get_all_data_train()\n",
    "X, Y = get_1_hot_abstract_encodings(word_array, tag_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_document_length = len(X[0])\n",
    "# vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "\n",
    "# # Randomly shuffle data\n",
    "# np.random.seed(10)\n",
    "# shuffle_indices = np.random.permutation(np.arange(len(Y)))\n",
    "# x_shuffled = X[shuffle_indices]\n",
    "# y_shuffled = Y[shuffle_indices]\n",
    "\n",
    "# # Split train/test set\n",
    "# # TODO: This is very crude, should use cross-validation\n",
    "# dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(Y)))\n",
    "# x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "# y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "# print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "# print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36901"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sub_X) for sub_X in X]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479442842\n",
      "\n",
      "2016-11-17T23:20:45.049768: step 1, loss 138.92, pre 0.0260461, rec 0.489264, f1 0.0494593\n",
      "2016-11-17T23:20:46.137925: step 2, loss 155.83, pre 0.0271973, rec 0.456731, f1 0.0513375\n",
      "2016-11-17T23:20:47.038420: step 3, loss 139.012, pre 0.0248875, rec 0.46349, f1 0.0472385\n",
      "2016-11-17T23:20:47.864358: step 4, loss 148.704, pre 0.0278896, rec 0.491404, f1 0.0527834\n",
      "2016-11-17T23:20:48.798999: step 5, loss 160.937, pre 0.0296884, rec 0.481113, f1 0.0559257\n",
      "2016-11-17T23:20:49.759797: step 6, loss 146.849, pre 0.026265, rec 0.465066, f1 0.0497218\n",
      "2016-11-17T23:20:50.677560: step 7, loss 160.402, pre 0.0288121, rec 0.471372, f1 0.0543049\n",
      "2016-11-17T23:20:51.535279: step 8, loss 159.559, pre 0.027922, rec 0.458082, f1 0.0526356\n",
      "2016-11-17T23:20:52.441710: step 9, loss 162.015, pre 0.0279558, rec 0.452743, f1 0.0526599\n",
      "2016-11-17T23:20:53.361357: step 10, loss 175.291, pre 0.0306571, rec 0.459707, f1 0.0574809\n",
      "2016-11-17T23:20:54.333284: step 11, loss 144.972, pre 0.0265676, rec 0.476085, f1 0.0503267\n",
      "2016-11-17T23:20:55.137694: step 12, loss 154.868, pre 0.0281892, rec 0.474845, f1 0.053219\n",
      "2016-11-17T23:20:55.989350: step 13, loss 168.87, pre 0.0306656, rec 0.477876, f1 0.0576329\n",
      "2016-11-17T23:20:56.893543: step 14, loss 152.84, pre 0.0271634, rec 0.462937, f1 0.0513158\n",
      "2016-11-17T23:20:57.782129: step 15, loss 138.006, pre 0.0234023, rec 0.444444, f1 0.0444634\n",
      "2016-11-17T23:20:58.910648: step 16, loss 161.037, pre 0.0278986, rec 0.452128, f1 0.0525543\n",
      "2016-11-17T23:20:59.771112: step 17, loss 154.015, pre 0.0275259, rec 0.465649, f1 0.0519792\n",
      "2016-11-17T23:21:00.655744: step 18, loss 167.76, pre 0.0288991, rec 0.452107, f1 0.0543257\n",
      "2016-11-17T23:21:01.732504: step 19, loss 164.499, pre 0.0286452, rec 0.454782, f1 0.0538957\n",
      "2016-11-17T23:21:02.636924: step 20, loss 149.604, pre 0.0279597, rec 0.486467, f1 0.0528801\n",
      "2016-11-17T23:21:03.511160: step 21, loss 160.977, pre 0.0284437, rec 0.465116, f1 0.053609\n",
      "2016-11-17T23:21:04.603566: step 22, loss 167.808, pre 0.0301682, rec 0.470401, f1 0.0567\n",
      "2016-11-17T23:21:05.571107: step 23, loss 142.745, pre 0.0260862, rec 0.471952, f1 0.0494398\n",
      "2016-11-17T23:21:06.446446: step 24, loss 178.306, pre 0.0314867, rec 0.461308, f1 0.0589498\n",
      "2016-11-17T23:21:07.351329: step 25, loss 166.647, pre 0.0299201, rec 0.470513, f1 0.0562625\n",
      "2016-11-17T23:21:08.347861: step 26, loss 168.487, pre 0.0302052, rec 0.468611, f1 0.0567523\n",
      "2016-11-17T23:21:09.211175: step 27, loss 152.863, pre 0.027214, rec 0.462238, f1 0.0514017\n",
      "2016-11-17T23:21:10.144728: step 28, loss 184.916, pre 0.0321744, rec 0.452808, f1 0.0600799\n",
      "2016-11-17T23:21:11.049851: step 29, loss 158.678, pre 0.028791, rec 0.476447, f1 0.0543007\n",
      "2016-11-17T23:21:12.205251: step 30, loss 156.391, pre 0.0275961, rec 0.461696, f1 0.0520793\n",
      "2016-11-17T23:21:13.022712: step 31, loss 182.63, pre 0.0321137, rec 0.457845, f1 0.0600177\n",
      "2016-11-17T23:21:13.844257: step 32, loss 146.427, pre 0.0255069, rec 0.45614, f1 0.0483122\n",
      "2016-11-17T23:21:14.690685: step 33, loss 166.135, pre 0.0291787, rec 0.462331, f1 0.054893\n",
      "2016-11-17T23:21:15.532998: step 34, loss 156.476, pre 0.0274337, rec 0.458647, f1 0.0517707\n",
      "2016-11-17T23:21:16.384372: step 35, loss 160.996, pre 0.028772, rec 0.467817, f1 0.0542099\n",
      "2016-11-17T23:21:17.200445: step 36, loss 153.058, pre 0.026061, rec 0.446779, f1 0.0492493\n",
      "2016-11-17T23:21:18.040325: step 37, loss 146.99, pre 0.0266166, rec 0.473493, f1 0.0504\n",
      "2016-11-17T23:21:18.922799: step 38, loss 136.349, pre 0.024313, rec 0.469436, f1 0.0462316\n",
      "2016-11-17T23:21:19.855693: step 39, loss 130.114, pre 0.0225542, rec 0.451029, f1 0.0429602\n",
      "2016-11-17T23:21:20.940088: step 40, loss 151.596, pre 0.0256263, rec 0.442716, f1 0.0484483\n",
      "2016-11-17T23:21:21.783040: step 41, loss 125.972, pre 0.0222295, rec 0.460951, f1 0.0424136\n",
      "2016-11-17T23:21:22.592378: step 42, loss 150.902, pre 0.0271174, rec 0.47063, f1 0.0512801\n",
      "2016-11-17T23:21:23.486042: step 43, loss 152.477, pre 0.0271951, rec 0.468816, f1 0.0514082\n",
      "2016-11-17T23:21:24.360071: step 44, loss 154.118, pre 0.0270437, rec 0.457321, f1 0.0510675\n",
      "2016-11-17T23:21:25.260410: step 45, loss 148.356, pre 0.0261513, rec 0.458544, f1 0.0494807\n",
      "2016-11-17T23:21:26.233109: step 46, loss 145.025, pre 0.0254223, rec 0.457565, f1 0.0481684\n",
      "2016-11-17T23:21:27.095901: step 47, loss 143.241, pre 0.0251577, rec 0.458551, f1 0.0476986\n",
      "2016-11-17T23:21:28.060207: step 48, loss 167.909, pre 0.0289861, rec 0.450893, f1 0.0544705\n",
      "2016-11-17T23:21:28.899700: step 49, loss 195.394, pre 0.0349382, rec 0.468562, f1 0.0650277\n",
      "2016-11-17T23:21:29.748494: step 50, loss 141.521, pre 0.0242764, rec 0.448902, f1 0.0460618\n",
      "2016-11-17T23:21:30.559601: step 51, loss 140.548, pre 0.0240806, rec 0.448171, f1 0.0457054\n",
      "2016-11-17T23:21:31.397928: step 52, loss 156.898, pre 0.0283318, rec 0.473792, f1 0.0534665\n",
      "2016-11-17T23:21:32.287811: step 53, loss 151.116, pre 0.0255008, rec 0.443577, f1 0.048229\n",
      "2016-11-17T23:21:33.100068: step 54, loss 156.609, pre 0.0272656, rec 0.453862, f1 0.051441\n",
      "2016-11-17T23:21:33.738313: step 55, loss 156.478, pre 0.0281548, rec 0.469712, f1 0.0531252\n",
      "2016-11-17T23:21:34.645452: step 56, loss 138.805, pre 0.0235212, rec 0.444015, f1 0.0446758\n",
      "2016-11-17T23:21:35.460112: step 57, loss 148.019, pre 0.0257991, rec 0.456978, f1 0.0488408\n",
      "2016-11-17T23:21:36.298774: step 58, loss 157.42, pre 0.0278726, rec 0.465716, f1 0.0525973\n",
      "2016-11-17T23:21:37.144322: step 59, loss 141.739, pre 0.0249226, rec 0.461887, f1 0.0472934\n",
      "2016-11-17T23:21:38.116654: step 60, loss 174.665, pre 0.0307743, rec 0.46022, f1 0.0576908\n",
      "2016-11-17T23:21:39.343438: step 61, loss 150.003, pre 0.026678, rec 0.468282, f1 0.0504802\n",
      "2016-11-17T23:21:40.348716: step 62, loss 150.725, pre 0.0270992, rec 0.467753, f1 0.0512303\n",
      "2016-11-17T23:21:41.395847: step 63, loss 194.062, pre 0.0344897, rec 0.469714, f1 0.064261\n",
      "2016-11-17T23:21:42.383467: step 64, loss 175.355, pre 0.0298063, rec 0.446822, f1 0.0558847\n",
      "2016-11-17T23:21:43.285641: step 65, loss 160.526, pre 0.0282907, rec 0.46036, f1 0.0533056\n",
      "2016-11-17T23:21:44.483881: step 66, loss 159.008, pre 0.0296321, rec 0.485255, f1 0.0558534\n",
      "2016-11-17T23:21:45.648028: step 67, loss 150.812, pre 0.0272164, rec 0.473088, f1 0.0514717\n",
      "2016-11-17T23:21:46.874130: step 68, loss 122.175, pre 0.0223719, rec 0.478603, f1 0.0427457\n",
      "2016-11-17T23:21:48.217119: step 69, loss 145.935, pre 0.0256579, rec 0.458211, f1 0.0485946\n",
      "2016-11-17T23:21:49.361146: step 70, loss 175.539, pre 0.0300608, rec 0.449939, f1 0.0563563\n",
      "2016-11-17T23:21:50.238532: step 71, loss 168.935, pre 0.0300324, rec 0.463924, f1 0.0564128\n",
      "2016-11-17T23:21:51.241614: step 72, loss 154.213, pre 0.0278119, rec 0.470222, f1 0.0525176\n",
      "2016-11-17T23:21:52.405162: step 73, loss 178.341, pre 0.0306612, rec 0.448649, f1 0.0573997\n",
      "2016-11-17T23:21:53.206780: step 74, loss 188.951, pre 0.0342441, rec 0.475141, f1 0.0638839\n",
      "2016-11-17T23:21:54.181449: step 75, loss 156.177, pre 0.0288395, rec 0.484642, f1 0.0544395\n",
      "2016-11-17T23:21:55.186108: step 76, loss 176.601, pre 0.032203, rec 0.479154, f1 0.0603501\n",
      "2016-11-17T23:21:56.367580: step 77, loss 140.634, pre 0.0259335, rec 0.479151, f1 0.0492039\n",
      "2016-11-17T23:21:57.474373: step 78, loss 171.058, pre 0.0307247, rec 0.470331, f1 0.0576813\n",
      "2016-11-17T23:21:58.554313: step 79, loss 204.329, pre 0.0358958, rec 0.458639, f1 0.0665805\n",
      "2016-11-17T23:21:59.625138: step 80, loss 168.3, pre 0.0292354, rec 0.452926, f1 0.0549256\n",
      "2016-11-17T23:22:00.641292: step 81, loss 154.017, pre 0.0260731, rec 0.442897, f1 0.049247\n",
      "2016-11-17T23:22:01.646267: step 82, loss 137.213, pre 0.0232986, rec 0.445312, f1 0.0442804\n",
      "2016-11-17T23:22:02.742242: step 83, loss 185.778, pre 0.0330955, rec 0.464327, f1 0.061787\n",
      "2016-11-17T23:22:03.731880: step 84, loss 138.536, pre 0.0240292, rec 0.452859, f1 0.0456369\n",
      "2016-11-17T23:22:04.547984: step 85, loss 139.074, pre 0.0243198, rec 0.46, f1 0.0461972\n",
      "2016-11-17T23:22:05.521068: step 86, loss 156.941, pre 0.0274815, rec 0.456714, f1 0.0518435\n",
      "2016-11-17T23:22:06.505149: step 87, loss 133.174, pre 0.0226623, rec 0.44686, f1 0.043137\n",
      "2016-11-17T23:22:07.545665: step 88, loss 157.279, pre 0.0281886, rec 0.46875, f1 0.0531792\n",
      "2016-11-17T23:22:08.533886: step 89, loss 126.849, pre 0.0229871, rec 0.472222, f1 0.0438401\n",
      "2016-11-17T23:22:09.485747: step 90, loss 176.851, pre 0.0307299, rec 0.451574, f1 0.057544\n",
      "2016-11-17T23:22:10.632890: step 91, loss 154.373, pre 0.0285247, rec 0.482044, f1 0.0538622\n",
      "2016-11-17T23:22:11.713595: step 92, loss 147.104, pre 0.0251137, rec 0.446468, f1 0.0475526\n",
      "2016-11-17T23:22:12.786572: step 93, loss 157.575, pre 0.027687, rec 0.460285, f1 0.0522322\n",
      "2016-11-17T23:22:13.787633: step 94, loss 154.222, pre 0.0272065, rec 0.463245, f1 0.0513945\n",
      "2016-11-17T23:22:14.610101: step 95, loss 164.395, pre 0.0290766, rec 0.46619, f1 0.0547391\n",
      "2016-11-17T23:22:15.695412: step 96, loss 139.092, pre 0.0250554, rec 0.469278, f1 0.0475709\n",
      "2016-11-17T23:22:16.608594: step 97, loss 194.566, pre 0.0348033, rec 0.472817, f1 0.0648343\n",
      "2016-11-17T23:22:17.393274: step 98, loss 182.67, pre 0.0319694, rec 0.460457, f1 0.0597878\n",
      "2016-11-17T23:22:18.207768: step 99, loss 142.301, pre 0.0240215, rec 0.445365, f1 0.0455843\n",
      "2016-11-17T23:22:19.070332: step 100, loss 148.136, pre 0.0271136, rec 0.475504, f1 0.051302\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479442842/checkpoints/model-100\n",
      "\n",
      "2016-11-17T23:22:20.438978: step 101, loss 144.107, pre 0.0246883, rec 0.449071, f1 0.0468036\n",
      "2016-11-17T23:22:21.282998: step 102, loss 160.761, pre 0.0289123, rec 0.471761, f1 0.0544855\n",
      "2016-11-17T23:22:22.280286: step 103, loss 133.557, pre 0.0241395, rec 0.4696, f1 0.0459186\n",
      "2016-11-17T23:22:23.107478: step 104, loss 151.015, pre 0.0271282, rec 0.471711, f1 0.0513057\n",
      "2016-11-17T23:22:24.016677: step 105, loss 126.14, pre 0.0229058, rec 0.474174, f1 0.0437005\n",
      "2016-11-17T23:22:24.883800: step 106, loss 127.481, pre 0.0224296, rec 0.459732, f1 0.0427724\n",
      "2016-11-17T23:22:25.732464: step 107, loss 143.718, pre 0.0244587, rec 0.448173, f1 0.046386\n",
      "2016-11-17T23:22:26.737118: step 108, loss 122.832, pre 0.0231659, rec 0.488291, f1 0.0442332\n",
      "2016-11-17T23:22:27.635022: step 109, loss 159.63, pre 0.0275924, rec 0.452046, f1 0.0520102\n",
      "2016-11-17T23:22:28.215300: step 110, loss 143.691, pre 0.026051, rec 0.47027, f1 0.0493673\n",
      "2016-11-17T23:22:29.236940: step 111, loss 160.899, pre 0.0293866, rec 0.477454, f1 0.0553654\n",
      "2016-11-17T23:22:30.156698: step 112, loss 159.422, pre 0.0302736, rec 0.498665, f1 0.0570817\n",
      "2016-11-17T23:22:31.150240: step 113, loss 157.505, pre 0.0285479, rec 0.469831, f1 0.0538252\n",
      "2016-11-17T23:22:32.336896: step 114, loss 143.255, pre 0.0248267, rec 0.455157, f1 0.0470852\n",
      "2016-11-17T23:22:33.330246: step 115, loss 137.398, pre 0.0245452, rec 0.465785, f1 0.0466329\n",
      "2016-11-17T23:22:34.252806: step 116, loss 141.126, pre 0.0249867, rec 0.462879, f1 0.047414\n",
      "2016-11-17T23:22:35.294574: step 117, loss 147.721, pre 0.0271449, rec 0.480866, f1 0.0513889\n",
      "2016-11-17T23:22:36.214730: step 118, loss 137.948, pre 0.023999, rec 0.458495, f1 0.0456107\n",
      "2016-11-17T23:22:37.102810: step 119, loss 143.831, pre 0.0251922, rec 0.458333, f1 0.0477593\n",
      "2016-11-17T23:22:38.066235: step 120, loss 191.635, pre 0.0341639, rec 0.4657, f1 0.0636579\n",
      "2016-11-17T23:22:39.063526: step 121, loss 170.456, pre 0.0309501, rec 0.47464, f1 0.058111\n",
      "2016-11-17T23:22:40.004317: step 122, loss 177.569, pre 0.0297463, rec 0.439275, f1 0.0557195\n",
      "2016-11-17T23:22:41.041812: step 123, loss 162.173, pre 0.028541, rec 0.459763, f1 0.0537456\n",
      "2016-11-17T23:22:41.880661: step 124, loss 156.02, pre 0.0268044, rec 0.448868, f1 0.0505879\n",
      "2016-11-17T23:22:42.955605: step 125, loss 153.877, pre 0.0276562, rec 0.469444, f1 0.0522351\n",
      "2016-11-17T23:22:43.844333: step 126, loss 148.395, pre 0.0246803, rec 0.438178, f1 0.0467286\n",
      "2016-11-17T23:22:44.664793: step 127, loss 178.643, pre 0.0320928, rec 0.468301, f1 0.0600691\n",
      "2016-11-17T23:22:45.591024: step 128, loss 145.107, pre 0.0262534, rec 0.472406, f1 0.0497424\n",
      "2016-11-17T23:22:46.421223: step 129, loss 156.678, pre 0.0288863, rec 0.485364, f1 0.0545274\n",
      "2016-11-17T23:22:47.367854: step 130, loss 136.646, pre 0.0258176, rec 0.487919, f1 0.0490403\n",
      "2016-11-17T23:22:48.254517: step 131, loss 133.099, pre 0.023403, rec 0.460611, f1 0.0445429\n",
      "2016-11-17T23:22:49.608366: step 132, loss 152.816, pre 0.0276072, rec 0.474493, f1 0.0521786\n",
      "2016-11-17T23:22:50.616073: step 133, loss 159.597, pre 0.028823, rec 0.473226, f1 0.0543365\n",
      "2016-11-17T23:22:51.479970: step 134, loss 135.553, pre 0.025037, rec 0.47915, f1 0.0475874\n",
      "2016-11-17T23:22:52.321814: step 135, loss 153.25, pre 0.0263384, rec 0.450734, f1 0.0497685\n",
      "2016-11-17T23:22:53.137097: step 136, loss 151.849, pre 0.0281125, rec 0.481742, f1 0.0531248\n",
      "2016-11-17T23:22:53.948982: step 137, loss 155.869, pre 0.0279093, rec 0.468814, f1 0.0526823\n",
      "2016-11-17T23:22:54.752522: step 138, loss 156.13, pre 0.0276518, rec 0.463699, f1 0.0521913\n",
      "2016-11-17T23:22:55.580602: step 139, loss 141.132, pre 0.0251143, rec 0.465909, f1 0.0476596\n",
      "2016-11-17T23:22:56.393274: step 140, loss 149.328, pre 0.0258589, rec 0.451937, f1 0.0489187\n",
      "2016-11-17T23:22:57.188952: step 141, loss 144.557, pre 0.0274348, rec 0.494109, f1 0.0519833\n",
      "2016-11-17T23:22:58.005741: step 142, loss 141.284, pre 0.0245791, rec 0.456818, f1 0.0466484\n",
      "2016-11-17T23:22:58.803578: step 143, loss 154.926, pre 0.0269309, rec 0.455425, f1 0.0508547\n",
      "2016-11-17T23:22:59.610945: step 144, loss 167.882, pre 0.0287027, rec 0.450542, f1 0.0539673\n",
      "2016-11-17T23:23:00.408425: step 145, loss 148.359, pre 0.02746, rec 0.483106, f1 0.0519661\n",
      "2016-11-17T23:23:01.230171: step 146, loss 133.184, pre 0.0234826, rec 0.464257, f1 0.044704\n",
      "2016-11-17T23:23:02.090968: step 147, loss 191.058, pre 0.0330802, rec 0.451794, f1 0.0616467\n",
      "2016-11-17T23:23:02.917277: step 148, loss 173.29, pre 0.0322187, rec 0.487085, f1 0.0604396\n",
      "2016-11-17T23:23:03.795567: step 149, loss 174.613, pre 0.0310661, rec 0.465116, f1 0.058242\n",
      "2016-11-17T23:23:04.627357: step 150, loss 198.02, pre 0.0344126, rec 0.457297, f1 0.0640085\n",
      "2016-11-17T23:23:05.446129: step 151, loss 157.371, pre 0.0281713, rec 0.46979, f1 0.0531551\n",
      "2016-11-17T23:23:06.340267: step 152, loss 155.839, pre 0.0273466, rec 0.461908, f1 0.0516362\n",
      "2016-11-17T23:23:07.173017: step 153, loss 153.421, pre 0.0274273, rec 0.461699, f1 0.0517787\n",
      "2016-11-17T23:23:08.028543: step 154, loss 136.911, pre 0.023788, rec 0.451916, f1 0.0451969\n",
      "2016-11-17T23:23:08.856666: step 155, loss 188.296, pre 0.0333591, rec 0.465645, f1 0.062258\n",
      "2016-11-17T23:23:09.661538: step 156, loss 170.244, pre 0.0304334, rec 0.466123, f1 0.0571363\n",
      "2016-11-17T23:23:10.491813: step 157, loss 159.129, pre 0.027853, rec 0.456624, f1 0.0525034\n",
      "2016-11-17T23:23:11.286248: step 158, loss 186.934, pre 0.0333757, rec 0.465981, f1 0.0622898\n",
      "2016-11-17T23:23:12.161104: step 159, loss 147.698, pre 0.0260028, rec 0.459088, f1 0.0492179\n",
      "2016-11-17T23:23:12.944678: step 160, loss 135.433, pre 0.0231803, rec 0.446203, f1 0.0440711\n",
      "2016-11-17T23:23:13.765649: step 161, loss 155.483, pre 0.0275514, rec 0.464605, f1 0.052018\n",
      "2016-11-17T23:23:14.587366: step 162, loss 140.317, pre 0.0254466, rec 0.472603, f1 0.048293\n",
      "2016-11-17T23:23:15.396823: step 163, loss 128.301, pre 0.0212381, rec 0.431799, f1 0.0404849\n",
      "2016-11-17T23:23:16.201567: step 164, loss 154.94, pre 0.0275297, rec 0.465839, f1 0.0519871\n",
      "2016-11-17T23:23:16.757864: step 165, loss 167.704, pre 0.031857, rec 0.492151, f1 0.0598406\n",
      "2016-11-17T23:23:17.607016: step 166, loss 162.66, pre 0.0276464, rec 0.446935, f1 0.0520717\n",
      "2016-11-17T23:23:18.438703: step 167, loss 156.544, pre 0.0260628, rec 0.4366, f1 0.0491892\n",
      "2016-11-17T23:23:19.225321: step 168, loss 169.19, pre 0.0302049, rec 0.471257, f1 0.0567711\n",
      "2016-11-17T23:23:20.015238: step 169, loss 148.348, pre 0.0271159, rec 0.47482, f1 0.051302\n",
      "2016-11-17T23:23:20.813638: step 170, loss 156.497, pre 0.0276543, rec 0.463115, f1 0.052192\n",
      "2016-11-17T23:23:21.608017: step 171, loss 162.307, pre 0.0294323, rec 0.471711, f1 0.0554074\n",
      "2016-11-17T23:23:22.396738: step 172, loss 143.262, pre 0.0257444, rec 0.466816, f1 0.0487976\n",
      "2016-11-17T23:23:23.186651: step 173, loss 163.023, pre 0.0295981, rec 0.477407, f1 0.0557403\n",
      "2016-11-17T23:23:23.991566: step 174, loss 148.539, pre 0.025919, rec 0.458213, f1 0.0490627\n",
      "2016-11-17T23:23:24.782437: step 175, loss 186.707, pre 0.0325494, rec 0.458453, f1 0.0607833\n",
      "2016-11-17T23:23:25.610329: step 176, loss 160.116, pre 0.027903, rec 0.455214, f1 0.0525828\n",
      "2016-11-17T23:23:26.395841: step 177, loss 172.084, pre 0.0312957, rec 0.477667, f1 0.0587428\n",
      "2016-11-17T23:23:27.187513: step 178, loss 133.118, pre 0.0236169, rec 0.466667, f1 0.0449586\n",
      "2016-11-17T23:23:27.989198: step 179, loss 157.357, pre 0.0252449, rec 0.421053, f1 0.0476338\n",
      "2016-11-17T23:23:28.866976: step 180, loss 156.544, pre 0.0275496, rec 0.460383, f1 0.0519881\n",
      "2016-11-17T23:23:29.725287: step 181, loss 164.79, pre 0.0291744, rec 0.462038, f1 0.0548832\n",
      "2016-11-17T23:23:30.513056: step 182, loss 173.417, pre 0.0301358, rec 0.454938, f1 0.0565271\n",
      "2016-11-17T23:23:31.293696: step 183, loss 170.431, pre 0.0306027, rec 0.469592, f1 0.0574607\n",
      "2016-11-17T23:23:32.087300: step 184, loss 156.047, pre 0.0292405, rec 0.486689, f1 0.0551665\n",
      "2016-11-17T23:23:32.886582: step 185, loss 173.853, pre 0.0299711, rec 0.453481, f1 0.0562261\n",
      "2016-11-17T23:23:33.665093: step 186, loss 159.889, pre 0.0295768, rec 0.481654, f1 0.0557314\n",
      "2016-11-17T23:23:34.444531: step 187, loss 160.35, pre 0.0280785, rec 0.460307, f1 0.0529283\n",
      "2016-11-17T23:23:35.228357: step 188, loss 148.441, pre 0.0259038, rec 0.45566, f1 0.0490208\n",
      "2016-11-17T23:23:36.017544: step 189, loss 164.956, pre 0.0286904, rec 0.456197, f1 0.0539856\n",
      "2016-11-17T23:23:36.836508: step 190, loss 154.461, pre 0.0274564, rec 0.466436, f1 0.0518601\n",
      "2016-11-17T23:23:37.643944: step 191, loss 163.33, pre 0.0271291, rec 0.434583, f1 0.0510701\n",
      "2016-11-17T23:23:38.451336: step 192, loss 137.417, pre 0.0249152, rec 0.47397, f1 0.0473419\n",
      "2016-11-17T23:23:39.278116: step 193, loss 157.227, pre 0.0284459, rec 0.470788, f1 0.0536502\n",
      "2016-11-17T23:23:40.083441: step 194, loss 170.076, pre 0.0300635, rec 0.463859, f1 0.0564673\n",
      "2016-11-17T23:23:40.906236: step 195, loss 141.438, pre 0.024888, rec 0.462179, f1 0.0472325\n",
      "2016-11-17T23:23:41.747711: step 196, loss 154.713, pre 0.026857, rec 0.451903, f1 0.0507007\n",
      "2016-11-17T23:23:42.597240: step 197, loss 135.209, pre 0.0238816, rec 0.462025, f1 0.0454157\n",
      "2016-11-17T23:23:43.393737: step 198, loss 178.357, pre 0.0326669, rec 0.477871, f1 0.0611534\n",
      "2016-11-17T23:23:44.228752: step 199, loss 139.01, pre 0.0230596, rec 0.434749, f1 0.0437962\n",
      "2016-11-17T23:23:45.061662: step 200, loss 171.423, pre 0.0309735, rec 0.475389, f1 0.0581577\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479442842/checkpoints/model-200\n",
      "\n",
      "2016-11-17T23:23:46.578322: step 201, loss 142.822, pre 0.0245861, rec 0.447862, f1 0.0466133\n",
      "2016-11-17T23:23:47.347394: step 202, loss 156.422, pre 0.0274225, rec 0.457592, f1 0.0517441\n",
      "2016-11-17T23:23:48.136338: step 203, loss 148.389, pre 0.0271914, rec 0.478792, f1 0.0514604\n",
      "2016-11-17T23:23:48.936362: step 204, loss 155.632, pre 0.0274401, rec 0.463918, f1 0.0518155\n",
      "2016-11-17T23:23:49.709108: step 205, loss 164.273, pre 0.0299318, rec 0.476283, f1 0.056324\n",
      "2016-11-17T23:23:50.488953: step 206, loss 172.309, pre 0.0300081, rec 0.457764, f1 0.056324\n",
      "2016-11-17T23:23:51.288960: step 207, loss 143.687, pre 0.0251006, rec 0.455291, f1 0.0475783\n",
      "2016-11-17T23:23:52.090392: step 208, loss 145.61, pre 0.0264278, rec 0.473607, f1 0.050062\n",
      "2016-11-17T23:23:52.879382: step 209, loss 154.967, pre 0.0288807, rec 0.486245, f1 0.054523\n",
      "2016-11-17T23:23:53.664015: step 210, loss 130.214, pre 0.0232128, rec 0.463875, f1 0.0442132\n",
      "2016-11-17T23:23:54.425980: step 211, loss 132.43, pre 0.0242175, rec 0.479452, f1 0.0461062\n",
      "2016-11-17T23:23:55.199601: step 212, loss 135.232, pre 0.0230888, rec 0.448494, f1 0.0439168\n",
      "2016-11-17T23:23:55.997191: step 213, loss 155.523, pre 0.0276252, rec 0.46323, f1 0.052141\n",
      "2016-11-17T23:23:56.778132: step 214, loss 170.236, pre 0.0301947, rec 0.461683, f1 0.0566823\n",
      "2016-11-17T23:23:57.594484: step 215, loss 155.722, pre 0.0268687, rec 0.451169, f1 0.0507171\n",
      "2016-11-17T23:23:58.416416: step 216, loss 160.467, pre 0.0283046, rec 0.460667, f1 0.0533323\n",
      "2016-11-17T23:23:59.245917: step 217, loss 143.094, pre 0.0252115, rec 0.458894, f1 0.047797\n",
      "2016-11-17T23:24:00.079283: step 218, loss 150.102, pre 0.0257778, rec 0.45182, f1 0.048773\n",
      "2016-11-17T23:24:00.852285: step 219, loss 144.833, pre 0.0250905, rec 0.456024, f1 0.047564\n",
      "2016-11-17T23:24:01.396856: step 220, loss 144.285, pre 0.0251761, rec 0.462783, f1 0.0477542\n",
      "2016-11-17T23:24:02.245601: step 221, loss 170.923, pre 0.0290074, rec 0.444793, f1 0.0544631\n",
      "2016-11-17T23:24:03.023175: step 222, loss 180.331, pre 0.0324068, rec 0.470379, f1 0.0606361\n",
      "2016-11-17T23:24:03.812632: step 223, loss 132.516, pre 0.0246042, rec 0.485117, f1 0.0468331\n",
      "2016-11-17T23:24:04.578685: step 224, loss 149.715, pre 0.0260593, rec 0.456755, f1 0.0493056\n",
      "2016-11-17T23:24:05.350551: step 225, loss 140.105, pre 0.0246025, rec 0.461832, f1 0.0467163\n",
      "2016-11-17T23:24:06.132760: step 226, loss 157.014, pre 0.0286311, rec 0.475187, f1 0.0540081\n",
      "2016-11-17T23:24:06.913350: step 227, loss 146.801, pre 0.0278334, rec 0.493836, f1 0.0526967\n",
      "2016-11-17T23:24:07.824036: step 228, loss 143.194, pre 0.0235256, rec 0.432108, f1 0.0446218\n",
      "2016-11-17T23:24:08.730715: step 229, loss 170.604, pre 0.0297196, rec 0.454203, f1 0.0557889\n",
      "2016-11-17T23:24:09.597566: step 230, loss 148.434, pre 0.0266377, rec 0.467243, f1 0.0504019\n",
      "2016-11-17T23:24:10.394080: step 231, loss 170.667, pre 0.0304552, rec 0.467126, f1 0.0571823\n",
      "2016-11-17T23:24:11.182695: step 232, loss 138.117, pre 0.0257607, rec 0.485339, f1 0.0489247\n",
      "2016-11-17T23:24:11.987595: step 233, loss 184.075, pre 0.0328808, rec 0.469803, f1 0.0614602\n",
      "2016-11-17T23:24:12.773733: step 234, loss 168.928, pre 0.0302906, rec 0.468691, f1 0.0569037\n",
      "2016-11-17T23:24:13.553404: step 235, loss 157.305, pre 0.0276481, rec 0.463265, f1 0.0521819\n",
      "2016-11-17T23:24:14.354989: step 236, loss 148.611, pre 0.0258529, rec 0.457493, f1 0.0489403\n",
      "2016-11-17T23:24:15.141184: step 237, loss 171.337, pre 0.0290402, rec 0.443054, f1 0.0545077\n",
      "2016-11-17T23:24:15.911466: step 238, loss 156.889, pre 0.0268694, rec 0.450137, f1 0.0507118\n",
      "2016-11-17T23:24:16.666556: step 239, loss 140.586, pre 0.0248973, rec 0.460836, f1 0.0472423\n",
      "2016-11-17T23:24:17.437543: step 240, loss 141.82, pre 0.0234681, rec 0.436033, f1 0.044539\n",
      "2016-11-17T23:24:18.246955: step 241, loss 141.069, pre 0.0249908, rec 0.461713, f1 0.0474151\n",
      "2016-11-17T23:24:19.168851: step 242, loss 168.218, pre 0.0308876, rec 0.481294, f1 0.0580497\n",
      "2016-11-17T23:24:20.399765: step 243, loss 157.704, pre 0.0285479, rec 0.469871, f1 0.0538256\n",
      "2016-11-17T23:24:21.546374: step 244, loss 165.116, pre 0.0297147, rec 0.472186, f1 0.0559108\n",
      "2016-11-17T23:24:22.741751: step 245, loss 143.572, pre 0.0260288, rec 0.477323, f1 0.0493656\n",
      "2016-11-17T23:24:23.542638: step 246, loss 137.553, pre 0.0251627, rec 0.474011, f1 0.0477885\n",
      "2016-11-17T23:24:24.346930: step 247, loss 184.567, pre 0.0316127, rec 0.452118, f1 0.0590935\n",
      "2016-11-17T23:24:25.183127: step 248, loss 171.819, pre 0.0297747, rec 0.458567, f1 0.0559186\n",
      "2016-11-17T23:24:25.988599: step 249, loss 136.933, pre 0.0252496, rec 0.48053, f1 0.0479782\n",
      "2016-11-17T23:24:26.787468: step 250, loss 151.099, pre 0.0269919, rec 0.470297, f1 0.0510537\n",
      "2016-11-17T23:24:27.613895: step 251, loss 150.01, pre 0.02564, rec 0.449286, f1 0.0485115\n",
      "2016-11-17T23:24:28.457309: step 252, loss 162.221, pre 0.0287091, rec 0.463415, f1 0.0540686\n",
      "2016-11-17T23:24:29.353700: step 253, loss 160.24, pre 0.0283767, rec 0.460974, f1 0.0534623\n",
      "2016-11-17T23:24:30.174190: step 254, loss 162.891, pre 0.0284156, rec 0.455322, f1 0.0534929\n",
      "2016-11-17T23:24:31.012912: step 255, loss 161.042, pre 0.028687, rec 0.465826, f1 0.0540457\n",
      "2016-11-17T23:24:31.867011: step 256, loss 182.21, pre 0.0337895, rec 0.486834, f1 0.0631931\n",
      "2016-11-17T23:24:32.914879: step 257, loss 146.195, pre 0.0263914, rec 0.469686, f1 0.0499747\n",
      "2016-11-17T23:24:33.860859: step 258, loss 156.868, pre 0.0291096, rec 0.485054, f1 0.0549231\n",
      "2016-11-17T23:24:34.862186: step 259, loss 144.851, pre 0.0259936, rec 0.469764, f1 0.0492615\n",
      "2016-11-17T23:24:35.897851: step 260, loss 138.501, pre 0.0245149, rec 0.459459, f1 0.0465462\n",
      "2016-11-17T23:24:36.748294: step 261, loss 167.302, pre 0.0289943, rec 0.456174, f1 0.0545232\n",
      "2016-11-17T23:24:37.645418: step 262, loss 174.186, pre 0.0314714, rec 0.473942, f1 0.0590234\n",
      "2016-11-17T23:24:38.490451: step 263, loss 165.664, pre 0.0312692, rec 0.491645, f1 0.0587987\n",
      "2016-11-17T23:24:39.404864: step 264, loss 127.543, pre 0.0217989, rec 0.446218, f1 0.0415672\n",
      "2016-11-17T23:24:40.283269: step 265, loss 135.629, pre 0.023462, rec 0.449447, f1 0.044596\n",
      "2016-11-17T23:24:41.151707: step 266, loss 153.774, pre 0.0265206, rec 0.453343, f1 0.0501097\n",
      "2016-11-17T23:24:41.923748: step 267, loss 170.275, pre 0.0294904, rec 0.457862, f1 0.0554118\n",
      "2016-11-17T23:24:42.689905: step 268, loss 167.207, pre 0.0286241, rec 0.448077, f1 0.0538106\n",
      "2016-11-17T23:24:43.453583: step 269, loss 162.248, pre 0.0273995, rec 0.441507, f1 0.051597\n",
      "2016-11-17T23:24:44.241135: step 270, loss 133.588, pre 0.0227125, rec 0.447833, f1 0.0432324\n",
      "2016-11-17T23:24:45.010373: step 271, loss 153.65, pre 0.0282337, rec 0.482639, f1 0.0533466\n",
      "2016-11-17T23:24:45.780101: step 272, loss 168.295, pre 0.0300258, rec 0.473016, f1 0.0564672\n",
      "2016-11-17T23:24:46.584919: step 273, loss 145.452, pre 0.0263524, rec 0.472834, f1 0.0499225\n",
      "2016-11-17T23:24:47.451951: step 274, loss 148.481, pre 0.025191, rec 0.447653, f1 0.0476978\n",
      "2016-11-17T23:24:48.018958: step 275, loss 140.717, pre 0.0244468, rec 0.454646, f1 0.0463987\n",
      "2016-11-17T23:24:48.890168: step 276, loss 172.057, pre 0.0298842, rec 0.456129, f1 0.0560934\n",
      "2016-11-17T23:24:49.691869: step 277, loss 183.103, pre 0.0322262, rec 0.457627, f1 0.0602122\n",
      "2016-11-17T23:24:50.452781: step 278, loss 161.485, pre 0.0296897, rec 0.481506, f1 0.0559306\n",
      "2016-11-17T23:24:51.237955: step 279, loss 133.328, pre 0.023969, rec 0.470353, f1 0.0456135\n",
      "2016-11-17T23:24:52.088216: step 280, loss 167.046, pre 0.03056, rec 0.476373, f1 0.0574354\n",
      "2016-11-17T23:24:52.877607: step 281, loss 169.955, pre 0.0295492, rec 0.454316, f1 0.0554893\n",
      "2016-11-17T23:24:53.680451: step 282, loss 164.806, pre 0.0289624, rec 0.460091, f1 0.0544944\n",
      "2016-11-17T23:24:54.495380: step 283, loss 139.958, pre 0.0258232, rec 0.476372, f1 0.0489908\n",
      "2016-11-17T23:24:55.318461: step 284, loss 147.19, pre 0.0261765, rec 0.466957, f1 0.049574\n",
      "2016-11-17T23:24:56.131719: step 285, loss 144.37, pre 0.0261703, rec 0.472633, f1 0.0495945\n",
      "2016-11-17T23:24:56.938132: step 286, loss 134.718, pre 0.0243401, rec 0.469469, f1 0.0462807\n",
      "2016-11-17T23:24:57.740275: step 287, loss 144.056, pre 0.0248961, rec 0.454275, f1 0.0472052\n",
      "2016-11-17T23:24:58.520031: step 288, loss 160.687, pre 0.0284103, rec 0.459387, f1 0.0535112\n",
      "2016-11-17T23:24:59.360654: step 289, loss 139.416, pre 0.0251065, rec 0.469732, f1 0.0476653\n",
      "2016-11-17T23:25:00.211748: step 290, loss 153.224, pre 0.0267661, rec 0.458799, f1 0.0505813\n",
      "2016-11-17T23:25:01.014380: step 291, loss 150.156, pre 0.026066, rec 0.454027, f1 0.0493015\n",
      "2016-11-17T23:25:01.858318: step 292, loss 140.105, pre 0.0237346, rec 0.444529, f1 0.0450632\n",
      "2016-11-17T23:25:02.681075: step 293, loss 189.385, pre 0.0344124, rec 0.475197, f1 0.0641772\n",
      "2016-11-17T23:25:03.484458: step 294, loss 165.317, pre 0.0290022, rec 0.456958, f1 0.0545426\n",
      "2016-11-17T23:25:04.332770: step 295, loss 136.199, pre 0.0247236, rec 0.474922, f1 0.0470004\n",
      "2016-11-17T23:25:05.124221: step 296, loss 141.862, pre 0.025822, rec 0.476298, f1 0.0489881\n",
      "2016-11-17T23:25:05.913441: step 297, loss 151.31, pre 0.0262007, rec 0.453645, f1 0.0495402\n",
      "2016-11-17T23:25:06.688564: step 298, loss 195.081, pre 0.0341575, rec 0.460526, f1 0.0635978\n",
      "2016-11-17T23:25:07.477114: step 299, loss 134.507, pre 0.0225742, rec 0.440989, f1 0.0429498\n",
      "2016-11-17T23:25:08.279007: step 300, loss 132.404, pre 0.0220834, rec 0.438412, f1 0.0420488\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479442842/checkpoints/model-300\n",
      "\n",
      "2016-11-17T23:25:09.506590: step 301, loss 157.19, pre 0.0266214, rec 0.444065, f1 0.0502315\n",
      "2016-11-17T23:25:10.335425: step 302, loss 171.738, pre 0.0297977, rec 0.454489, f1 0.0559285\n",
      "2016-11-17T23:25:11.108725: step 303, loss 133.505, pre 0.022997, rec 0.451043, f1 0.0437627\n",
      "2016-11-17T23:25:11.920219: step 304, loss 150.241, pre 0.0266428, rec 0.466904, f1 0.0504092\n",
      "2016-11-17T23:25:12.722398: step 305, loss 160.992, pre 0.0297038, rec 0.483444, f1 0.0559687\n",
      "2016-11-17T23:25:13.497022: step 306, loss 136.579, pre 0.0244082, rec 0.466354, f1 0.0463885\n",
      "2016-11-17T23:25:14.290184: step 307, loss 168.553, pre 0.0304606, rec 0.471483, f1 0.0572242\n",
      "2016-11-17T23:25:15.066278: step 308, loss 182.035, pre 0.0331855, rec 0.473623, f1 0.062025\n",
      "2016-11-17T23:25:15.851708: step 309, loss 167.211, pre 0.0300311, rec 0.468371, f1 0.0564432\n",
      "2016-11-17T23:25:16.685662: step 310, loss 167.745, pre 0.0299345, rec 0.465605, f1 0.0562524\n",
      "2016-11-17T23:25:17.494247: step 311, loss 133.499, pre 0.023828, rec 0.465973, f1 0.0453377\n",
      "2016-11-17T23:25:18.322350: step 312, loss 167.058, pre 0.030355, rec 0.47476, f1 0.0570617\n",
      "2016-11-17T23:25:19.171900: step 313, loss 155.938, pre 0.0291675, rec 0.487022, f1 0.0550388\n",
      "2016-11-17T23:25:19.981116: step 314, loss 188.483, pre 0.0346793, rec 0.482173, f1 0.0647048\n",
      "2016-11-17T23:25:20.757178: step 315, loss 180.524, pre 0.0322197, rec 0.465364, f1 0.0602668\n",
      "2016-11-17T23:25:21.540644: step 316, loss 178.353, pre 0.0324914, rec 0.475763, f1 0.0608286\n",
      "2016-11-17T23:25:22.321310: step 317, loss 152.828, pre 0.0273127, rec 0.464336, f1 0.0515908\n",
      "2016-11-17T23:25:23.134768: step 318, loss 143.028, pre 0.0260799, rec 0.477612, f1 0.049459\n",
      "2016-11-17T23:25:23.954797: step 319, loss 169.569, pre 0.030668, rec 0.469773, f1 0.0575773\n",
      "2016-11-17T23:25:24.744797: step 320, loss 165.989, pre 0.029911, rec 0.471686, f1 0.0562548\n",
      "2016-11-17T23:25:25.544821: step 321, loss 146.465, pre 0.0261528, rec 0.468613, f1 0.0495409\n",
      "2016-11-17T23:25:26.386501: step 322, loss 156.126, pre 0.0266574, rec 0.446809, f1 0.050313\n",
      "2016-11-17T23:25:27.195175: step 323, loss 139.826, pre 0.0245875, rec 0.458301, f1 0.0466711\n",
      "2016-11-17T23:25:28.023563: step 324, loss 116.864, pre 0.0204048, rec 0.457875, f1 0.0390686\n",
      "2016-11-17T23:25:28.959773: step 325, loss 134.921, pre 0.023307, rec 0.453968, f1 0.0443376\n",
      "2016-11-17T23:25:30.079286: step 326, loss 163.234, pre 0.0290193, rec 0.464309, f1 0.0546246\n",
      "2016-11-17T23:25:31.054766: step 327, loss 149.532, pre 0.0256285, rec 0.450573, f1 0.0484984\n",
      "2016-11-17T23:25:32.012130: step 328, loss 149.527, pre 0.0277698, rec 0.483963, f1 0.0525257\n",
      "2016-11-17T23:25:32.816081: step 329, loss 155.508, pre 0.0271595, rec 0.457674, f1 0.0512761\n",
      "2016-11-17T23:25:33.524910: step 330, loss 165.478, pre 0.0297827, rec 0.4723, f1 0.0560321\n",
      "2016-11-17T23:25:34.611062: step 331, loss 164.889, pre 0.0294345, rec 0.46792, f1 0.0553851\n",
      "2016-11-17T23:25:35.535416: step 332, loss 148.777, pre 0.0267627, rec 0.46949, f1 0.0506388\n",
      "2016-11-17T23:25:36.629945: step 333, loss 168.529, pre 0.0310324, rec 0.482278, f1 0.0583126\n",
      "2016-11-17T23:25:37.663430: step 334, loss 159.836, pre 0.0284056, rec 0.464883, f1 0.0535398\n",
      "2016-11-17T23:25:38.693123: step 335, loss 150.403, pre 0.0256608, rec 0.450463, f1 0.0485556\n",
      "2016-11-17T23:25:39.546919: step 336, loss 155.207, pre 0.0273973, rec 0.463129, f1 0.0517341\n",
      "2016-11-17T23:25:40.698901: step 337, loss 144.644, pre 0.0258441, rec 0.467504, f1 0.0489805\n",
      "2016-11-17T23:25:41.732721: step 338, loss 140.807, pre 0.0270637, rec 0.497734, f1 0.051336\n",
      "2016-11-17T23:25:42.641120: step 339, loss 160.414, pre 0.0288782, rec 0.467377, f1 0.0543954\n",
      "2016-11-17T23:25:43.417189: step 340, loss 165.955, pre 0.0283619, rec 0.449612, f1 0.0533579\n",
      "2016-11-17T23:25:44.359659: step 341, loss 145.869, pre 0.0265429, rec 0.472914, f1 0.0502646\n",
      "2016-11-17T23:25:45.239280: step 342, loss 139.994, pre 0.0244172, rec 0.456422, f1 0.0463545\n",
      "2016-11-17T23:25:46.113429: step 343, loss 161.757, pre 0.0286228, rec 0.462657, f1 0.0539104\n",
      "2016-11-17T23:25:47.052003: step 344, loss 164.775, pre 0.0288438, rec 0.457792, f1 0.0542683\n",
      "2016-11-17T23:25:47.886948: step 345, loss 165.397, pre 0.0293961, rec 0.464124, f1 0.0552903\n",
      "2016-11-17T23:25:48.898251: step 346, loss 140.178, pre 0.0254498, rec 0.478294, f1 0.0483281\n",
      "2016-11-17T23:25:49.969224: step 347, loss 138.69, pre 0.0251775, rec 0.474981, f1 0.0478202\n",
      "2016-11-17T23:25:50.741594: step 348, loss 141.876, pre 0.0266966, rec 0.49024, f1 0.0506359\n",
      "2016-11-17T23:25:51.621474: step 349, loss 167.184, pre 0.029413, rec 0.459373, f1 0.0552861\n",
      "2016-11-17T23:25:52.489286: step 350, loss 139.07, pre 0.0241136, rec 0.454965, f1 0.0457998\n",
      "2016-11-17T23:25:53.331151: step 351, loss 161.045, pre 0.0276336, rec 0.449767, f1 0.0520681\n",
      "2016-11-17T23:25:54.117101: step 352, loss 185.816, pre 0.0320825, rec 0.451613, f1 0.0599091\n",
      "2016-11-17T23:25:54.952604: step 353, loss 163.041, pre 0.0289452, rec 0.464262, f1 0.054493\n",
      "2016-11-17T23:25:55.748623: step 354, loss 162.232, pre 0.029106, rec 0.470046, f1 0.0548177\n",
      "2016-11-17T23:25:56.556658: step 355, loss 133.936, pre 0.0243325, rec 0.4749, f1 0.0462931\n",
      "2016-11-17T23:25:57.418158: step 356, loss 142.477, pre 0.0244463, rec 0.455639, f1 0.046403\n",
      "2016-11-17T23:25:58.258325: step 357, loss 150.238, pre 0.0256369, rec 0.451498, f1 0.0485188\n",
      "2016-11-17T23:25:59.064682: step 358, loss 125.028, pre 0.0223893, rec 0.465812, f1 0.042725\n",
      "2016-11-17T23:25:59.857910: step 359, loss 127.263, pre 0.0231415, rec 0.473993, f1 0.0441286\n",
      "2016-11-17T23:26:00.631981: step 360, loss 155.014, pre 0.0268202, rec 0.450587, f1 0.050627\n",
      "2016-11-17T23:26:01.382620: step 361, loss 166.691, pre 0.0290824, rec 0.455363, f1 0.054673\n",
      "2016-11-17T23:26:02.148095: step 362, loss 154.349, pre 0.027885, rec 0.469896, f1 0.0526459\n",
      "2016-11-17T23:26:02.908048: step 363, loss 160.782, pre 0.0297115, rec 0.481432, f1 0.0559689\n",
      "2016-11-17T23:26:03.677243: step 364, loss 182.579, pre 0.0306429, rec 0.443008, f1 0.057321\n",
      "2016-11-17T23:26:04.440242: step 365, loss 156.737, pre 0.0275882, rec 0.45802, f1 0.0520417\n",
      "2016-11-17T23:26:05.221955: step 366, loss 163.044, pre 0.0277471, rec 0.445759, f1 0.0522423\n",
      "2016-11-17T23:26:06.001553: step 367, loss 175.707, pre 0.0314607, rec 0.468085, f1 0.0589586\n",
      "2016-11-17T23:26:06.842701: step 368, loss 135.003, pre 0.022769, rec 0.443209, f1 0.0433129\n",
      "2016-11-17T23:26:07.713349: step 369, loss 126.81, pre 0.0236781, rec 0.486555, f1 0.0451585\n",
      "2016-11-17T23:26:08.505743: step 370, loss 170.429, pre 0.0299615, rec 0.458883, f1 0.0562502\n",
      "2016-11-17T23:26:09.315860: step 371, loss 149.895, pre 0.0273805, rec 0.476868, f1 0.0517874\n",
      "2016-11-17T23:26:10.114977: step 372, loss 162.776, pre 0.0285585, rec 0.457293, f1 0.0537597\n",
      "2016-11-17T23:26:10.887721: step 373, loss 172.503, pre 0.0298032, rec 0.453135, f1 0.055928\n",
      "2016-11-17T23:26:11.661577: step 374, loss 154.259, pre 0.0276402, rec 0.471607, f1 0.0522199\n",
      "2016-11-17T23:26:12.432078: step 375, loss 164.593, pre 0.0298979, rec 0.472763, f1 0.0562392\n",
      "2016-11-17T23:26:13.196774: step 376, loss 177.488, pre 0.03222, rec 0.475646, f1 0.0603517\n",
      "2016-11-17T23:26:13.982909: step 377, loss 158.813, pre 0.027509, rec 0.452461, f1 0.0518647\n",
      "2016-11-17T23:26:14.773389: step 378, loss 141.652, pre 0.0254089, rec 0.469834, f1 0.0482105\n",
      "2016-11-17T23:26:15.557728: step 379, loss 146.371, pre 0.0260297, rec 0.461651, f1 0.0492807\n",
      "2016-11-17T23:26:16.383501: step 380, loss 152.639, pre 0.0258487, rec 0.445927, f1 0.0488649\n",
      "2016-11-17T23:26:17.198330: step 381, loss 154.078, pre 0.0270083, rec 0.460417, f1 0.0510236\n",
      "2016-11-17T23:26:18.012221: step 382, loss 174.053, pre 0.0309487, rec 0.459459, f1 0.0579912\n",
      "2016-11-17T23:26:18.849549: step 383, loss 173.049, pre 0.0322713, rec 0.484606, f1 0.0605129\n",
      "2016-11-17T23:26:19.671369: step 384, loss 148.908, pre 0.0257414, rec 0.449317, f1 0.0486931\n",
      "2016-11-17T23:26:20.395106: step 385, loss 169.621, pre 0.0275446, rec 0.429493, f1 0.0517692\n",
      "2016-11-17T23:26:21.246815: step 386, loss 133.114, pre 0.0232482, rec 0.459003, f1 0.044255\n",
      "2016-11-17T23:26:22.057480: step 387, loss 144.834, pre 0.0258392, rec 0.466421, f1 0.0489657\n",
      "2016-11-17T23:26:22.845723: step 388, loss 164.966, pre 0.0303775, rec 0.479638, f1 0.0571363\n",
      "2016-11-17T23:26:23.616316: step 389, loss 144.403, pre 0.0254038, rec 0.463704, f1 0.0481687\n",
      "2016-11-17T23:26:24.372862: step 390, loss 162.191, pre 0.0292128, rec 0.472021, f1 0.0550205\n",
      "2016-11-17T23:26:25.189168: step 391, loss 177.483, pre 0.0316388, rec 0.462651, f1 0.0592273\n",
      "2016-11-17T23:26:25.979344: step 392, loss 147.863, pre 0.0266058, rec 0.467486, f1 0.0503463\n",
      "2016-11-17T23:26:26.753830: step 393, loss 158.609, pre 0.0290908, rec 0.476799, f1 0.0548358\n",
      "2016-11-17T23:26:27.521568: step 394, loss 153.151, pre 0.027232, rec 0.466155, f1 0.0514578\n",
      "2016-11-17T23:26:28.311782: step 395, loss 162.675, pre 0.0281501, rec 0.456221, f1 0.0530283\n",
      "2016-11-17T23:26:29.147283: step 396, loss 143.625, pre 0.0250534, rec 0.454545, f1 0.0474893\n",
      "2016-11-17T23:26:29.949064: step 397, loss 187.583, pre 0.0329809, rec 0.460661, f1 0.0615549\n",
      "2016-11-17T23:26:30.742081: step 398, loss 134.06, pre 0.0229866, rec 0.450839, f1 0.043743\n",
      "2016-11-17T23:26:31.507822: step 399, loss 161.356, pre 0.0296789, rec 0.483807, f1 0.055927\n",
      "2016-11-17T23:26:32.390960: step 400, loss 148.521, pre 0.0278558, rec 0.490674, f1 0.0527188\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479442842/checkpoints/model-400\n",
      "\n",
      "2016-11-17T23:26:33.621184: step 401, loss 160.136, pre 0.0280264, rec 0.460254, f1 0.0528354\n",
      "2016-11-17T23:26:34.366803: step 402, loss 143.929, pre 0.0249164, rec 0.454613, f1 0.0472435\n",
      "2016-11-17T23:26:35.117075: step 403, loss 147.729, pre 0.0262142, rec 0.46165, f1 0.0496112\n",
      "2016-11-17T23:26:35.899143: step 404, loss 162.868, pre 0.0281926, rec 0.454306, f1 0.0530905\n",
      "2016-11-17T23:26:36.662233: step 405, loss 139.4, pre 0.0246762, rec 0.461656, f1 0.0468482\n",
      "2016-11-17T23:26:37.429723: step 406, loss 154.828, pre 0.0262391, rec 0.442521, f1 0.0495406\n",
      "2016-11-17T23:26:38.230719: step 407, loss 132.903, pre 0.0234362, rec 0.461786, f1 0.0446085\n",
      "2016-11-17T23:26:39.016555: step 408, loss 155.346, pre 0.0281129, rec 0.475601, f1 0.0530878\n",
      "2016-11-17T23:26:39.803628: step 409, loss 166.545, pre 0.0303672, rec 0.478205, f1 0.0571079\n",
      "2016-11-17T23:26:40.606879: step 410, loss 168.136, pre 0.0297502, rec 0.463446, f1 0.0559113\n",
      "2016-11-17T23:26:41.384761: step 411, loss 170.03, pre 0.0306571, rec 0.47299, f1 0.057582\n",
      "2016-11-17T23:26:42.170673: step 412, loss 137.308, pre 0.0247988, rec 0.474339, f1 0.0471334\n",
      "2016-11-17T23:26:42.938345: step 413, loss 158.047, pre 0.0267763, rec 0.44844, f1 0.0505352\n",
      "2016-11-17T23:26:43.708413: step 414, loss 166.012, pre 0.0299446, rec 0.465894, f1 0.0562723\n",
      "2016-11-17T23:26:44.582558: step 415, loss 141.41, pre 0.0245003, rec 0.45193, f1 0.0464808\n",
      "2016-11-17T23:26:45.368957: step 416, loss 140.694, pre 0.0248074, rec 0.467325, f1 0.0471138\n",
      "2016-11-17T23:26:46.171976: step 417, loss 150.303, pre 0.0278184, rec 0.485106, f1 0.0526194\n",
      "2016-11-17T23:26:46.954425: step 418, loss 175.352, pre 0.0312449, rec 0.46496, f1 0.0585549\n",
      "2016-11-17T23:26:47.724791: step 419, loss 157.128, pre 0.0279206, rec 0.465306, f1 0.0526802\n",
      "2016-11-17T23:26:48.498396: step 420, loss 158.202, pre 0.0290032, rec 0.481457, f1 0.0547105\n",
      "2016-11-17T23:26:49.273298: step 421, loss 143.698, pre 0.0249467, rec 0.4538, f1 0.0472936\n",
      "2016-11-17T23:26:50.057777: step 422, loss 143.029, pre 0.0250336, rec 0.459985, f1 0.047483\n",
      "2016-11-17T23:26:50.830691: step 423, loss 145.896, pre 0.0262005, rec 0.468864, f1 0.0496278\n",
      "2016-11-17T23:26:51.608897: step 424, loss 174.789, pre 0.0317792, rec 0.473427, f1 0.0595604\n",
      "2016-11-17T23:26:52.383226: step 425, loss 171.778, pre 0.0306612, rec 0.464841, f1 0.0575279\n",
      "2016-11-17T23:26:53.140731: step 426, loss 172.212, pre 0.0318237, rec 0.482972, f1 0.0597129\n",
      "2016-11-17T23:26:53.921088: step 427, loss 133.572, pre 0.0249122, rec 0.486832, f1 0.0473989\n",
      "2016-11-17T23:26:54.685552: step 428, loss 165.211, pre 0.0280645, rec 0.446463, f1 0.0528093\n",
      "2016-11-17T23:26:55.470142: step 429, loss 183.097, pre 0.031602, rec 0.448215, f1 0.0590412\n",
      "2016-11-17T23:26:56.267888: step 430, loss 166.845, pre 0.0292525, rec 0.457344, f1 0.0549879\n",
      "2016-11-17T23:26:57.066651: step 431, loss 166.107, pre 0.0289628, rec 0.45232, f1 0.0544397\n",
      "2016-11-17T23:26:57.885359: step 432, loss 150.141, pre 0.0272941, rec 0.47155, f1 0.0516014\n",
      "2016-11-17T23:26:58.706321: step 433, loss 148.906, pre 0.0260841, rec 0.45977, f1 0.0493675\n",
      "2016-11-17T23:26:59.559437: step 434, loss 135.601, pre 0.0248346, rec 0.478363, f1 0.0472178\n",
      "2016-11-17T23:27:00.396002: step 435, loss 139.368, pre 0.0233571, rec 0.441878, f1 0.0443689\n",
      "2016-11-17T23:27:01.213614: step 436, loss 155.569, pre 0.0270392, rec 0.456297, f1 0.051053\n",
      "2016-11-17T23:27:02.013405: step 437, loss 172.839, pre 0.0296038, rec 0.452573, f1 0.0555725\n",
      "2016-11-17T23:27:02.791395: step 438, loss 159.569, pre 0.0284878, rec 0.469524, f1 0.0537165\n",
      "2016-11-17T23:27:03.567753: step 439, loss 163.921, pre 0.0283293, rec 0.451992, f1 0.0533169\n",
      "2016-11-17T23:27:04.125498: step 440, loss 154.725, pre 0.0283814, rec 0.479438, f1 0.0535905\n",
      "2016-11-17T23:27:04.949565: step 441, loss 177.955, pre 0.0298088, rec 0.440627, f1 0.0558399\n",
      "2016-11-17T23:27:05.749560: step 442, loss 154.735, pre 0.0275966, rec 0.467541, f1 0.052117\n",
      "2016-11-17T23:27:06.520997: step 443, loss 174.786, pre 0.02903, rec 0.441032, f1 0.0544744\n",
      "2016-11-17T23:27:07.292015: step 444, loss 133.948, pre 0.0226364, rec 0.439552, f1 0.0430554\n",
      "2016-11-17T23:27:08.076881: step 445, loss 156.447, pre 0.0285714, rec 0.476808, f1 0.0539123\n",
      "2016-11-17T23:27:08.868002: step 446, loss 146.18, pre 0.0245511, rec 0.442406, f1 0.0465206\n",
      "2016-11-17T23:27:09.643016: step 447, loss 155.119, pre 0.0280309, rec 0.472127, f1 0.0529198\n",
      "2016-11-17T23:27:10.433760: step 448, loss 159.428, pre 0.0273861, rec 0.451613, f1 0.0516407\n",
      "2016-11-17T23:27:11.228324: step 449, loss 141.08, pre 0.0246324, rec 0.457511, f1 0.0467478\n",
      "2016-11-17T23:27:12.047253: step 450, loss 165.54, pre 0.0291956, rec 0.46447, f1 0.0549379\n",
      "2016-11-17T23:27:12.818842: step 451, loss 170.942, pre 0.0294298, rec 0.451754, f1 0.0552596\n",
      "2016-11-17T23:27:13.592772: step 452, loss 145.926, pre 0.0259203, rec 0.463736, f1 0.0490964\n",
      "2016-11-17T23:27:14.370294: step 453, loss 144.932, pre 0.0254568, rec 0.462731, f1 0.0482586\n",
      "2016-11-17T23:27:15.137804: step 454, loss 140.681, pre 0.0247883, rec 0.463118, f1 0.0470579\n",
      "2016-11-17T23:27:15.926395: step 455, loss 172.024, pre 0.0304476, rec 0.467993, f1 0.0571754\n",
      "2016-11-17T23:27:16.699325: step 456, loss 130.144, pre 0.0236366, rec 0.472518, f1 0.0450211\n",
      "2016-11-17T23:27:17.455833: step 457, loss 181.861, pre 0.0327942, rec 0.471521, f1 0.0613235\n",
      "2016-11-17T23:27:18.227473: step 458, loss 151.862, pre 0.0265654, rec 0.455955, f1 0.0502056\n",
      "2016-11-17T23:27:19.017255: step 459, loss 171.367, pre 0.0310976, rec 0.472897, f1 0.0583577\n",
      "2016-11-17T23:27:19.791064: step 460, loss 119.45, pre 0.0220489, rec 0.479464, f1 0.042159\n",
      "2016-11-17T23:27:20.563594: step 461, loss 156.339, pre 0.0280244, rec 0.468216, f1 0.0528835\n",
      "2016-11-17T23:27:21.377899: step 462, loss 163.561, pre 0.0280878, rec 0.453176, f1 0.0528971\n",
      "2016-11-17T23:27:22.313504: step 463, loss 176.095, pre 0.0317584, rec 0.469053, f1 0.059489\n",
      "2016-11-17T23:27:23.448101: step 464, loss 147.85, pre 0.0257663, rec 0.458364, f1 0.0487899\n",
      "2016-11-17T23:27:24.240024: step 465, loss 116.3, pre 0.0200859, rec 0.452118, f1 0.038463\n",
      "2016-11-17T23:27:25.047466: step 466, loss 174.974, pre 0.0307177, rec 0.459658, f1 0.057587\n",
      "2016-11-17T23:27:25.992280: step 467, loss 163.694, pre 0.0282635, rec 0.454843, f1 0.05322\n",
      "2016-11-17T23:27:26.785421: step 468, loss 134.045, pre 0.0240687, rec 0.466932, f1 0.0457777\n",
      "2016-11-17T23:27:27.607374: step 469, loss 157.712, pre 0.0277145, rec 0.462008, f1 0.0522921\n",
      "2016-11-17T23:27:28.464498: step 470, loss 120.934, pre 0.0209897, rec 0.450841, f1 0.0401119\n",
      "2016-11-17T23:27:29.292954: step 471, loss 152.253, pre 0.0276865, rec 0.472651, f1 0.0523089\n",
      "2016-11-17T23:27:30.232918: step 472, loss 141.828, pre 0.0253635, rec 0.467973, f1 0.048119\n",
      "2016-11-17T23:27:31.026025: step 473, loss 153.44, pre 0.0277186, rec 0.47112, f1 0.0523568\n",
      "2016-11-17T23:27:31.857539: step 474, loss 165.001, pre 0.0298797, rec 0.469256, f1 0.056182\n",
      "2016-11-17T23:27:32.700316: step 475, loss 152.134, pre 0.0273556, rec 0.467697, f1 0.051688\n",
      "2016-11-17T23:27:33.505587: step 476, loss 179.968, pre 0.0310372, rec 0.452976, f1 0.0580938\n",
      "2016-11-17T23:27:34.343833: step 477, loss 145.629, pre 0.026397, rec 0.47654, f1 0.0500231\n",
      "2016-11-17T23:27:35.167718: step 478, loss 146.644, pre 0.0255834, rec 0.456934, f1 0.0484539\n",
      "2016-11-17T23:27:35.987073: step 479, loss 166.287, pre 0.0289431, rec 0.458172, f1 0.0544467\n",
      "2016-11-17T23:27:36.824470: step 480, loss 167.029, pre 0.0296945, rec 0.464789, f1 0.0558225\n",
      "2016-11-17T23:27:37.638100: step 481, loss 184.9, pre 0.0334153, rec 0.470826, f1 0.0624019\n",
      "2016-11-17T23:27:38.427557: step 482, loss 166.363, pre 0.0294381, rec 0.465296, f1 0.0553728\n",
      "2016-11-17T23:27:39.266610: step 483, loss 148.978, pre 0.0249397, rec 0.439481, f1 0.0472008\n",
      "2016-11-17T23:27:40.166561: step 484, loss 167.385, pre 0.0285353, rec 0.448143, f1 0.0536542\n",
      "2016-11-17T23:27:41.003992: step 485, loss 143.902, pre 0.0268035, rec 0.487407, f1 0.0508128\n",
      "2016-11-17T23:27:41.791029: step 486, loss 141.854, pre 0.0254845, rec 0.468374, f1 0.0483388\n",
      "2016-11-17T23:27:42.561145: step 487, loss 150.482, pre 0.0256663, rec 0.444444, f1 0.0485301\n",
      "2016-11-17T23:27:43.332555: step 488, loss 193.196, pre 0.0344212, rec 0.469027, f1 0.0641355\n",
      "2016-11-17T23:27:44.126484: step 489, loss 152.103, pre 0.0269742, rec 0.465214, f1 0.0509917\n",
      "2016-11-17T23:27:44.903255: step 490, loss 157.479, pre 0.0285314, rec 0.469831, f1 0.053796\n",
      "2016-11-17T23:27:45.684064: step 491, loss 135.345, pre 0.0232815, rec 0.451306, f1 0.0442787\n",
      "2016-11-17T23:27:46.447072: step 492, loss 142.548, pre 0.0245565, rec 0.449286, f1 0.0465678\n",
      "2016-11-17T23:27:47.223393: step 493, loss 175.9, pre 0.0302487, rec 0.452497, f1 0.0567067\n",
      "2016-11-17T23:27:48.004406: step 494, loss 163.015, pre 0.0283729, rec 0.45568, f1 0.0534195\n",
      "2016-11-17T23:27:48.562045: step 495, loss 165.384, pre 0.0314252, rec 0.493446, f1 0.0590873\n",
      "2016-11-17T23:27:49.390654: step 496, loss 158.705, pre 0.0286465, rec 0.469717, f1 0.0539997\n",
      "2016-11-17T23:27:50.154246: step 497, loss 168.609, pre 0.0296772, rec 0.462904, f1 0.0557784\n",
      "2016-11-17T23:27:50.929829: step 498, loss 151.191, pre 0.027385, rec 0.474576, f1 0.0517819\n",
      "2016-11-17T23:27:51.706207: step 499, loss 180.381, pre 0.0316438, rec 0.457295, f1 0.0591916\n",
      "2016-11-17T23:27:52.474560: step 500, loss 138.798, pre 0.0255293, rec 0.481937, f1 0.04849\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479442842/checkpoints/model-500\n",
      "\n",
      "2016-11-17T23:27:53.623395: step 501, loss 149.147, pre 0.0267513, rec 0.467765, f1 0.0506084\n",
      "2016-11-17T23:27:54.409186: step 502, loss 143.191, pre 0.0262619, rec 0.477645, f1 0.0497864\n",
      "2016-11-17T23:27:55.206569: step 503, loss 163.251, pre 0.0279823, rec 0.451444, f1 0.0526981\n",
      "2016-11-17T23:27:55.989337: step 504, loss 160.219, pre 0.0281782, rec 0.461949, f1 0.0531164\n",
      "2016-11-17T23:27:56.760217: step 505, loss 153.388, pre 0.0266101, rec 0.455307, f1 0.0502815\n",
      "2016-11-17T23:27:57.534259: step 506, loss 147.958, pre 0.0253949, rec 0.448226, f1 0.0480665\n",
      "2016-11-17T23:27:58.305680: step 507, loss 146.254, pre 0.0254168, rec 0.455344, f1 0.0481461\n",
      "2016-11-17T23:27:59.108265: step 508, loss 162.539, pre 0.0287443, rec 0.4625, f1 0.0541248\n",
      "2016-11-17T23:27:59.894392: step 509, loss 152.475, pre 0.0272761, rec 0.468816, f1 0.0515527\n",
      "2016-11-17T23:28:00.668294: step 510, loss 146.072, pre 0.0252453, rec 0.454545, f1 0.047834\n",
      "2016-11-17T23:28:01.439785: step 511, loss 187.063, pre 0.0336155, rec 0.469446, f1 0.0627385\n",
      "2016-11-17T23:28:02.248844: step 512, loss 162.398, pre 0.0299927, rec 0.48457, f1 0.056489\n",
      "2016-11-17T23:28:03.043315: step 513, loss 167.045, pre 0.0290145, rec 0.455769, f1 0.0545559\n",
      "2016-11-17T23:28:03.833307: step 514, loss 149.663, pre 0.0267959, rec 0.467523, f1 0.0506868\n",
      "2016-11-17T23:28:04.617010: step 515, loss 171.431, pre 0.0294727, rec 0.453125, f1 0.0553456\n",
      "2016-11-17T23:28:05.384024: step 516, loss 140.446, pre 0.0240283, rec 0.450801, f1 0.0456247\n",
      "2016-11-17T23:28:06.164575: step 517, loss 188.993, pre 0.0335055, rec 0.466063, f1 0.0625166\n",
      "2016-11-17T23:28:06.936579: step 518, loss 161.213, pre 0.0289796, rec 0.471836, f1 0.0546054\n",
      "2016-11-17T23:28:07.694994: step 519, loss 158.651, pre 0.0276437, rec 0.45614, f1 0.0521283\n",
      "2016-11-17T23:28:08.442378: step 520, loss 132.101, pre 0.0227927, rec 0.455799, f1 0.0434144\n",
      "2016-11-17T23:28:09.208557: step 521, loss 129.471, pre 0.0227889, rec 0.458678, f1 0.0434204\n",
      "2016-11-17T23:28:10.004050: step 522, loss 160.93, pre 0.0288956, rec 0.473125, f1 0.0544649\n",
      "2016-11-17T23:28:10.767051: step 523, loss 137.66, pre 0.0250101, rec 0.477519, f1 0.0475309\n",
      "2016-11-17T23:28:11.536413: step 524, loss 153.757, pre 0.0276607, rec 0.469771, f1 0.0522452\n",
      "2016-11-17T23:28:12.310917: step 525, loss 164.237, pre 0.0302634, rec 0.481169, f1 0.0569452\n",
      "2016-11-17T23:28:13.087584: step 526, loss 140.465, pre 0.0239528, rec 0.449275, f1 0.0454809\n",
      "2016-11-17T23:28:13.870920: step 527, loss 160.582, pre 0.0287631, rec 0.467731, f1 0.0541936\n",
      "2016-11-17T23:28:14.686701: step 528, loss 168.592, pre 0.0290175, rec 0.45108, f1 0.0545273\n",
      "2016-11-17T23:28:15.468322: step 529, loss 144.095, pre 0.026088, rec 0.475556, f1 0.0494626\n",
      "2016-11-17T23:28:16.294073: step 530, loss 176.719, pre 0.0322581, rec 0.477657, f1 0.0604347\n",
      "2016-11-17T23:28:17.131785: step 531, loss 158.278, pre 0.0281696, rec 0.465226, f1 0.0531226\n",
      "2016-11-17T23:28:17.980531: step 532, loss 147.109, pre 0.026838, rec 0.478955, f1 0.0508279\n",
      "2016-11-17T23:28:18.812029: step 533, loss 157.483, pre 0.0281799, rec 0.470149, f1 0.0531727\n",
      "2016-11-17T23:28:19.615665: step 534, loss 178.346, pre 0.0312806, rec 0.460108, f1 0.0585787\n",
      "2016-11-17T23:28:20.553378: step 535, loss 167.041, pre 0.0297719, rec 0.465131, f1 0.0559618\n",
      "2016-11-17T23:28:21.341230: step 536, loss 140.442, pre 0.0245311, rec 0.459604, f1 0.0465763\n",
      "2016-11-17T23:28:22.144309: step 537, loss 144.817, pre 0.0264492, rec 0.480472, f1 0.0501384\n",
      "2016-11-17T23:28:22.914293: step 538, loss 140.793, pre 0.0250987, rec 0.463174, f1 0.0476172\n",
      "2016-11-17T23:28:23.701590: step 539, loss 144.603, pre 0.026087, rec 0.469719, f1 0.0494288\n",
      "2016-11-17T23:28:24.478248: step 540, loss 162.184, pre 0.0275624, rec 0.444151, f1 0.0519039\n",
      "2016-11-17T23:28:25.260335: step 541, loss 148.626, pre 0.0255029, rec 0.447008, f1 0.0482528\n",
      "2016-11-17T23:28:26.044141: step 542, loss 148.016, pre 0.0253874, rec 0.446054, f1 0.0480406\n",
      "2016-11-17T23:28:26.794983: step 543, loss 157.584, pre 0.0270336, rec 0.452753, f1 0.0510208\n",
      "2016-11-17T23:28:27.549177: step 544, loss 166.449, pre 0.0297296, rec 0.468208, f1 0.0559092\n",
      "2016-11-17T23:28:28.323253: step 545, loss 160.4, pre 0.0280137, rec 0.457638, f1 0.0527956\n",
      "2016-11-17T23:28:29.161068: step 546, loss 175.191, pre 0.0307367, rec 0.46182, f1 0.0576373\n",
      "2016-11-17T23:28:29.976474: step 547, loss 150.813, pre 0.0261197, rec 0.452803, f1 0.0493904\n",
      "2016-11-17T23:28:30.766835: step 548, loss 137.528, pre 0.0244371, rec 0.465008, f1 0.046434\n",
      "2016-11-17T23:28:31.611266: step 549, loss 137.183, pre 0.0235806, rec 0.448087, f1 0.0448035\n",
      "2016-11-17T23:28:32.171592: step 550, loss 160.993, pre 0.0287804, rec 0.46332, f1 0.0541944\n",
      "2016-11-17T23:28:32.994803: step 551, loss 165.013, pre 0.030404, rec 0.483516, f1 0.0572106\n",
      "2016-11-17T23:28:33.781099: step 552, loss 130.573, pre 0.0246099, rec 0.492659, f1 0.046878\n",
      "2016-11-17T23:28:34.553208: step 553, loss 140.514, pre 0.0248884, rec 0.458143, f1 0.047212\n",
      "2016-11-17T23:28:35.345155: step 554, loss 162.069, pre 0.0268199, rec 0.43605, f1 0.0505318\n",
      "2016-11-17T23:28:36.125758: step 555, loss 170.223, pre 0.0289024, rec 0.444584, f1 0.0542764\n",
      "2016-11-17T23:28:36.912810: step 556, loss 134.907, pre 0.0219345, rec 0.423904, f1 0.0417108\n",
      "2016-11-17T23:28:37.672070: step 557, loss 149.607, pre 0.0267948, rec 0.467857, f1 0.0506868\n",
      "2016-11-17T23:28:38.444114: step 558, loss 147.874, pre 0.0266792, rec 0.472924, f1 0.0505089\n",
      "2016-11-17T23:28:39.378679: step 559, loss 149.511, pre 0.0268514, rec 0.469286, f1 0.0507963\n",
      "2016-11-17T23:28:40.244129: step 560, loss 146.558, pre 0.0267423, rec 0.477058, f1 0.0506456\n",
      "2016-11-17T23:28:41.049678: step 561, loss 157.493, pre 0.0278052, rec 0.459606, f1 0.0524379\n",
      "2016-11-17T23:28:41.843332: step 562, loss 174.07, pre 0.0300841, rec 0.45326, f1 0.0564232\n",
      "2016-11-17T23:28:42.625794: step 563, loss 159.48, pre 0.0272059, rec 0.444892, f1 0.0512761\n",
      "2016-11-17T23:28:43.400859: step 564, loss 159.424, pre 0.0291231, rec 0.480589, f1 0.0549182\n",
      "2016-11-17T23:28:44.189095: step 565, loss 142.921, pre 0.0242419, rec 0.445611, f1 0.0459823\n",
      "2016-11-17T23:28:44.966606: step 566, loss 148.756, pre 0.0260827, rec 0.459382, f1 0.0493627\n",
      "2016-11-17T23:28:45.748454: step 567, loss 168.851, pre 0.0303276, rec 0.472486, f1 0.0569968\n",
      "2016-11-17T23:28:46.520865: step 568, loss 160.505, pre 0.0286134, rec 0.467377, f1 0.0539253\n",
      "2016-11-17T23:28:47.289959: step 569, loss 138.032, pre 0.0244661, rec 0.463207, f1 0.0464773\n",
      "2016-11-17T23:28:48.074141: step 570, loss 160.554, pre 0.0273153, rec 0.447931, f1 0.0514906\n",
      "2016-11-17T23:28:48.860712: step 571, loss 154.341, pre 0.0265962, rec 0.450382, f1 0.0502264\n",
      "2016-11-17T23:28:49.634812: step 572, loss 141.548, pre 0.024572, rec 0.453858, f1 0.04662\n",
      "2016-11-17T23:28:50.411891: step 573, loss 129.864, pre 0.0225672, rec 0.451772, f1 0.0429871\n",
      "2016-11-17T23:28:51.218603: step 574, loss 163.945, pre 0.0291266, rec 0.466102, f1 0.0548271\n",
      "2016-11-17T23:28:52.039135: step 575, loss 162.043, pre 0.0288261, rec 0.46504, f1 0.0542871\n",
      "2016-11-17T23:28:52.835839: step 576, loss 154.274, pre 0.0264681, rec 0.447917, f1 0.0499826\n",
      "2016-11-17T23:28:53.847325: step 577, loss 143.765, pre 0.024617, rec 0.447839, f1 0.0466687\n",
      "2016-11-17T23:28:54.694403: step 578, loss 150.923, pre 0.0260123, rec 0.451384, f1 0.0491898\n",
      "2016-11-17T23:28:55.617480: step 579, loss 165.233, pre 0.029425, rec 0.466365, f1 0.0553572\n",
      "2016-11-17T23:28:56.464250: step 580, loss 161.79, pre 0.0291314, rec 0.471947, f1 0.0548755\n",
      "2016-11-17T23:28:57.289621: step 581, loss 173.36, pre 0.0305668, rec 0.461444, f1 0.0573356\n",
      "2016-11-17T23:28:58.145595: step 582, loss 155.974, pre 0.0277812, rec 0.464702, f1 0.0524281\n",
      "2016-11-17T23:28:58.979161: step 583, loss 146.447, pre 0.0266591, rec 0.476676, f1 0.0504941\n",
      "2016-11-17T23:28:59.801001: step 584, loss 182.492, pre 0.0318124, rec 0.455132, f1 0.0594682\n",
      "2016-11-17T23:29:00.585383: step 585, loss 149.328, pre 0.0263223, rec 0.459885, f1 0.0497945\n",
      "2016-11-17T23:29:01.524956: step 586, loss 151.019, pre 0.0262889, rec 0.456414, f1 0.0497144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0c3913cb79c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m#             if current_step % FLAGS.evaluate_every == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0c3913cb79c1>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m             }\n\u001b[1;32m     73\u001b[0m             _, step, summaries, loss, accuracy, scores, predictions, temp, extracted, truth, correct, gold, precision, recall, f1 = sess.run(\n\u001b[0;32m---> 74\u001b[0;31m                 [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions, cnn.temp, cnn.extracted, cnn.truth, cnn.correct, cnn.gold, cnn.precision, cnn.recall, cnn.f1],feed_dict)\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, pre {:g}, rec {:g}, f1 {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=X.shape[1],\n",
    "            num_classes=Y.shape[1],\n",
    "            vocab_size=max([max(sub_X) for sub_X in X]) + 1, # TODO: get vocab size another way\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss) # TODO check cnn.loss\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "#         Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy, scores, predictions, temp, extracted, truth, correct, gold, precision, recall, f1 = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions, cnn.temp, cnn.extracted, cnn.truth, cnn.correct, cnn.gold, cnn.precision, cnn.recall, cnn.f1],feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, pre {:g}, rec {:g}, f1 {:g}\".format(time_str, step, loss, precision, recall, f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "#             print \"type of scores: \", type(scores)\n",
    "#             print type(scores[0])\n",
    "#             print \"scores: \\n\", scores[0]\n",
    "#             print \" \"\n",
    "#             print \"type of predictions: \", type(predictions)\n",
    "#             print \"predictions: \\n\", predictions\n",
    "#             print type(temp)\n",
    "#             print \"temp: \\n\", temp[0]\n",
    "#             print \"gold: \\n\", gold[0]\n",
    "#             print \"extracted: \", extracted\n",
    "#             print \"extracted python: \", sum([sum(score) for score in scores])\n",
    "#             print \"truth: \", truth\n",
    "#             print \"truth python: \", sum([sum(x) for x in gold])\n",
    "#             print \"correct: \", correct\n",
    "#             print \"correct python: \", sum([sum(x) for x in temp])\n",
    "#             print \"precision: \", precision\n",
    "#             print \"recall: \", recall\n",
    "#             print \"f1: \", f1\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy, scores, predictions = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(X, Y)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "#             if current_step % FLAGS.evaluate_every == 0:\n",
    "#                 print(\"\\nEvaluation:\")\n",
    "#                 dev_step(x_dev, y_dev, writer=dev_summary_writer) # from x_dev \n",
    "#                 print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
