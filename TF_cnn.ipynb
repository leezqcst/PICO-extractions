{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess_data import get_all_data_train\n",
    "from TF_preprocess_data import get_1_hot_sentence_encodings\n",
    "from text_cnn import TextCNN\n",
    "import datetime\n",
    "# import data_helpers\n",
    "import time\n",
    "import os\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# dennybritz's Sentance classification using cnn\n",
    "# https://github.com/dennybritz/cnn-text-classification-tf\n",
    "# njl's Sentiment Analysis example \n",
    "# https://github.com/nicholaslocascio/tensorflow-nlp-tutorial/blob/master/sentiment-analysis/Sentiment-RNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=512\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 512, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 512, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# per sentence \n",
    "word_array, tag_array = get_all_data_train(sentences=True)\n",
    "X, Y = get_1_hot_sentence_encodings(word_array, tag_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print Y[0]\n",
    "# print X[0]\n",
    "# print word_array[0]\n",
    "# print tag_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_document_length = len(X[0])\n",
    "# vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "\n",
    "# # Randomly shuffle data\n",
    "# np.random.seed(10)\n",
    "# shuffle_indices = np.random.permutation(np.arange(len(Y)))\n",
    "# x_shuffled = X[shuffle_indices]\n",
    "# y_shuffled = Y[shuffle_indices]\n",
    "\n",
    "# # Split train/test set\n",
    "# # TODO: This is very crude, should use cross-validation\n",
    "# dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(Y)))\n",
    "# x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "# y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "# print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "# print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max([max(sub_X) for sub_X in X]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479691547\n",
      "\n",
      "2016-11-20T20:25:51.155119: step 1, loss 0.973223, pre 0.00653316, rec 0.494624, f1 0.012896\n",
      "2016-11-20T20:25:53.383786: step 2, loss 0.974935, pre 0.00885951, rec 0.443662, f1 0.0173721\n",
      "2016-11-20T20:25:55.592767: step 3, loss 0.970993, pre 0.00715103, rec 0.373134, f1 0.0140331\n",
      "2016-11-20T20:25:57.949103: step 4, loss 0.974958, pre 0.0112061, rec 0.454545, f1 0.0218729\n",
      "2016-11-20T20:26:00.268183: step 5, loss 0.969485, pre 0.00846606, rec 0.421429, f1 0.0165987\n",
      "2016-11-20T20:26:02.551358: step 6, loss 0.97667, pre 0.00921916, rec 0.442953, f1 0.0180624\n",
      "2016-11-20T20:26:04.862907: step 7, loss 0.973193, pre 0.0123804, rec 0.446701, f1 0.0240931\n",
      "2016-11-20T20:26:07.169605: step 8, loss 0.979827, pre 0.00789146, rec 0.431818, f1 0.0154997\n",
      "2016-11-20T20:26:09.683658: step 9, loss 0.973833, pre 0.00693953, rec 0.441441, f1 0.0136643\n",
      "2016-11-20T20:26:12.042543: step 10, loss 0.967121, pre 0.0151732, rec 0.475336, f1 0.0294077\n",
      "2016-11-20T20:26:14.451946: step 11, loss 0.970416, pre 0.0113863, rec 0.427807, f1 0.0221822\n",
      "2016-11-20T20:26:16.817952: step 12, loss 0.976594, pre 0.0103208, rec 0.48366, f1 0.0202103\n",
      "2016-11-20T20:26:19.203754: step 13, loss 0.97203, pre 0.00740635, rec 0.452174, f1 0.014574\n",
      "2016-11-20T20:26:21.630406: step 14, loss 0.973817, pre 0.00679983, rec 0.448598, f1 0.0133966\n",
      "2016-11-20T20:26:24.096924: step 15, loss 0.975085, pre 0.00297999, rec 0.362069, f1 0.00591133\n",
      "2016-11-20T20:26:26.537581: step 16, loss 0.972263, pre 0.0095022, rec 0.449664, f1 0.0186111\n",
      "2016-11-20T20:26:29.041071: step 17, loss 0.97477, pre 0.00775303, rec 0.429688, f1 0.0152312\n",
      "2016-11-20T20:26:31.529081: step 18, loss 0.972949, pre 0.00878187, rec 0.402597, f1 0.0171888\n",
      "2016-11-20T20:26:34.016018: step 19, loss 0.975488, pre 0.00480362, rec 0.53125, f1 0.00952114\n",
      "2016-11-20T20:26:36.398542: step 20, loss 0.970535, pre 0.013885, rec 0.466667, f1 0.0269675\n",
      "2016-11-20T20:26:38.840063: step 21, loss 0.969644, pre 0.00916774, rec 0.42953, f1 0.0179523\n",
      "2016-11-20T20:26:41.396766: step 22, loss 0.972193, pre 0.00669802, rec 0.343066, f1 0.0131395\n",
      "2016-11-20T20:26:43.862818: step 23, loss 0.970967, pre 0.0123387, rec 0.356557, f1 0.023852\n",
      "2016-11-20T20:26:46.293924: step 24, loss 0.972747, pre 0.0112931, rec 0.414508, f1 0.0219871\n",
      "2016-11-20T20:26:48.742917: step 25, loss 0.974758, pre 0.00872625, rec 0.364706, f1 0.0170447\n",
      "2016-11-20T20:26:51.171041: step 26, loss 0.973044, pre 0.0114197, rec 0.421875, f1 0.0222375\n",
      "2016-11-20T20:26:53.619591: step 27, loss 0.974066, pre 0.00902553, rec 0.447552, f1 0.0176942\n",
      "2016-11-20T20:26:56.021967: step 28, loss 0.978341, pre 0.00294613, rec 0.477273, f1 0.00585611\n",
      "2016-11-20T20:26:58.550347: step 29, loss 0.976885, pre 0.00755773, rec 0.406015, f1 0.0148392\n",
      "2016-11-20T20:27:01.011623: step 30, loss 0.971042, pre 0.00940305, rec 0.44898, f1 0.0184203\n",
      "2016-11-20T20:27:03.533511: step 31, loss 0.975286, pre 0.00732188, rec 0.440678, f1 0.0144044\n",
      "2016-11-20T20:27:06.121812: step 32, loss 0.97688, pre 0.0089373, rec 0.460432, f1 0.0175342\n",
      "2016-11-20T20:27:08.680118: step 33, loss 0.974879, pre 0.00872257, rec 0.439716, f1 0.0171058\n",
      "2016-11-20T20:27:11.169871: step 34, loss 0.9776, pre 0.00351321, rec 0.396825, f1 0.00696476\n",
      "2016-11-20T20:27:13.657868: step 35, loss 0.974422, pre 0.00608548, rec 0.438776, f1 0.0120045\n",
      "2016-11-20T20:27:16.106483: step 36, loss 0.972545, pre 0.00949277, rec 0.471831, f1 0.0186111\n",
      "2016-11-20T20:27:18.625406: step 37, loss 0.973095, pre 0.0129359, rec 0.433962, f1 0.0251229\n",
      "2016-11-20T20:27:21.016752: step 38, loss 0.97626, pre 0.00701951, rec 0.403226, f1 0.0137988\n",
      "2016-11-20T20:27:23.468860: step 39, loss 0.973286, pre 0.0045584, rec 0.344086, f1 0.00899761\n",
      "2016-11-20T20:27:25.799235: step 40, loss 0.973845, pre 0.0102817, rec 0.450617, f1 0.0201047\n",
      "2016-11-20T20:27:28.242998: step 41, loss 0.975944, pre 0.0056346, rec 0.4, f1 0.0111127\n",
      "2016-11-20T20:27:30.906443: step 42, loss 0.978385, pre 0.00655418, rec 0.345588, f1 0.0128644\n",
      "2016-11-20T20:27:33.313706: step 43, loss 0.973899, pre 0.00861339, rec 0.409396, f1 0.0168718\n",
      "2016-11-20T20:27:35.691159: step 44, loss 0.973957, pre 0.00805426, rec 0.401408, f1 0.0157917\n",
      "2016-11-20T20:27:38.068477: step 45, loss 0.973361, pre 0.00849017, rec 0.447761, f1 0.0166644\n",
      "2016-11-20T20:27:40.428627: step 46, loss 0.973865, pre 0.00903189, rec 0.387879, f1 0.0176527\n",
      "2016-11-20T20:27:42.857440: step 47, loss 0.976261, pre 0.00451085, rec 0.376471, f1 0.00891489\n",
      "2016-11-20T20:27:45.214349: step 48, loss 0.97647, pre 0.00673684, rec 0.428571, f1 0.0132652\n",
      "2016-11-20T20:27:47.575378: step 49, loss 0.975703, pre 0.00311438, rec 0.354839, f1 0.00617457\n",
      "2016-11-20T20:27:50.181417: step 50, loss 0.975241, pre 0.00438659, rec 0.326316, f1 0.0086568\n",
      "2016-11-20T20:27:53.480845: step 51, loss 0.968867, pre 0.0127107, rec 0.427885, f1 0.0246879\n",
      "2016-11-20T20:27:55.885749: step 52, loss 0.976773, pre 0.00976562, rec 0.4375, f1 0.0191048\n",
      "2016-11-20T20:27:58.278204: step 53, loss 0.979316, pre 0.00694541, rec 0.49505, f1 0.0136986\n",
      "2016-11-20T20:28:00.658892: step 54, loss 0.971737, pre 0.00839858, rec 0.427536, f1 0.0164735\n",
      "2016-11-20T20:28:02.992222: step 55, loss 0.970973, pre 0.00842135, rec 0.375796, f1 0.0164735\n",
      "2016-11-20T20:28:05.345542: step 56, loss 0.975217, pre 0.00634786, rec 0.409091, f1 0.0125017\n",
      "2016-11-20T20:28:07.657338: step 57, loss 0.970418, pre 0.00631458, rec 0.392857, f1 0.0124294\n",
      "2016-11-20T20:28:09.995261: step 58, loss 0.972235, pre 0.00852394, rec 0.410959, f1 0.0167015\n",
      "2016-11-20T20:28:12.365544: step 59, loss 0.971859, pre 0.0145091, rec 0.418699, f1 0.0280463\n",
      "2016-11-20T20:28:15.000378: step 60, loss 0.972464, pre 0.00513845, rec 0.428571, f1 0.0101551\n",
      "2016-11-20T20:28:17.497180: step 61, loss 0.974029, pre 0.00581313, rec 0.471264, f1 0.0114846\n",
      "2016-11-20T20:28:19.860572: step 62, loss 0.972045, pre 0.0143622, rec 0.386364, f1 0.0276948\n",
      "2016-11-20T20:28:22.201563: step 63, loss 0.974398, pre 0.00622612, rec 0.427184, f1 0.0122734\n",
      "2016-11-20T20:28:24.602553: step 64, loss 0.97327, pre 0.0112724, rec 0.386473, f1 0.0219058\n",
      "2016-11-20T20:28:27.575905: step 65, loss 0.976311, pre 0.00854103, rec 0.476562, f1 0.0167813\n",
      "2016-11-20T20:28:30.686226: step 66, loss 0.976471, pre 0.00422773, rec 0.447761, f1 0.00837638\n",
      "2016-11-20T20:28:33.939855: step 67, loss 0.971992, pre 0.0067028, rec 0.394958, f1 0.0131819\n",
      "2016-11-20T20:28:36.444325: step 68, loss 0.971982, pre 0.00951164, rec 0.41875, f1 0.0186008\n",
      "2016-11-20T20:28:39.008307: step 69, loss 0.976373, pre 0.00478873, rec 0.343434, f1 0.00944576\n",
      "2016-11-20T20:28:42.223664: step 70, loss 0.973469, pre 0.00512018, rec 0.439024, f1 0.0101223\n",
      "2016-11-20T20:28:45.104167: step 71, loss 0.975341, pre 0.00564653, rec 0.43956, f1 0.0111498\n",
      "2016-11-20T20:28:48.025216: step 72, loss 0.975971, pre 0.00910364, rec 0.448276, f1 0.0178449\n",
      "2016-11-20T20:28:50.933001: step 73, loss 0.972324, pre 0.00824098, rec 0.42029, f1 0.016165\n",
      "2016-11-20T20:28:54.082509: step 74, loss 0.973079, pre 0.00849858, rec 0.361446, f1 0.0166067\n",
      "2016-11-20T20:28:57.656894: step 75, loss 0.973377, pre 0.0104372, rec 0.402174, f1 0.0203464\n",
      "2016-11-20T20:29:01.005889: step 76, loss 0.97304, pre 0.0059744, rec 0.428571, f1 0.0117845\n",
      "2016-11-20T20:29:04.197426: step 77, loss 0.971848, pre 0.00429738, rec 0.384615, f1 0.00849979\n",
      "2016-11-20T20:29:07.562788: step 78, loss 0.976154, pre 0.00535136, rec 0.358491, f1 0.0105453\n",
      "2016-11-20T20:29:10.552782: step 79, loss 0.970318, pre 0.0137569, rec 0.47549, f1 0.0267402\n",
      "2016-11-20T20:29:12.886832: step 80, loss 0.968688, pre 0.0132781, rec 0.424658, f1 0.0257511\n",
      "2016-11-20T20:29:15.164421: step 81, loss 0.974356, pre 0.00440028, rec 0.326316, f1 0.00868347\n",
      "2016-11-20T20:29:17.524942: step 82, loss 0.973521, pre 0.00848536, rec 0.451128, f1 0.0166574\n",
      "2016-11-20T20:29:19.813188: step 83, loss 0.972017, pre 0.010905, rec 0.452941, f1 0.0212972\n",
      "2016-11-20T20:29:22.095464: step 84, loss 0.974908, pre 0.00969101, rec 0.401163, f1 0.0189248\n",
      "2016-11-20T20:29:24.423496: step 85, loss 0.969986, pre 0.0078853, rec 0.404412, f1 0.015469\n",
      "2016-11-20T20:29:26.726751: step 86, loss 0.971781, pre 0.00515095, rec 0.428571, f1 0.0101796\n",
      "2016-11-20T20:29:29.099752: step 87, loss 0.972057, pre 0.00656018, rec 0.442308, f1 0.0129286\n",
      "2016-11-20T20:29:31.364182: step 88, loss 0.977445, pre 0.00393424, rec 0.4375, f1 0.00779836\n",
      "2016-11-20T20:29:33.625788: step 89, loss 0.975061, pre 0.00746689, rec 0.398496, f1 0.0146591\n",
      "2016-11-20T20:29:35.924472: step 90, loss 0.979295, pre 0.0123916, rec 0.418605, f1 0.0240706\n",
      "2016-11-20T20:29:38.211724: step 91, loss 0.973524, pre 0.00778706, rec 0.454545, f1 0.0153118\n",
      "2016-11-20T20:29:40.480313: step 92, loss 0.975001, pre 0.010516, rec 0.480769, f1 0.0205818\n",
      "2016-11-20T20:29:42.776288: step 93, loss 0.9747, pre 0.00495751, rec 0.402299, f1 0.00979432\n",
      "2016-11-20T20:29:45.059715: step 94, loss 0.972537, pre 0.00725256, rec 0.418033, f1 0.0142578\n",
      "2016-11-20T20:29:47.323909: step 95, loss 0.973432, pre 0.00876697, rec 0.382716, f1 0.0171413\n",
      "2016-11-20T20:29:49.579924: step 96, loss 0.971444, pre 0.00938967, rec 0.370787, f1 0.0183155\n",
      "2016-11-20T20:29:51.881606: step 97, loss 0.976329, pre 0.00366921, rec 0.45614, f1 0.00727985\n",
      "2016-11-20T20:29:54.154166: step 98, loss 0.974914, pre 0.00142349, rec 0.333333, f1 0.00283487\n",
      "2016-11-20T20:29:56.409509: step 99, loss 0.968355, pre 0.00991807, rec 0.5, f1 0.0194503\n",
      "2016-11-20T20:29:58.736417: step 100, loss 0.970546, pre 0.00417446, rec 0.371795, f1 0.00825623\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479691547/checkpoints/model-100\n",
      "\n",
      "2016-11-20T20:30:01.784582: step 101, loss 0.976303, pre 0.00632378, rec 0.416667, f1 0.0124585\n",
      "2016-11-20T20:30:04.013617: step 102, loss 0.973356, pre 0.0073717, rec 0.464286, f1 0.014513\n",
      "2016-11-20T20:30:06.317988: step 103, loss 0.968549, pre 0.00948685, rec 0.452055, f1 0.0185837\n",
      "2016-11-20T20:30:08.712028: step 104, loss 0.97376, pre 0.00483298, rec 0.357895, f1 0.00953717\n",
      "2016-11-20T20:30:10.992185: step 105, loss 0.974509, pre 0.00650361, rec 0.422018, f1 0.0128098\n",
      "2016-11-20T20:30:13.281447: step 106, loss 0.973182, pre 0.00905361, rec 0.385542, f1 0.0176918\n",
      "2016-11-20T20:30:15.561499: step 107, loss 0.976876, pre 0.00352212, rec 0.362319, f1 0.00697642\n",
      "2016-11-20T20:30:17.838531: step 108, loss 0.974297, pre 0.0117829, rec 0.502994, f1 0.0230263\n",
      "2016-11-20T20:30:20.156948: step 109, loss 0.970759, pre 0.00758118, rec 0.355705, f1 0.0148459\n",
      "2016-11-20T20:30:22.440777: step 110, loss 0.970589, pre 0.00786838, rec 0.447154, f1 0.0154646\n",
      "2016-11-20T20:30:24.695031: step 111, loss 0.975679, pre 0.00325548, rec 0.333333, f1 0.006448\n",
      "2016-11-20T20:30:26.978300: step 112, loss 0.971684, pre 0.00755632, rec 0.430894, f1 0.0148522\n",
      "2016-11-20T20:30:29.570159: step 113, loss 0.972823, pre 0.00583796, rec 0.394231, f1 0.0115055\n",
      "2016-11-20T20:30:32.292521: step 114, loss 0.972526, pre 0.00570288, rec 0.444444, f1 0.0112613\n",
      "2016-11-20T20:30:34.901237: step 115, loss 0.976882, pre 0.0064498, rec 0.403509, f1 0.0126967\n",
      "2016-11-20T20:30:37.153348: step 116, loss 0.973193, pre 0.00807594, rec 0.422222, f1 0.0158487\n",
      "2016-11-20T20:30:39.439840: step 117, loss 0.973258, pre 0.00793426, rec 0.424242, f1 0.0155772\n",
      "2016-11-20T20:30:41.744670: step 118, loss 0.972983, pre 0.00836168, rec 0.475806, f1 0.0164345\n",
      "2016-11-20T20:30:44.008098: step 119, loss 0.970847, pre 0.00983046, rec 0.445161, f1 0.0192361\n",
      "2016-11-20T20:30:46.282627: step 120, loss 0.976574, pre 0.00478536, rec 0.369565, f1 0.00944838\n",
      "2016-11-20T20:30:48.547736: step 121, loss 0.974059, pre 0.00860852, rec 0.472868, f1 0.0169092\n",
      "2016-11-20T20:30:50.901461: step 122, loss 0.970389, pre 0.00970597, rec 0.404762, f1 0.0189573\n",
      "2016-11-20T20:30:53.276720: step 123, loss 0.972618, pre 0.0115754, rec 0.438503, f1 0.0225554\n",
      "2016-11-20T20:30:55.547536: step 124, loss 0.972864, pre 0.00583713, rec 0.372727, f1 0.0114943\n",
      "2016-11-20T20:30:57.892690: step 125, loss 0.971709, pre 0.0117514, rec 0.430052, f1 0.0228776\n",
      "2016-11-20T20:31:00.229640: step 126, loss 0.974411, pre 0.00706514, rec 0.362319, f1 0.01386\n",
      "2016-11-20T20:31:02.516474: step 127, loss 0.972207, pre 0.00500143, rec 0.460526, f1 0.00989539\n",
      "2016-11-20T20:31:04.766117: step 128, loss 0.976734, pre 0.00728495, rec 0.460177, f1 0.0143429\n",
      "2016-11-20T20:31:07.062267: step 129, loss 0.968476, pre 0.00735825, rec 0.372263, f1 0.0144312\n",
      "2016-11-20T20:31:09.449561: step 130, loss 0.97409, pre 0.011378, rec 0.455056, f1 0.0222009\n",
      "2016-11-20T20:31:11.818387: step 131, loss 0.974804, pre 0.00733427, rec 0.468468, f1 0.0144424\n",
      "2016-11-20T20:31:14.088596: step 132, loss 0.971152, pre 0.00785266, rec 0.413534, f1 0.0154126\n",
      "2016-11-20T20:31:16.360513: step 133, loss 0.971697, pre 0.00839977, rec 0.40411, f1 0.0164575\n",
      "2016-11-20T20:31:18.628855: step 134, loss 0.97683, pre 0.00742089, rec 0.441667, f1 0.0145965\n",
      "2016-11-20T20:31:20.928272: step 135, loss 0.973213, pre 0.00933126, rec 0.409938, f1 0.0182472\n",
      "2016-11-20T20:31:23.188643: step 136, loss 0.976584, pre 0.00631756, rec 0.401786, f1 0.0124395\n",
      "2016-11-20T20:31:25.471570: step 137, loss 0.977635, pre 0.00490677, rec 0.3125, f1 0.00966184\n",
      "2016-11-20T20:31:27.769126: step 138, loss 0.975368, pre 0.00912281, rec 0.474453, f1 0.0179014\n",
      "2016-11-20T20:31:30.038039: step 139, loss 0.972177, pre 0.0133916, rec 0.448113, f1 0.026006\n",
      "2016-11-20T20:31:32.399982: step 140, loss 0.975765, pre 0.00619631, rec 0.419048, f1 0.012212\n",
      "2016-11-20T20:31:34.642213: step 141, loss 0.97411, pre 0.00581148, rec 0.386792, f1 0.0114509\n",
      "2016-11-20T20:31:36.938654: step 142, loss 0.974217, pre 0.0117862, rec 0.449198, f1 0.0229696\n",
      "2016-11-20T20:31:39.206354: step 143, loss 0.976399, pre 0.0107392, rec 0.398964, f1 0.0209154\n",
      "2016-11-20T20:31:41.499372: step 144, loss 0.975633, pre 0.007174, rec 0.395349, f1 0.0140923\n",
      "2016-11-20T20:31:43.731995: step 145, loss 0.974975, pre 0.0131248, rec 0.41048, f1 0.0254363\n",
      "2016-11-20T20:31:45.987317: step 146, loss 0.972613, pre 0.00612448, rec 0.373913, f1 0.0120516\n",
      "2016-11-20T20:31:48.302014: step 147, loss 0.976649, pre 0.00617717, rec 0.483516, f1 0.0121985\n",
      "2016-11-20T20:31:50.572422: step 148, loss 0.967661, pre 0.0127653, rec 0.447236, f1 0.0248222\n",
      "2016-11-20T20:31:52.861459: step 149, loss 0.974715, pre 0.00509771, rec 0.537313, f1 0.0100996\n",
      "2016-11-20T20:31:55.160699: step 150, loss 0.973257, pre 0.0104417, rec 0.453988, f1 0.0204138\n",
      "2016-11-20T20:31:57.425343: step 151, loss 0.973695, pre 0.00497512, rec 0.416667, f1 0.00983284\n",
      "2016-11-20T20:31:59.733372: step 152, loss 0.971927, pre 0.00684541, rec 0.387097, f1 0.0134529\n",
      "2016-11-20T20:32:02.040563: step 153, loss 0.971021, pre 0.00630102, rec 0.4, f1 0.0124066\n",
      "2016-11-20T20:32:04.297359: step 154, loss 0.974489, pre 0.00776069, rec 0.482456, f1 0.0152757\n",
      "2016-11-20T20:32:06.590210: step 155, loss 0.977865, pre 0.00587413, rec 0.456522, f1 0.011599\n",
      "2016-11-20T20:32:08.864978: step 156, loss 0.973594, pre 0.00624379, rec 0.385965, f1 0.0122888\n",
      "2016-11-20T20:32:11.121681: step 157, loss 0.970531, pre 0.012775, rec 0.491803, f1 0.0249032\n",
      "2016-11-20T20:32:13.462522: step 158, loss 0.973603, pre 0.0102904, rec 0.41954, f1 0.0200881\n",
      "2016-11-20T20:32:15.809326: step 159, loss 0.974501, pre 0.0110862, rec 0.39899, f1 0.0215729\n",
      "2016-11-20T20:32:18.099815: step 160, loss 0.972932, pre 0.00682109, rec 0.40678, f1 0.0134172\n",
      "2016-11-20T20:32:20.380852: step 161, loss 0.977001, pre 0.00893356, rec 0.447552, f1 0.0175174\n",
      "2016-11-20T20:32:22.672212: step 162, loss 0.972908, pre 0.0112867, rec 0.45977, f1 0.0220325\n",
      "2016-11-20T20:32:24.924251: step 163, loss 0.967726, pre 0.0126219, rec 0.4, f1 0.0244716\n",
      "2016-11-20T20:32:27.243115: step 164, loss 0.972081, pre 0.0132581, rec 0.447619, f1 0.0257534\n",
      "2016-11-20T20:32:29.508707: step 165, loss 0.976054, pre 0.00841043, rec 0.394737, f1 0.0164699\n",
      "2016-11-20T20:32:31.838853: step 166, loss 0.978021, pre 0.00476124, rec 0.453333, f1 0.0094235\n",
      "2016-11-20T20:32:34.093058: step 167, loss 0.970943, pre 0.00996725, rec 0.463576, f1 0.0195149\n",
      "2016-11-20T20:32:36.378311: step 168, loss 0.975412, pre 0.00592467, rec 0.411765, f1 0.0116813\n",
      "2016-11-20T20:32:38.656742: step 169, loss 0.970999, pre 0.00573394, rec 0.366972, f1 0.0112915\n",
      "2016-11-20T20:32:40.911056: step 170, loss 0.973141, pre 0.0090549, rec 0.432432, f1 0.0177384\n",
      "2016-11-20T20:32:43.174678: step 171, loss 0.974261, pre 0.00860245, rec 0.54955, f1 0.0169397\n",
      "2016-11-20T20:32:45.528346: step 172, loss 0.968522, pre 0.0146807, rec 0.407115, f1 0.0283395\n",
      "2016-11-20T20:32:47.792987: step 173, loss 0.975025, pre 0.00607259, rec 0.447917, f1 0.0119827\n",
      "2016-11-20T20:32:50.052271: step 174, loss 0.974618, pre 0.00747848, rec 0.469027, f1 0.0147222\n",
      "2016-11-20T20:32:52.313454: step 175, loss 0.975817, pre 0.00521788, rec 0.393617, f1 0.0102992\n",
      "2016-11-20T20:32:54.580878: step 176, loss 0.976582, pre 0.00268324, rec 0.339286, f1 0.00532437\n",
      "2016-11-20T20:32:56.853190: step 177, loss 0.969581, pre 0.00860462, rec 0.416667, f1 0.016861\n",
      "2016-11-20T20:32:59.129308: step 178, loss 0.971807, pre 0.0111914, rec 0.403061, f1 0.0217781\n",
      "2016-11-20T20:33:01.415039: step 179, loss 0.97486, pre 0.00747216, rec 0.395522, f1 0.0146672\n",
      "2016-11-20T20:33:03.660873: step 180, loss 0.976692, pre 0.00548061, rec 0.433333, f1 0.0108243\n",
      "2016-11-20T20:33:05.925795: step 181, loss 0.969671, pre 0.00831899, rec 0.432836, f1 0.0163242\n",
      "2016-11-20T20:33:08.196247: step 182, loss 0.975377, pre 0.00884583, rec 0.459854, f1 0.0173578\n",
      "2016-11-20T20:33:10.431554: step 183, loss 0.975353, pre 0.00898498, rec 0.453901, f1 0.0176211\n",
      "2016-11-20T20:33:12.696558: step 184, loss 0.971983, pre 0.0113234, rec 0.506329, f1 0.0221515\n",
      "2016-11-20T20:33:14.955731: step 185, loss 0.975171, pre 0.0102341, rec 0.424419, f1 0.0199863\n",
      "2016-11-20T20:33:17.330739: step 186, loss 0.974681, pre 0.00803383, rec 0.425373, f1 0.0157698\n",
      "2016-11-20T20:33:19.600375: step 187, loss 0.972593, pre 0.0117149, rec 0.44385, f1 0.0228273\n",
      "2016-11-20T20:33:21.881789: step 188, loss 0.973705, pre 0.00652205, rec 0.403509, f1 0.0128366\n",
      "2016-11-20T20:33:24.131010: step 189, loss 0.972813, pre 0.00864635, rec 0.417808, f1 0.0169421\n",
      "2016-11-20T20:33:26.399546: step 190, loss 0.969967, pre 0.0109843, rec 0.466667, f1 0.0214634\n",
      "2016-11-20T20:33:28.712490: step 191, loss 0.974224, pre 0.00538778, rec 0.330435, f1 0.0106027\n",
      "2016-11-20T20:33:30.958155: step 192, loss 0.973461, pre 0.0140174, rec 0.425532, f1 0.0271407\n",
      "2016-11-20T20:33:33.253629: step 193, loss 0.97811, pre 0.00448179, rec 0.444444, f1 0.0088741\n",
      "2016-11-20T20:33:35.523562: step 194, loss 0.977233, pre 0.00920117, rec 0.468085, f1 0.0180476\n",
      "2016-11-20T20:33:37.796848: step 195, loss 0.97512, pre 0.00871521, rec 0.413333, f1 0.0170705\n",
      "2016-11-20T20:33:40.068453: step 196, loss 0.972492, pre 0.00865617, rec 0.414966, f1 0.0169586\n",
      "2016-11-20T20:33:42.317185: step 197, loss 0.975084, pre 0.00551159, rec 0.378641, f1 0.010865\n",
      "2016-11-20T20:33:44.573711: step 198, loss 0.976665, pre 0.00631579, rec 0.432692, f1 0.0124499\n",
      "2016-11-20T20:33:46.844532: step 199, loss 0.971311, pre 0.0128786, rec 0.469072, f1 0.0250689\n",
      "2016-11-20T20:33:49.139342: step 200, loss 0.974709, pre 0.00900521, rec 0.457143, f1 0.0176625\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479691547/checkpoints/model-200\n",
      "\n",
      "2016-11-20T20:33:52.333909: step 201, loss 0.970601, pre 0.0155389, rec 0.415094, f1 0.0299564\n",
      "2016-11-20T20:33:54.543280: step 202, loss 0.972336, pre 0.00725772, rec 0.51, f1 0.0143118\n",
      "2016-11-20T20:33:56.885874: step 203, loss 0.969986, pre 0.0078853, rec 0.504587, f1 0.0155279\n",
      "2016-11-20T20:33:59.291340: step 204, loss 0.972691, pre 0.00682691, rec 0.428571, f1 0.0134397\n",
      "2016-11-20T20:34:01.568114: step 205, loss 0.976941, pre 0.00338123, rec 0.421053, f1 0.0067086\n",
      "2016-11-20T20:34:03.838039: step 206, loss 0.972784, pre 0.010181, rec 0.428571, f1 0.0198895\n",
      "2016-11-20T20:34:06.130426: step 207, loss 0.973254, pre 0.0111346, rec 0.50641, f1 0.0217901\n",
      "2016-11-20T20:34:08.424292: step 208, loss 0.97182, pre 0.0076934, rec 0.4, f1 0.0150964\n",
      "2016-11-20T20:34:10.702744: step 209, loss 0.970165, pre 0.00731393, rec 0.481132, f1 0.0144088\n",
      "2016-11-20T20:34:12.963603: step 210, loss 0.977463, pre 0.00337505, rec 0.470588, f1 0.00670204\n",
      "2016-11-20T20:34:15.230050: step 211, loss 0.979338, pre 0.00321409, rec 0.38983, f1 0.00637561\n",
      "2016-11-20T20:34:17.470534: step 212, loss 0.975662, pre 0.00564016, rec 0.444444, f1 0.011139\n",
      "2016-11-20T20:34:19.735217: step 213, loss 0.973578, pre 0.00610362, rec 0.325758, f1 0.0119827\n",
      "2016-11-20T20:34:22.116177: step 214, loss 0.969514, pre 0.00945423, rec 0.44898, f1 0.0185185\n",
      "2016-11-20T20:34:24.393161: step 215, loss 0.970261, pre 0.00745306, rec 0.45614, f1 0.0146665\n",
      "2016-11-20T20:34:26.712788: step 216, loss 0.972232, pre 0.00922378, rec 0.439189, f1 0.0180681\n",
      "2016-11-20T20:34:29.284048: step 217, loss 0.97616, pre 0.00576977, rec 0.369369, f1 0.0113621\n",
      "2016-11-20T20:34:31.555903: step 218, loss 0.975036, pre 0.00326287, rec 0.365079, f1 0.00646794\n",
      "2016-11-20T20:34:33.818734: step 219, loss 0.973178, pre 0.00793651, rec 0.4, f1 0.0155642\n",
      "2016-11-20T20:34:36.089746: step 220, loss 0.970038, pre 0.0112616, rec 0.417989, f1 0.0219323\n",
      "2016-11-20T20:34:38.360107: step 221, loss 0.974939, pre 0.00996631, rec 0.398876, f1 0.0194467\n",
      "2016-11-20T20:34:40.629173: step 222, loss 0.974722, pre 0.00551861, rec 0.419355, f1 0.0108939\n",
      "2016-11-20T20:34:42.934417: step 223, loss 0.972539, pre 0.00654898, rec 0.474227, f1 0.0129195\n",
      "2016-11-20T20:34:45.195439: step 224, loss 0.974339, pre 0.00678733, rec 0.421053, f1 0.0133593\n",
      "2016-11-20T20:34:47.481572: step 225, loss 0.974376, pre 0.00566572, rec 0.366972, f1 0.0111592\n",
      "2016-11-20T20:34:49.765489: step 226, loss 0.977031, pre 0.0074157, rec 0.414062, f1 0.0145704\n",
      "2016-11-20T20:34:52.034347: step 227, loss 0.977783, pre 0.00836004, rec 0.582524, f1 0.0164835\n",
      "2016-11-20T20:34:54.294573: step 228, loss 0.977544, pre 0.0033741, rec 0.393443, f1 0.00669083\n",
      "2016-11-20T20:34:56.566635: step 229, loss 0.97725, pre 0.00685698, rec 0.445455, f1 0.0135061\n",
      "2016-11-20T20:34:58.868976: step 230, loss 0.975072, pre 0.00649076, rec 0.326241, f1 0.0127283\n",
      "2016-11-20T20:35:01.153058: step 231, loss 0.974989, pre 0.00719019, rec 0.455357, f1 0.0141568\n",
      "2016-11-20T20:35:03.410450: step 232, loss 0.970592, pre 0.0115188, rec 0.409091, f1 0.0224066\n",
      "2016-11-20T20:35:05.655057: step 233, loss 0.977624, pre 0.00337316, rec 0.393443, f1 0.00668896\n",
      "2016-11-20T20:35:07.941949: step 234, loss 0.977903, pre 0.00406732, rec 0.42029, f1 0.00805668\n",
      "2016-11-20T20:35:10.278266: step 235, loss 0.975302, pre 0.00746059, rec 0.420635, f1 0.0146611\n",
      "2016-11-20T20:35:12.562666: step 236, loss 0.974585, pre 0.00789734, rec 0.405797, f1 0.0154932\n",
      "2016-11-20T20:35:14.816640: step 237, loss 0.974062, pre 0.00791184, rec 0.408759, f1 0.0155232\n",
      "2016-11-20T20:35:17.103278: step 238, loss 0.975948, pre 0.00674916, rec 0.410256, f1 0.0132798\n",
      "2016-11-20T20:35:19.360210: step 239, loss 0.971411, pre 0.00287604, rec 0.384615, f1 0.00570939\n",
      "2016-11-20T20:35:21.641876: step 240, loss 0.971357, pre 0.0089718, rec 0.398734, f1 0.0175487\n",
      "2016-11-20T20:35:23.896327: step 241, loss 0.973494, pre 0.00932203, rec 0.409938, f1 0.0182295\n",
      "2016-11-20T20:35:26.138416: step 242, loss 0.975209, pre 0.0041059, rec 0.460317, f1 0.00813921\n",
      "2016-11-20T20:35:28.474167: step 243, loss 0.972551, pre 0.0055611, rec 0.428571, f1 0.0109797\n",
      "2016-11-20T20:35:31.009263: step 244, loss 0.972344, pre 0.0094995, rec 0.378531, f1 0.0185339\n",
      "2016-11-20T20:35:33.280266: step 245, loss 0.973891, pre 0.00637755, rec 0.401786, f1 0.0125558\n",
      "2016-11-20T20:35:35.536653: step 246, loss 0.971717, pre 0.00713267, rec 0.431034, f1 0.0140331\n",
      "2016-11-20T20:35:37.882170: step 247, loss 0.973108, pre 0.00947398, rec 0.424051, f1 0.0185339\n",
      "2016-11-20T20:35:40.150181: step 248, loss 0.971492, pre 0.0116213, rec 0.443243, f1 0.0226488\n",
      "2016-11-20T20:35:42.385261: step 249, loss 0.973495, pre 0.00680755, rec 0.390244, f1 0.0133817\n",
      "2016-11-20T20:35:44.687619: step 250, loss 0.971857, pre 0.00839499, rec 0.385621, f1 0.0164323\n",
      "2016-11-20T20:35:47.034335: step 251, loss 0.968231, pre 0.0106291, rec 0.43787, f1 0.0207545\n",
      "2016-11-20T20:35:49.305737: step 252, loss 0.972657, pre 0.00724947, rec 0.443478, f1 0.0142657\n",
      "2016-11-20T20:35:51.571173: step 253, loss 0.971422, pre 0.00882939, rec 0.462687, f1 0.0173281\n",
      "2016-11-20T20:35:53.851592: step 254, loss 0.974938, pre 0.00816556, rec 0.439394, f1 0.0160332\n",
      "2016-11-20T20:35:56.117151: step 255, loss 0.970783, pre 0.0074392, rec 0.344371, f1 0.0145638\n",
      "2016-11-20T20:35:58.432369: step 256, loss 0.966034, pre 0.0159323, rec 0.422053, f1 0.0307054\n",
      "2016-11-20T20:36:00.704647: step 257, loss 0.974547, pre 0.00720136, rec 0.5, f1 0.0141982\n",
      "2016-11-20T20:36:02.970511: step 258, loss 0.978857, pre 0.00252242, rec 0.305085, f1 0.00500347\n",
      "2016-11-20T20:36:05.242079: step 259, loss 0.975953, pre 0.00535513, rec 0.408602, f1 0.0105717\n",
      "2016-11-20T20:36:07.523925: step 260, loss 0.975743, pre 0.00993841, rec 0.455128, f1 0.0194521\n",
      "2016-11-20T20:36:09.778693: step 261, loss 0.976656, pre 0.00659371, rec 0.370079, f1 0.0129566\n",
      "2016-11-20T20:36:12.033636: step 262, loss 0.97283, pre 0.0105962, rec 0.446429, f1 0.0207011\n",
      "2016-11-20T20:36:14.291498: step 263, loss 0.975326, pre 0.00732085, rec 0.419355, f1 0.0143905\n",
      "2016-11-20T20:36:16.626378: step 264, loss 0.975254, pre 0.0052282, rec 0.45679, f1 0.0103381\n",
      "2016-11-20T20:36:18.886837: step 265, loss 0.973978, pre 0.0111064, rec 0.42246, f1 0.0216438\n",
      "2016-11-20T20:36:21.142397: step 266, loss 0.976447, pre 0.00436804, rec 0.413333, f1 0.00864473\n",
      "2016-11-20T20:36:23.409101: step 267, loss 0.972538, pre 0.00907415, rec 0.423841, f1 0.0177679\n",
      "2016-11-20T20:36:25.634477: step 268, loss 0.97306, pre 0.0115574, rec 0.468571, f1 0.0225585\n",
      "2016-11-20T20:36:27.959878: step 269, loss 0.974241, pre 0.0116475, rec 0.427835, f1 0.0226776\n",
      "2016-11-20T20:36:30.264731: step 270, loss 0.971339, pre 0.0095333, rec 0.387283, f1 0.0186085\n",
      "2016-11-20T20:36:32.534610: step 271, loss 0.978023, pre 0.00656241, rec 0.398305, f1 0.0129121\n",
      "2016-11-20T20:36:34.815897: step 272, loss 0.971412, pre 0.00473118, rec 0.402439, f1 0.00935242\n",
      "2016-11-20T20:36:37.104135: step 273, loss 0.966314, pre 0.0140946, rec 0.417021, f1 0.0272677\n",
      "2016-11-20T20:36:39.424497: step 274, loss 0.972596, pre 0.00851305, rec 0.387097, f1 0.0166597\n",
      "2016-11-20T20:36:41.693223: step 275, loss 0.966993, pre 0.0129366, rec 0.466321, f1 0.0251748\n",
      "2016-11-20T20:36:43.924789: step 276, loss 0.97809, pre 0.00573106, rec 0.5125, f1 0.0113354\n",
      "2016-11-20T20:36:46.288106: step 277, loss 0.971766, pre 0.00244323, rec 0.414634, f1 0.00485784\n",
      "2016-11-20T20:36:48.576810: step 278, loss 0.975208, pre 0.00662719, rec 0.419643, f1 0.0130483\n",
      "2016-11-20T20:36:50.854354: step 279, loss 0.976565, pre 0.00936016, rec 0.411043, f1 0.0183035\n",
      "2016-11-20T20:36:53.124910: step 280, loss 0.976037, pre 0.00646885, rec 0.433962, f1 0.0127477\n",
      "2016-11-20T20:36:55.379734: step 281, loss 0.971564, pre 0.00938567, rec 0.4, f1 0.018341\n",
      "2016-11-20T20:36:57.701246: step 282, loss 0.977896, pre 0.00615041, rec 0.478261, f1 0.0121446\n",
      "2016-11-20T20:36:59.995350: step 283, loss 0.973085, pre 0.0089172, rec 0.431507, f1 0.0174733\n",
      "2016-11-20T20:37:02.275779: step 284, loss 0.973745, pre 0.00652112, rec 0.418182, f1 0.012842\n",
      "2016-11-20T20:37:04.548136: step 285, loss 0.975376, pre 0.00704126, rec 0.49505, f1 0.013885\n",
      "2016-11-20T20:37:06.807143: step 286, loss 0.975254, pre 0.0052282, rec 0.406593, f1 0.0103237\n",
      "2016-11-20T20:37:09.152188: step 287, loss 0.974624, pre 0.00608118, rec 0.483146, f1 0.0120112\n",
      "2016-11-20T20:37:11.445706: step 288, loss 0.972762, pre 0.00710631, rec 0.381679, f1 0.0139528\n",
      "2016-11-20T20:37:13.694023: step 289, loss 0.969033, pre 0.0113019, rec 0.456647, f1 0.0220578\n",
      "2016-11-20T20:37:15.991859: step 290, loss 0.973977, pre 0.00930626, rec 0.385965, f1 0.0181743\n",
      "2016-11-20T20:37:18.341649: step 291, loss 0.975648, pre 0.00731261, rec 0.382353, f1 0.0143508\n",
      "2016-11-20T20:37:20.642867: step 292, loss 0.974769, pre 0.00593807, rec 0.47191, f1 0.0117286\n",
      "2016-11-20T20:37:22.908539: step 293, loss 0.973122, pre 0.0121076, rec 0.457447, f1 0.0235907\n",
      "2016-11-20T20:37:25.183365: step 294, loss 0.972492, pre 0.00865617, rec 0.432624, f1 0.0169727\n",
      "2016-11-20T20:37:27.490769: step 295, loss 0.976391, pre 0.00853863, rec 0.393548, f1 0.0167146\n",
      "2016-11-20T20:37:29.766059: step 296, loss 0.974819, pre 0.00747321, rec 0.464912, f1 0.01471\n",
      "2016-11-20T20:37:32.008404: step 297, loss 0.971751, pre 0.0110544, rec 0.464286, f1 0.0215947\n",
      "2016-11-20T20:37:34.270508: step 298, loss 0.97129, pre 0.00728467, rec 0.398438, f1 0.0143078\n",
      "2016-11-20T20:37:36.611264: step 299, loss 0.972532, pre 0.00865494, rec 0.480315, f1 0.0170035\n",
      "2016-11-20T20:37:38.923545: step 300, loss 0.971784, pre 0.00881809, rec 0.449275, f1 0.0172967\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1479691547/checkpoints/model-300\n",
      "\n",
      "2016-11-20T20:37:41.939589: step 301, loss 0.97195, pre 0.00486409, rec 0.311927, f1 0.00957881\n",
      "2016-11-20T20:37:44.243508: step 302, loss 0.971519, pre 0.0107847, rec 0.415301, f1 0.0210235\n",
      "2016-11-20T20:37:46.601001: step 303, loss 0.970885, pre 0.0105293, rec 0.453988, f1 0.0205813\n",
      "2016-11-20T20:37:48.872273: step 304, loss 0.974539, pre 0.00928793, rec 0.511628, f1 0.0182446\n",
      "2016-11-20T20:37:51.157211: step 305, loss 0.975894, pre 0.00591466, rec 0.392523, f1 0.0116537\n",
      "2016-11-20T20:37:53.484076: step 306, loss 0.972603, pre 0.00893237, rec 0.484615, f1 0.0175414\n",
      "2016-11-20T20:37:55.735719: step 307, loss 0.976213, pre 0.00660391, rec 0.370079, f1 0.0129763\n",
      "2016-11-20T20:37:58.053343: step 308, loss 0.971902, pre 0.00951434, rec 0.465278, f1 0.0186474\n",
      "2016-11-20T20:38:00.371269: step 309, loss 0.972236, pre 0.00599144, rec 0.415842, f1 0.0118127\n",
      "2016-11-20T20:38:02.638322: step 310, loss 0.969736, pre 0.00817556, rec 0.487179, f1 0.0160813\n",
      "2016-11-20T20:38:04.922646: step 311, loss 0.966243, pre 0.0224177, rec 0.434066, f1 0.0426336\n",
      "2016-11-20T20:38:07.194160: step 312, loss 0.97206, pre 0.0058538, rec 0.41, f1 0.0115428\n",
      "2016-11-20T20:38:09.459426: step 313, loss 0.97164, pre 0.0107801, rec 0.444444, f1 0.0210497\n",
      "2016-11-20T20:38:11.787020: step 314, loss 0.976754, pre 0.00603593, rec 0.413462, f1 0.0118982\n",
      "2016-11-20T20:38:14.065652: step 315, loss 0.975095, pre 0.0113382, rec 0.430851, f1 0.0220949\n",
      "2016-11-20T20:38:16.338229: step 316, loss 0.976226, pre 0.00743651, rec 0.430894, f1 0.0146207\n",
      "2016-11-20T20:38:18.625714: step 317, loss 0.97798, pre 0.00725345, rec 0.468468, f1 0.0142857\n",
      "2016-11-20T20:38:20.887579: step 318, loss 0.971135, pre 0.00587308, rec 0.398058, f1 0.0115754\n",
      "2016-11-20T20:38:23.175831: step 319, loss 0.973894, pre 0.0100028, rec 0.412791, f1 0.0195323\n",
      "2016-11-20T20:38:25.414389: step 320, loss 0.977181, pre 0.00588813, rec 0.477273, f1 0.0116327\n",
      "2016-11-20T20:38:27.752802: step 321, loss 0.972934, pre 0.00428082, rec 0.365854, f1 0.00846262\n",
      "2016-11-20T20:38:30.091675: step 322, loss 0.971819, pre 0.0102157, rec 0.428571, f1 0.0199557\n",
      "2016-11-20T20:38:32.406643: step 323, loss 0.971399, pre 0.0107893, rec 0.47205, f1 0.0210965\n",
      "2016-11-20T20:38:34.694239: step 324, loss 0.977072, pre 0.00491642, rec 0.4375, f1 0.00972357\n",
      "2016-11-20T20:38:36.968109: step 325, loss 0.972995, pre 0.00738112, rec 0.429752, f1 0.014513\n",
      "2016-11-20T20:38:39.248249: step 326, loss 0.975238, pre 0.00508834, rec 0.439024, f1 0.0100601\n",
      "2016-11-20T20:38:41.540335: step 327, loss 0.970196, pre 0.00759639, rec 0.398496, f1 0.0149086\n",
      "2016-11-20T20:38:43.782109: step 328, loss 0.972916, pre 0.00920159, rec 0.416667, f1 0.0180055\n",
      "2016-11-20T20:38:46.099427: step 329, loss 0.97149, pre 0.0098081, rec 0.428571, f1 0.0191773\n",
      "2016-11-20T20:38:48.390060: step 330, loss 0.971795, pre 0.00783476, rec 0.466102, f1 0.0154105\n",
      "2016-11-20T20:38:50.658195: step 331, loss 0.972774, pre 0.0104594, rec 0.513889, f1 0.0205015\n",
      "2016-11-20T20:38:52.931355: step 332, loss 0.972264, pre 0.00697807, rec 0.35, f1 0.0136833\n",
      "2016-11-20T20:38:55.224980: step 333, loss 0.971684, pre 0.00755632, rec 0.449153, f1 0.0148626\n",
      "2016-11-20T20:38:57.762566: step 334, loss 0.975467, pre 0.00856621, rec 0.5, f1 0.0168438\n",
      "2016-11-20T20:39:00.017232: step 335, loss 0.974513, pre 0.00762066, rec 0.453782, f1 0.0149896\n",
      "2016-11-20T20:39:02.279134: step 336, loss 0.97245, pre 0.00683274, rec 0.432432, f1 0.0134529\n",
      "2016-11-20T20:39:04.526089: step 337, loss 0.976309, pre 0.00674063, rec 0.417391, f1 0.013267\n",
      "2016-11-20T20:39:06.880152: step 338, loss 0.973283, pre 0.00779368, rec 0.443548, f1 0.0153182\n",
      "2016-11-20T20:39:09.641568: step 339, loss 0.977548, pre 0.0044906, rec 0.533333, f1 0.00890621\n",
      "2016-11-20T20:39:11.911656: step 340, loss 0.97374, pre 0.00792079, rec 0.373333, f1 0.0155125\n",
      "2016-11-20T20:39:14.221995: step 341, loss 0.974143, pre 0.012202, rec 0.410377, f1 0.0236993\n",
      "2016-11-20T20:39:16.686908: step 342, loss 0.972057, pre 0.00656018, rec 0.489362, f1 0.0129468\n",
      "2016-11-20T20:39:19.226272: step 343, loss 0.972042, pre 0.0107649, rec 0.417582, f1 0.0209887\n",
      "2016-11-20T20:39:22.608170: step 344, loss 0.97428, pre 0.00552721, rec 0.393939, f1 0.0109015\n",
      "2016-11-20T20:39:25.159054: step 345, loss 0.972064, pre 0.0113202, rec 0.473373, f1 0.0221117\n",
      "2016-11-20T20:39:29.009209: step 346, loss 0.97629, pre 0.00548832, rec 0.322314, f1 0.0107929\n",
      "2016-11-20T20:39:31.914122: step 347, loss 0.973989, pre 0.010138, rec 0.48, f1 0.0198566\n",
      "2016-11-20T20:39:34.355526: step 348, loss 0.977766, pre 0.00642997, rec 0.380165, f1 0.012646\n",
      "2016-11-20T20:39:36.732031: step 349, loss 0.972186, pre 0.00880557, rec 0.449275, f1 0.0172726\n",
      "2016-11-20T20:39:39.047021: step 350, loss 0.97168, pre 0.0107786, rec 0.426966, f1 0.0210264\n",
      "2016-11-20T20:39:41.402031: step 351, loss 0.975198, pre 0.00508906, rec 0.352941, f1 0.0100334\n",
      "2016-11-20T20:39:43.824931: step 352, loss 0.977949, pre 0.00697837, rec 0.4, f1 0.0137174\n",
      "2016-11-20T20:39:46.323916: step 353, loss 0.976382, pre 0.013061, rec 0.441315, f1 0.0253711\n",
      "2016-11-20T20:39:48.681573: step 354, loss 0.974642, pre 0.00983837, rec 0.397727, f1 0.0192018\n",
      "2016-11-20T20:39:51.063095: step 355, loss 0.973311, pre 0.00877069, rec 0.405229, f1 0.0171698\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0c3913cb79c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m#             if current_step % FLAGS.evaluate_every == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0c3913cb79c1>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m             }\n\u001b[1;32m     73\u001b[0m             _, step, summaries, loss, accuracy, scores, predictions, temp, extracted, truth, correct, gold, precision, recall, f1 = sess.run(\n\u001b[0;32m---> 74\u001b[0;31m                 [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions, cnn.temp, cnn.extracted, cnn.truth, cnn.correct, cnn.gold, cnn.precision, cnn.recall, cnn.f1],feed_dict)\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, pre {:g}, rec {:g}, f1 {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=X.shape[1],\n",
    "            num_classes=Y.shape[1],\n",
    "            vocab_size=max([max(sub_X) for sub_X in X]) + 1, # TODO: get vocab size another way\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss) # TODO check cnn.loss\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "#         Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy, scores, predictions, temp, extracted, truth, correct, gold, precision, recall, f1 = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions, cnn.temp, cnn.extracted, cnn.truth, cnn.correct, cnn.gold, cnn.precision, cnn.recall, cnn.f1],feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, pre {:g}, rec {:g}, f1 {:g}\".format(time_str, step, loss, precision, recall, f1))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "#             print \"type of scores: \", type(scores)\n",
    "#             print type(scores[0])\n",
    "#             print \"scores: \\n\", scores[0]\n",
    "#             print \" \"\n",
    "#             print \"type of predictions: \", type(predictions)\n",
    "#             print \"predictions: \\n\", predictions\n",
    "#             print type(temp)\n",
    "#             print \"temp: \\n\", temp[0]\n",
    "#             print \"gold: \\n\", gold[0]\n",
    "#             print \"extracted: \", extracted\n",
    "#             print \"extracted python: \", sum([sum(score) for score in scores])\n",
    "#             print \"truth: \", truth\n",
    "#             print \"truth python: \", sum([sum(x) for x in gold])\n",
    "#             print \"correct: \", correct\n",
    "#             print \"correct python: \", sum([sum(x) for x in temp])\n",
    "#             print \"precision: \", precision\n",
    "#             print \"recall: \", recall\n",
    "#             print \"f1: \", f1\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy, scores, predictions = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(X, Y)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "#             if current_step % FLAGS.evaluate_every == 0:\n",
    "#                 print(\"\\nEvaluation:\")\n",
    "#                 dev_step(x_dev, y_dev, writer=dev_summary_writer) # from x_dev \n",
    "#                 print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
