{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feautres:\n",
    "   1. Features based on the token itself:\n",
    "   \n",
    "       1. actual token (one-hot-encoding or word2vec (http://bio.nlplab.org/#word-vectors)\n",
    "       2. POS tag (geniatagger or ntlk)\n",
    "       3. inside parentheses or not\n",
    "       4. Named Entity by geniatagger\n",
    "       5. Prefixes or suffixes?\n",
    "      \n",
    "   2. Features based on the phrase containing the token: (geniatagger -'chunker')\n",
    "   \n",
    "       1. the type of phrase\n",
    "       2. whether it is the first or last token in the phrase\n",
    "       3. UMLS semantic type ? (https://semanticnetwork.nlm.nih.gov/)\n",
    "       \n",
    "   3. Features based on the four nearest tokens on each side of the token in question:\n",
    "   \n",
    "      1. tokens themselves\n",
    "      2. their POS\n",
    "      3. whether each token is in the same phrase as the token in question\n",
    "      4. semantic tags ??\n",
    "   \n",
    "Additional features after tokenization:\n",
    "\n",
    "1. Semantic tags tagged manually for words include people or measurements\n",
    "    1. People: *people, participants, subjects, men, women, children, patient*  \n",
    "    2. Meaurement: *length, volumen, weight, etc.\n",
    "    \n",
    "2. Semantic tags from Word-Net (https://wordnet.princeton.edu/)\n",
    "\n",
    "    \n",
    "\n",
    "source: Automatic summarization of results from clinical trials; An Introduction to Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jing/anaconda/envs/tensorflow/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from geniatagger import GeniaTagger\n",
    "from preprocess_data import get_all_data\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "punctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory for geniatagger\n",
    "genia_directory = 'geniatagger-3.0.2/geniatagger'\n",
    "\n",
    "tagger = GeniaTagger(genia_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all data\n",
    "# Format of word_array: [[word1, word2, ...], ...]\n",
    "word_array, tag_array = get_all_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec_model = 'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "word2vec_model = 'PubMed-w2v.bin'\n",
    "w2v = Word2Vec.load_word2vec_format(word2vec_model, binary=True)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = word_array[1]\n",
    "test = ' '.join(word)\n",
    "genia_tags = tagger.parse(test)\n",
    "abstract,tagged_abs,label= clean_tags(word, genia_tags, tag_array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print elements of a list with spaces\n",
    "# Element l[i] is padded to have length space[i]\n",
    "def print_with_spaces(l, spaces):\n",
    "    # This pads strings to be of space length and aligned left\n",
    "    formatter = lambda space: '{:' + str(space) + '}'\n",
    "    \n",
    "    print ''.join([formatter(space).format(string) for string, space in zip(l, spaces)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "'''\n",
    "Clean up genia tags\n",
    "\n",
    "INPUT: \n",
    "- abstract (word array from prepocessing)\n",
    "- genia_tags (output of genia parser)\n",
    "- label (tag array from preprocessing)\n",
    "\n",
    "OUTPUT:\n",
    "- abstract_mod (word array that match with the output of genia parser)\n",
    "- clean tags\n",
    "    - a list of tuples\n",
    "    - Features: pos,chunk_clean,iob,named_entity, whether word inside parentheses.\n",
    "- label_mod (label array that match with the output of genia parser)\n",
    "'''\n",
    "\n",
    "\n",
    "def clean_tags(abstract, genia_tags, label):\n",
    "    cleaned_tags = []\n",
    "    abstract_mod = []\n",
    "    label_mod =[]\n",
    "    miss_match=False\n",
    "    miss_word = []\n",
    "    # Keep track of whether word is inside parantheses\n",
    "    inside_paren = False\n",
    "    \n",
    "    idx=0\n",
    "\n",
    "    for word, base_form, pos, chunk, named_entity in genia_tags:\n",
    "        # ';' has POS ':'\n",
    "        if word == pos or pos == ':' or pos == '(' or pos == ')' or word == '%':\n",
    "            # This means the word is puctuation, parentheses, etc.,\n",
    "            # so we do not make features for it.\n",
    "            if pos == '(':\n",
    "                inside_paren = True\n",
    "            elif pos == ')':\n",
    "                inside_paren = False\n",
    "#             elif len(word) > 1 and word not in ['``',  '\\'\\'', '--', 'TO', '...','3-5']:\n",
    "#                 # This shouldn't happen\n",
    "#                 raise ValueError('Unidentified word: ' + word)\n",
    "            \n",
    "        # Strip out IOB from chunk\n",
    "        if chunk == 'O':\n",
    "            chunk_clean=chunk\n",
    "        elif len(chunk) > 2:\n",
    "            chunk_clean=chunk[2:]\n",
    "        else:\n",
    "            raise ValueError('Unidentified chunk: ' + chunk)\n",
    "        \n",
    "        # Get IOB\n",
    "        iob = chunk[0]\n",
    "        if iob != 'O' and iob != 'I' and iob != 'B':\n",
    "            raise ValueError('Unidentified chunk:s ' + chunk)\n",
    "        \n",
    "        # Strip out IOB from named_entity\n",
    "        if named_entity == 'O':\n",
    "            ne_clean=named_entity\n",
    "        elif len(named_entity) > 2:\n",
    "            ne_clean=named_entity[2:]\n",
    "        else:\n",
    "            raise ValueError('Unidentified named entity: ' + named_entity)\n",
    "\n",
    "        \n",
    "        cleaned_tags.append((pos,chunk_clean,iob,named_entity,inside_paren))\n",
    "\n",
    "        #recreate the abstract based on the parser\n",
    "        abstract_mod.append(word)\n",
    "        \n",
    "        #recreate label for each word\n",
    "        org_word = abstract[idx]\n",
    "        # update index of the word in old word list if match\n",
    "        \n",
    "        if word == org_word:\n",
    "            label_mod.append(label[idx])\n",
    "            idx +=1\n",
    "            miss_match=False\n",
    "            miss_word = []\n",
    "        #if the previous word is not matched\n",
    "        elif miss_match:\n",
    "            miss_word.append(word)\n",
    "            if ''.join(miss_word) == org_word:\n",
    "                label_mod.append(label[idx])\n",
    "                idx+=1\n",
    "                miss_match=False\n",
    "                miss_word = []\n",
    "            elif word in org_word:\n",
    "                label_mod.append(label[idx])\n",
    "            elif word in ['\\'\\''] and org_word[-1]=='\"': \n",
    "                label_mod.append(label[idx])\n",
    "                idx+=1\n",
    "                miss_match=False\n",
    "                miss_word = []\n",
    "            elif word in ['\\'\\'','``'] and '\"' in org_word: \n",
    "                label_mod.append(label[idx])\n",
    "                miss_word[-1]='\"'\n",
    "            else:\n",
    "                print word \n",
    "                print org_word\n",
    "                raise ValueError( \"Error at word:\"+ str(idx))\n",
    "        #check if the word match partially\n",
    "        elif word == org_word[:len(word)]: \n",
    "            label_mod.append(label[idx])\n",
    "            miss_match=True\n",
    "            miss_word.append(word)\n",
    "        elif word in ['``',  '\\'\\''] and org_word[0]=='\"':\n",
    "            label_mod.append(label[idx])\n",
    "            miss_match=True\n",
    "            miss_word.append('\"')\n",
    "        else:\n",
    "            print word \n",
    "            print org_word\n",
    "            raise ValueError( \"Error at word:\"+ str(idx))\n",
    "\n",
    "    return abstract_mod, cleaned_tags, label_mod\n",
    "\n",
    "if DEBUG:\n",
    "    abstract_id=1001\n",
    "    test = ' '. join (word_array[abstract_id])\n",
    "    genia_tags=tagger.parse(test)\n",
    "    abstract_mod,cleaned_tags,label_mod = clean_tags(word_array[abstract_id], genia_tags, tag_array[abstract_id])\n",
    "    print zip(word_array[abstract_id][103:113],tag_array[abstract_id][103:113] )    \n",
    "    print zip(abstract_mod[110:130],label_mod[110:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word[+3]': 'SPan-1', 'ne[+3]': 'B-protein', 'chunkiob[+3]': 'I', 'samechunk[+3]': False, 'pos[+3]': 'NN', 'chunk[+3]': 'NP', 'inside_paren[+3]': 'False', 'isupper[+3]': False, 'istitle[+3]': False}\n",
      "[('of', ('IN', 'PP', 'B', 'O', False)), ('CA19-9', ('NN', 'NP', 'B', 'B-protein', False)), ('and', ('CC', 'NP', 'I', 'O', False)), ('SPan-1', ('NN', 'NP', 'I', 'B-protein', False))]\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "\n",
    "'''\n",
    "Get features for a word at index word_i,\n",
    "where d indicates the distance between word_i\n",
    " and the word that we want to create features for,\n",
    " i.e. the owner of feature_dict\n",
    "\n",
    "INPUT:\n",
    "- word_i: index of the word that we want to extract its own features\n",
    "- d: distance from the word that we want to create feature dictionary for \n",
    "- abstract: the word array for the abstract the word is in\n",
    "- tagged_abs: the list of tuples from clean_tags\n",
    "- feature_dict: the dictionary that we want to store all the features for the word\n",
    "- direction: whether the word_i is before or after the word\n",
    "- w2v: whether we want to use word2vec, or the word2vec we use\n",
    "- w2v_size: the size of the w2v\n",
    "\n",
    "OUTPUT:\n",
    "- feature_dict\n",
    "\n",
    "source:\n",
    "https://github.com/bwallace/Deep-PICO/blob/master/crf.py\n",
    "'''\n",
    "\n",
    "def tags2features(word_i, d, abstract, tagged_abs, feature_dict, direction, w2v, w2v_size=100):\n",
    "     \n",
    "    if direction == 'before':\n",
    "        position = '-'\n",
    "    elif direction == 'after':\n",
    "        position = '+'\n",
    "    else:\n",
    "        position = ''\n",
    "    \n",
    "    \"\"\" or we can use base form of the word\"\"\"\n",
    "    \n",
    "    word = abstract[word_i]\n",
    "    \n",
    "    # get all the tags\n",
    "    \"\"\"  NEED TO UPDATE WITH NEW DATA \"\"\"\n",
    "    pos, chunk, iob , named_entity, inside_paren = tagged_abs[word_i]\n",
    "      \n",
    "    if w2v:\n",
    "        try:\n",
    "            w2v_word = w2v[word]\n",
    "            found_word = True\n",
    "        except:\n",
    "            w2v_word = None\n",
    "            found_word = False\n",
    "        \n",
    "        for n in range(w2v_size):\n",
    "            if found_word:\n",
    "                feature_dict[\"w2v[{}{}][{}]\".format(position, d, n)] = w2v_word[n]\n",
    "            else:\n",
    "                feature_dict[\"w2v[{}{}][{}]\".format(position, d, n)] = 0\n",
    "# Cosine similarity between the word and the previous word\n",
    "#             if word_i > 0 and found_word:\n",
    "#                 try:\n",
    "#                     cosine_simil = w2v.similarity(abstract[word_i-1], abstract[word_i])\n",
    "#                 except:\n",
    "#                     cosine_simil = 0\n",
    "#                 feature_dict['cos'] = cosine_simil    \n",
    "    else:\n",
    "        feature_dict['word[{}{}]'.format(position, d)] = word\n",
    "   \n",
    "    #add features to the feature dict\n",
    "    #pos tag\n",
    "    feature_dict['pos[{}{}]'.format(position, d)] = pos\n",
    "    # type of phrase\n",
    "    feature_dict['chunk[{}{}]'.format(position, d)] = chunk\n",
    "    # location of the word in a phrase\n",
    "    feature_dict['chunkiob[{}{}]'.format(position, d)] = iob    \n",
    "    #Named Entity\n",
    "    feature_dict['ne[{}{}]'.format(position, d)] = named_entity\n",
    "    #Inside parentheses\n",
    "    feature_dict['inside_paren[{}{}]'.format(position, d)] = str(inside_paren)\n",
    "    \n",
    "    #Whether the word is all capitalized\n",
    "    feature_dict['isupper[{}{}]'.format(position, d)] = word.isupper()\n",
    "    feature_dict['istitle[{}{}]'.format(position, d)] = word.istitle()\n",
    "    \n",
    "    #features for word itself\n",
    "    if d==0:\n",
    "        if (iob == 'I' or iob == 'B') and \\\n",
    "        (word_i == len(abstract)-1 or tagged_abs[word_i+1][2] != 'I'):\n",
    "            feature_dict['chunkend[{}{}]'.format(position, d)] = True\n",
    "        else:\n",
    "            feature_dict['chunkend[{}{}]'.format(position, d)] = False\n",
    "    #features for neighbor words:\n",
    "    else:\n",
    "        if position=='-':\n",
    "            sign=1\n",
    "        elif position =='+':\n",
    "            sign =-1\n",
    "        \n",
    "        #identify the main word\n",
    "        word_main = word_i+ (sign*d)\n",
    "        \n",
    "        start = min(word_i, word_main)\n",
    "        end = max(word_i, word_main)\n",
    "        \n",
    "        feature_dict['samechunk[{}{}]'.format(position, d)]=True\n",
    "        for b in range(start+1, end+1):\n",
    "            if tagged_abs[b][2]=='B' or tagged_abs[b][2]=='O':\n",
    "                feature_dict['samechunk[{}{}]'.format(position, d)]=False\n",
    "       \n",
    "    return feature_dict\n",
    "\n",
    "if DEBUG:\n",
    "    feature_dict ={}\n",
    "    feature_dict = tags2features(4, 3, abstract, tagged_abs, feature_dict, 'after', False, w2v_size=100)\n",
    "    print feature_dict\n",
    "    print zip(abstract[1:5],tagged_abs[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that extracts features of neigbor words\n",
    "with a certain window size\n",
    "add more features to the existing feature dictionary\n",
    "\n",
    "INPUT:\n",
    "- n_before, n_after: the size of the window that we are interested in\n",
    "- word_i: index of the word that we want to create feature dictionary for \n",
    "- abstract: the word array for the abstract the word is in\n",
    "- tagged_abs: the list of tuples from clean_tags\n",
    "- feature_dict: the dictionary that we want to store all the features for the word\n",
    "- w2v: whether we want to use word2vec, or the word2vec we use\n",
    "- w2v_size: the size of the w2v\n",
    "\n",
    "OUTPUT:\n",
    "- feature_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def tags2features_window(n_before,n_after,word_i,abstract, tagged_abs, feature_dict, w2v, w2v_size=100):\n",
    "    for d in range(1,n_before+1):\n",
    "        #for the word that do not have enough words before, \n",
    "        #they will just do not have as many features\n",
    "        if word_i-d >=0:\n",
    "            feature_dict = tags2features(word_i-d, d, abstract, tagged_abs, feature_dict, 'before', w2v, w2v_size=w2v_size)\n",
    "    for d in range(1,n_after+1):\n",
    "        #for the word that do not have enough words before, \n",
    "        #they will just do not have as many features\n",
    "        if word_i+d <(len(abstract)-1):\n",
    "            feature_dict = tags2features(word_i+d, d, abstract, tagged_abs, feature_dict, 'after', w2v, w2v_size=w2v_size)\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "DISPLAY = True\n",
    "'''\n",
    "Get features for all abstracts and return\n",
    "an array X that contains the features for all abtracts\n",
    "and array Y for the corresponding labels\n",
    "\n",
    "INPUT:\n",
    "- abstract_list: word_array from prepocessing\n",
    "- tag_list: labels from prepocessing\n",
    "- window: (n_before,n_after) the number of neighbors that we want to consider\n",
    "- w2v: whether we want to use word2vec\n",
    "- wiki: whether we want to use wiki word2vec.\n",
    "\n",
    "OUTPUT:\n",
    "- X: [[list of features dictionary for each word in abstract 1], \n",
    "[list of features dictionary for each word in abstract 2 ], ...]\n",
    "- Y: [[labels for abstract 1], [lables for abstract 2],...]\n",
    "\n",
    "'''\n",
    "def abstracts2features(abstract_list,tag_list,n_before,n_after,w2v, w2v_size=100):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(abstract_list)):\n",
    "        abstract_txt = ' '. join (abstract_list[i])\n",
    "        label = tag_list[i]\n",
    "        \n",
    "        if DEBUG:\n",
    "            if not abstract_txt.startswith('Association of efavirenz'):\n",
    "                continue\n",
    "            print abstract\n",
    "        \n",
    "        if DISPLAY:\n",
    "            # Print progress\n",
    "            print '\\r{0}: {1}'.format(i, abstract_txt[:30]),\n",
    "    \n",
    "        # Get tags from genia tagger\n",
    "        # Format: [(Association, Association, NN, B-NP, O), ...]\n",
    "        # Visualization here: http://nactem7.mib.man.ac.uk/geniatagger/\n",
    "        genia_tags = tagger.parse(abstract_txt)\n",
    "\n",
    "        '''Step 1: Clean up genia tags'''\n",
    "\n",
    "        abstract,tagged_abs,labels = clean_tags(abstract_list[i], genia_tags, label)\n",
    "\n",
    "        '''Step 2: Get features the abstract'''\n",
    "        \n",
    "        if w2v:\n",
    "            print('Loading word2vec model...')\n",
    "\n",
    "            if wiki:\n",
    "                print 'Using wiki word2vec...'\n",
    "                word2vec_model = 'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "            else:\n",
    "                print 'Using non-wiki word2vec...'\n",
    "                word2vec_model = 'PubMed-w2v.bin'\n",
    "            w2v = Word2Vec.load_word2vec_format(word2vec_model, binary=True)\n",
    "            print('Loaded word2vec model')\n",
    "        else:\n",
    "            w2v=False\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for i, word in enumerate(abstract):\n",
    "            feature_dict = {}\n",
    "            \n",
    "            #get features for the current word\n",
    "            feature_dict = tags2features(i, 0,abstract ,tagged_abs, feature_dict, '', w2v,w2v_size=w2v_size)\n",
    "            #get features for the neighbor words\n",
    "            feature_dict = tags2features_window(n_before,n_after,i,abstract, tagged_abs, feature_dict, w2v, w2v_size=w2v_size)\n",
    "            \n",
    "            features.append(feature_dict)    \n",
    "        X.append(features)\n",
    "        Y.append(labels)\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "if DEBUG:\n",
    "    X,Y = abstracts2features(word_array[1:10],tag_array[1:10],(1,1),False, w2v_size=100)\n",
    "    print X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282: Effects of individualized brea\n"
     ]
    }
   ],
   "source": [
    "X,Y = abstracts2features(word_array[4718:],tag_array[4718:],1,1,False, w2v_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
