{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess_data import get_all_data_train\n",
    "from TF_preprocess_data import get_1_hot_sentence_encodings\n",
    "from text_cnn_by_word import TextCNN\n",
    "import datetime\n",
    "# import data_helpers\n",
    "import time\n",
    "import os\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# dennybritz's Sentance classification using cnn\n",
    "# https://github.com/dennybritz/cnn-text-classification-tf\n",
    "# njl's Sentiment Analysis example \n",
    "# https://github.com/nicholaslocascio/tensorflow-nlp-tutorial/blob/master/sentiment-analysis/Sentiment-RNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into train and dev \n",
    "word_array, tag_array = get_all_data_train(sentences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pad single words with n words on either side \n",
    "n = 3\n",
    "x_n = []\n",
    "for abstract in word_array:\n",
    "    # pad abstract with * \n",
    "    padding = [\"*\"] * n\n",
    "    padded_abstract = padding\n",
    "    padded_abstract.extend(abstract)\n",
    "    padded_abstract.extend(padding)\n",
    "    # for all words (excluding padding)\n",
    "    for i in range(n, len(abstract)+n):\n",
    "        x_n.append(padded_abstract[i-n:i+n+1])\n",
    "\n",
    "y_binary = [y for x in tag_array for y in x] # flatten tag array\n",
    "y = np.array([[1,0] if tag == 'P' else [0,1] for tag in y_binary ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[')', ',', 'but', 'not', 'in', 'other', 'outcome'], [',', 'but', 'not', 'in', 'other', 'outcome', 'measures'], ['but', 'not', 'in', 'other', 'outcome', 'measures', '.'], ['not', 'in', 'other', 'outcome', 'measures', '.', 'CONCLUSION'], ['in', 'other', 'outcome', 'measures', '.', 'CONCLUSION', 'With'], ['other', 'outcome', 'measures', '.', 'CONCLUSION', 'With', 'shorter'], ['outcome', 'measures', '.', 'CONCLUSION', 'With', 'shorter', 'operative'], ['measures', '.', 'CONCLUSION', 'With', 'shorter', 'operative', 'time'], ['.', 'CONCLUSION', 'With', 'shorter', 'operative', 'time', ','], ['CONCLUSION', 'With', 'shorter', 'operative', 'time', ',', 'less'], ['With', 'shorter', 'operative', 'time', ',', 'less', 'blood'], ['shorter', 'operative', 'time', ',', 'less', 'blood', 'loss'], ['operative', 'time', ',', 'less', 'blood', 'loss', 'and'], ['time', ',', 'less', 'blood', 'loss', 'and', 'similar'], [',', 'less', 'blood', 'loss', 'and', 'similar', 'morbidity'], ['less', 'blood', 'loss', 'and', 'similar', 'morbidity', 'profile'], ['blood', 'loss', 'and', 'similar', 'morbidity', 'profile', 'exteriorization'], ['loss', 'and', 'similar', 'morbidity', 'profile', 'exteriorization', 'of'], ['and', 'similar', 'morbidity', 'profile', 'exteriorization', 'of', 'uterus'], ['similar', 'morbidity', 'profile', 'exteriorization', 'of', 'uterus', 'during'], ['morbidity', 'profile', 'exteriorization', 'of', 'uterus', 'during', 'caesarean'], ['profile', 'exteriorization', 'of', 'uterus', 'during', 'caesarean', 'section'], ['exteriorization', 'of', 'uterus', 'during', 'caesarean', 'section', 'seems'], ['of', 'uterus', 'during', 'caesarean', 'section', 'seems', 'to'], ['uterus', 'during', 'caesarean', 'section', 'seems', 'to', 'be'], ['during', 'caesarean', 'section', 'seems', 'to', 'be', 'preferred'], ['caesarean', 'section', 'seems', 'to', 'be', 'preferred', 'except'], ['section', 'seems', 'to', 'be', 'preferred', 'except', 'where'], ['seems', 'to', 'be', 'preferred', 'except', 'where', 'it'], ['to', 'be', 'preferred', 'except', 'where', 'it', 'is'], ['be', 'preferred', 'except', 'where', 'it', 'is', 'not'], ['preferred', 'except', 'where', 'it', 'is', 'not', 'possible'], ['except', 'where', 'it', 'is', 'not', 'possible', 'because'], ['where', 'it', 'is', 'not', 'possible', 'because', 'of'], ['it', 'is', 'not', 'possible', 'because', 'of', 'adhesions'], ['is', 'not', 'possible', 'because', 'of', 'adhesions', 'and'], ['not', 'possible', 'because', 'of', 'adhesions', 'and', 'surgeons'], ['possible', 'because', 'of', 'adhesions', 'and', 'surgeons', 'inexperience'], ['because', 'of', 'adhesions', 'and', 'surgeons', 'inexperience', '.'], ['of', 'adhesions', 'and', 'surgeons', 'inexperience', '.', '*'], ['adhesions', 'and', 'surgeons', 'inexperience', '.', '*', '*'], ['and', 'surgeons', 'inexperience', '.', '*', '*', '*'], ['*', '*', '*', 'Adjuvant', 'subcutaneous', 'interleukin-2', 'in'], ['*', '*', 'Adjuvant', 'subcutaneous', 'interleukin-2', 'in', 'patients'], ['*', 'Adjuvant', 'subcutaneous', 'interleukin-2', 'in', 'patients', 'with'], ['Adjuvant', 'subcutaneous', 'interleukin-2', 'in', 'patients', 'with', 'resected'], ['subcutaneous', 'interleukin-2', 'in', 'patients', 'with', 'resected', 'renal'], ['interleukin-2', 'in', 'patients', 'with', 'resected', 'renal', 'cell'], ['in', 'patients', 'with', 'resected', 'renal', 'cell', 'carcinoma'], ['patients', 'with', 'resected', 'renal', 'cell', 'carcinoma', ':']]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print x_n[150:200]\n",
    "print y[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "document_length = 2*n+1\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(document_length)\n",
    "n_array = [' '.join(word) for word in x_n]\n",
    "x = np.array(list(vocab_processor.fit_transform(n_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(927022, 7)\n",
      "(927022, 2)\n",
      "[1 2 3 4 0 0 0]\n",
      "['*', '*', '*', 'Uterine', 'incision', 'closure', 'at']\n",
      "[0 1]\n",
      "[1 2 3 4 5 0 0]\n",
      "['*', '*', 'Uterine', 'incision', 'closure', 'at', 'caesarean']\n",
      "[1 2 3 4 5 6 0]\n",
      "['*', 'Uterine', 'incision', 'closure', 'at', 'caesarean', 'section']\n",
      "[1 2 3 4 5 6 0]\n",
      "['Uterine', 'incision', 'closure', 'at', 'caesarean', 'section', ':']\n",
      "[2 3 4 5 6 7 0]\n",
      "['incision', 'closure', 'at', 'caesarean', 'section', ':', 'a']\n",
      "[3 4 5 6 7 8 0]\n",
      "['closure', 'at', 'caesarean', 'section', ':', 'a', 'randomised']\n",
      "[4 5 6 7 8 9 0]\n",
      "['at', 'caesarean', 'section', ':', 'a', 'randomised', 'comparative']\n",
      "[ 5  6  7  8  9 10  0]\n",
      "['caesarean', 'section', ':', 'a', 'randomised', 'comparative', 'study']\n",
      "[ 8  9 10 11 12  3 13]\n",
      "['randomised', 'comparative', 'study', 'of', 'intraperitoneal', 'closure', 'and']\n"
     ]
    }
   ],
   "source": [
    "print type(x)\n",
    "print type(y)\n",
    "print x.shape\n",
    "print y.shape\n",
    "print x[0]\n",
    "print x_n[0]\n",
    "print y[0]\n",
    "\n",
    "print x[1]\n",
    "print x_n[1]\n",
    "print x[2]\n",
    "print x_n[2]\n",
    "print x[3]\n",
    "print x_n[3]\n",
    "print x[4]\n",
    "print x_n[4]\n",
    "print x[5]\n",
    "print x_n[5]\n",
    "print x[6]\n",
    "print x_n[6]\n",
    "print x[7]\n",
    "print x_n[7]\n",
    "print x[11]\n",
    "print x_n[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_document_length = len(X[0])\n",
    "# vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "\n",
    "# # # Randomly shuffle data\n",
    "# np.random.seed(10)\n",
    "# shuffle_indices = np.random.permutation(np.arange(len(Y)))\n",
    "# x_shuffled = X[shuffle_indices]\n",
    "# y_shuffled = Y[shuffle_indices]\n",
    "\n",
    "# # Split train/test set\n",
    "# # TODO: This is very crude, should use cross-validation\n",
    "# dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(Y)))\n",
    "# x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "# y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "# print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "# print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copied unchanged function\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1480625936\n",
      "\n",
      "2016-12-01T15:58:58.557768: step 1, loss 4.204, acc 0.140625\n",
      "2016-12-01T15:58:58.696152: step 2, loss 3.362, acc 0.34375\n",
      "2016-12-01T15:58:58.830220: step 3, loss 2.53243, acc 0.234375\n",
      "2016-12-01T15:58:58.964551: step 4, loss 2.01759, acc 0.390625\n",
      "2016-12-01T15:58:59.102063: step 5, loss 0.879602, acc 0.6875\n",
      "2016-12-01T15:58:59.234827: step 6, loss 0.916369, acc 0.734375\n",
      "2016-12-01T15:58:59.368092: step 7, loss 0.645204, acc 0.859375\n",
      "2016-12-01T15:58:59.516923: step 8, loss 0.571862, acc 0.859375\n",
      "2016-12-01T15:58:59.663642: step 9, loss 0.586443, acc 0.90625\n",
      "2016-12-01T15:58:59.803216: step 10, loss 1.09289, acc 0.796875\n",
      "2016-12-01T15:58:59.940401: step 11, loss 0.35822, acc 0.90625\n",
      "2016-12-01T15:59:00.074611: step 12, loss 0.512276, acc 0.9375\n",
      "2016-12-01T15:59:00.217246: step 13, loss 0.833971, acc 0.90625\n",
      "2016-12-01T15:59:00.350431: step 14, loss 0.461513, acc 0.953125\n",
      "2016-12-01T15:59:00.494765: step 15, loss 0.259372, acc 0.96875\n",
      "2016-12-01T15:59:00.641817: step 16, loss 0.574739, acc 0.921875\n",
      "2016-12-01T15:59:00.845650: step 17, loss 0.17115, acc 0.96875\n",
      "2016-12-01T15:59:00.995466: step 18, loss 0.813349, acc 0.90625\n",
      "2016-12-01T15:59:01.145596: step 19, loss 2.23392, acc 0.78125\n",
      "2016-12-01T15:59:01.286113: step 20, loss 0.753844, acc 0.90625\n",
      "2016-12-01T15:59:01.437462: step 21, loss 1.0401, acc 0.875\n",
      "2016-12-01T15:59:01.586320: step 22, loss 0.559755, acc 0.9375\n",
      "2016-12-01T15:59:01.729333: step 23, loss 1.17506, acc 0.890625\n",
      "2016-12-01T15:59:01.889818: step 24, loss 0.668165, acc 0.921875\n",
      "2016-12-01T15:59:02.040053: step 25, loss 0.123764, acc 0.96875\n",
      "2016-12-01T15:59:02.183063: step 26, loss 0.826918, acc 0.921875\n",
      "2016-12-01T15:59:02.325405: step 27, loss 1.22609, acc 0.875\n",
      "2016-12-01T15:59:02.464127: step 28, loss 0.247955, acc 0.9375\n",
      "2016-12-01T15:59:02.619108: step 29, loss 0.787953, acc 0.90625\n",
      "2016-12-01T15:59:02.764230: step 30, loss 0.871248, acc 0.921875\n",
      "2016-12-01T15:59:02.935631: step 31, loss 0.536953, acc 0.90625\n",
      "2016-12-01T15:59:03.126505: step 32, loss 0.435614, acc 0.921875\n",
      "2016-12-01T15:59:03.309614: step 33, loss 0.580623, acc 0.90625\n",
      "2016-12-01T15:59:03.495587: step 34, loss 0.816357, acc 0.875\n",
      "2016-12-01T15:59:03.647164: step 35, loss 0.506637, acc 0.9375\n",
      "2016-12-01T15:59:03.786026: step 36, loss 0.744868, acc 0.875\n",
      "2016-12-01T15:59:03.963752: step 37, loss 0.639101, acc 0.875\n",
      "2016-12-01T15:59:04.125792: step 38, loss 0.0728201, acc 0.96875\n",
      "2016-12-01T15:59:04.271039: step 39, loss 0.318668, acc 0.921875\n",
      "2016-12-01T15:59:04.428783: step 40, loss 0.597996, acc 0.921875\n",
      "2016-12-01T15:59:04.579518: step 41, loss 0.345886, acc 0.921875\n",
      "2016-12-01T15:59:04.713277: step 42, loss 0.716318, acc 0.84375\n",
      "2016-12-01T15:59:04.847587: step 43, loss 0.47706, acc 0.859375\n",
      "2016-12-01T15:59:04.996637: step 44, loss 0.557142, acc 0.859375\n",
      "2016-12-01T15:59:05.195305: step 45, loss 0.452261, acc 0.921875\n",
      "2016-12-01T15:59:05.375941: step 46, loss 0.222702, acc 0.9375\n",
      "2016-12-01T15:59:05.526467: step 47, loss 0.537826, acc 0.84375\n",
      "2016-12-01T15:59:05.728524: step 48, loss 0.454239, acc 0.8125\n",
      "2016-12-01T15:59:05.909538: step 49, loss 0.342054, acc 0.84375\n",
      "2016-12-01T15:59:06.090060: step 50, loss 0.578438, acc 0.859375\n",
      "2016-12-01T15:59:06.298438: step 51, loss 0.539446, acc 0.78125\n",
      "2016-12-01T15:59:06.529556: step 52, loss 0.352465, acc 0.921875\n",
      "2016-12-01T15:59:06.809590: step 53, loss 1.01329, acc 0.78125\n",
      "2016-12-01T15:59:06.953680: step 54, loss 0.426453, acc 0.890625\n",
      "2016-12-01T15:59:07.092758: step 55, loss 0.333158, acc 0.875\n",
      "2016-12-01T15:59:07.229585: step 56, loss 0.321106, acc 0.875\n",
      "2016-12-01T15:59:07.364943: step 57, loss 0.365603, acc 0.890625\n",
      "2016-12-01T15:59:07.508265: step 58, loss 0.24461, acc 0.953125\n",
      "2016-12-01T15:59:07.649864: step 59, loss 0.421245, acc 0.921875\n",
      "2016-12-01T15:59:07.793451: step 60, loss 0.378951, acc 0.921875\n",
      "2016-12-01T15:59:07.938963: step 61, loss 0.68233, acc 0.859375\n",
      "2016-12-01T15:59:08.075993: step 62, loss 0.0802414, acc 0.96875\n",
      "2016-12-01T15:59:08.211667: step 63, loss 0.361683, acc 0.9375\n",
      "2016-12-01T15:59:08.351971: step 64, loss 0.535511, acc 0.9375\n",
      "2016-12-01T15:59:08.493971: step 65, loss 0.395842, acc 0.9375\n",
      "2016-12-01T15:59:08.625331: step 66, loss 0.583449, acc 0.859375\n",
      "2016-12-01T15:59:08.761551: step 67, loss 0.558301, acc 0.875\n",
      "2016-12-01T15:59:08.914484: step 68, loss 0.745611, acc 0.859375\n",
      "2016-12-01T15:59:09.070990: step 69, loss 0.313639, acc 0.921875\n",
      "2016-12-01T15:59:09.214497: step 70, loss 0.596574, acc 0.859375\n",
      "2016-12-01T15:59:09.368622: step 71, loss 0.408224, acc 0.90625\n",
      "2016-12-01T15:59:09.515383: step 72, loss 0.193854, acc 0.9375\n",
      "2016-12-01T15:59:09.670677: step 73, loss 0.22288, acc 0.9375\n",
      "2016-12-01T15:59:09.829187: step 74, loss 0.442074, acc 0.84375\n",
      "2016-12-01T15:59:10.004423: step 75, loss 0.40835, acc 0.875\n",
      "2016-12-01T15:59:10.142017: step 76, loss 0.848253, acc 0.84375\n",
      "2016-12-01T15:59:10.293868: step 77, loss 0.349902, acc 0.890625\n",
      "2016-12-01T15:59:10.551912: step 78, loss 0.216732, acc 0.953125\n",
      "2016-12-01T15:59:10.687824: step 79, loss 0.537281, acc 0.8125\n",
      "2016-12-01T15:59:10.837612: step 80, loss 0.260699, acc 0.90625\n",
      "2016-12-01T15:59:10.996445: step 81, loss 0.568114, acc 0.859375\n",
      "2016-12-01T15:59:11.141270: step 82, loss 0.395421, acc 0.875\n",
      "2016-12-01T15:59:11.290588: step 83, loss 0.348943, acc 0.890625\n",
      "2016-12-01T15:59:11.434343: step 84, loss 0.169381, acc 0.953125\n",
      "2016-12-01T15:59:11.577561: step 85, loss 0.432204, acc 0.890625\n",
      "2016-12-01T15:59:11.735078: step 86, loss 0.524337, acc 0.875\n",
      "2016-12-01T15:59:11.871000: step 87, loss 0.586139, acc 0.828125\n",
      "2016-12-01T15:59:12.019124: step 88, loss 0.556147, acc 0.796875\n",
      "2016-12-01T15:59:12.155960: step 89, loss 0.447898, acc 0.859375\n",
      "2016-12-01T15:59:12.441440: step 90, loss 0.506662, acc 0.875\n",
      "2016-12-01T15:59:12.587772: step 91, loss 0.537747, acc 0.890625\n",
      "2016-12-01T15:59:12.743270: step 92, loss 0.387281, acc 0.890625\n",
      "2016-12-01T15:59:12.877103: step 93, loss 0.566743, acc 0.828125\n",
      "2016-12-01T15:59:13.031642: step 94, loss 0.242193, acc 0.90625\n",
      "2016-12-01T15:59:13.169640: step 95, loss 0.370943, acc 0.875\n",
      "2016-12-01T15:59:13.325843: step 96, loss 0.390935, acc 0.921875\n",
      "2016-12-01T15:59:13.469885: step 97, loss 0.211912, acc 0.96875\n",
      "2016-12-01T15:59:13.616783: step 98, loss 0.13918, acc 0.953125\n",
      "2016-12-01T15:59:13.763483: step 99, loss 0.38082, acc 0.921875\n",
      "2016-12-01T15:59:13.901670: step 100, loss 0.265686, acc 0.921875\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1480625936/checkpoints/model-100\n",
      "\n",
      "2016-12-01T15:59:14.656361: step 101, loss 0.0555377, acc 0.984375\n",
      "2016-12-01T15:59:14.793153: step 102, loss 0.177406, acc 0.9375\n",
      "2016-12-01T15:59:14.943019: step 103, loss 0.743974, acc 0.859375\n",
      "2016-12-01T15:59:15.096118: step 104, loss 0.70239, acc 0.84375\n",
      "2016-12-01T15:59:15.246608: step 105, loss 0.620509, acc 0.875\n",
      "2016-12-01T15:59:15.398564: step 106, loss 0.323336, acc 0.9375\n",
      "2016-12-01T15:59:15.540160: step 107, loss 0.681416, acc 0.84375\n",
      "2016-12-01T15:59:15.683978: step 108, loss 0.532283, acc 0.859375\n",
      "2016-12-01T15:59:15.820111: step 109, loss 0.790741, acc 0.859375\n",
      "2016-12-01T15:59:15.981172: step 110, loss 0.56748, acc 0.859375\n",
      "2016-12-01T15:59:16.118284: step 111, loss 0.11681, acc 0.953125\n",
      "2016-12-01T15:59:16.276247: step 112, loss 0.318827, acc 0.875\n",
      "2016-12-01T15:59:16.430437: step 113, loss 0.582849, acc 0.84375\n",
      "2016-12-01T15:59:16.584509: step 114, loss 0.215455, acc 0.90625\n",
      "2016-12-01T15:59:16.733303: step 115, loss 0.30567, acc 0.90625\n",
      "2016-12-01T15:59:16.887848: step 116, loss 0.415353, acc 0.84375\n",
      "2016-12-01T15:59:17.025652: step 117, loss 0.366145, acc 0.859375\n",
      "2016-12-01T15:59:17.159872: step 118, loss 0.592151, acc 0.8125\n",
      "2016-12-01T15:59:17.305146: step 119, loss 0.882404, acc 0.765625\n",
      "2016-12-01T15:59:17.472383: step 120, loss 0.879918, acc 0.796875\n",
      "2016-12-01T15:59:17.619813: step 121, loss 0.400057, acc 0.828125\n",
      "2016-12-01T15:59:17.784246: step 122, loss 0.425412, acc 0.890625\n",
      "2016-12-01T15:59:17.923121: step 123, loss 0.343496, acc 0.875\n",
      "2016-12-01T15:59:18.195122: step 124, loss 0.526581, acc 0.859375\n",
      "2016-12-01T15:59:18.334529: step 125, loss 0.360541, acc 0.890625\n",
      "2016-12-01T15:59:18.483070: step 126, loss 0.466439, acc 0.875\n",
      "2016-12-01T15:59:18.641236: step 127, loss 0.47625, acc 0.90625\n",
      "2016-12-01T15:59:18.780373: step 128, loss 0.543185, acc 0.828125\n",
      "2016-12-01T15:59:18.919900: step 129, loss 0.28588, acc 0.890625\n",
      "2016-12-01T15:59:19.079338: step 130, loss 0.395136, acc 0.921875\n",
      "2016-12-01T15:59:19.213440: step 131, loss 0.548888, acc 0.859375\n",
      "2016-12-01T15:59:19.359682: step 132, loss 0.519104, acc 0.875\n",
      "2016-12-01T15:59:19.497423: step 133, loss 0.448468, acc 0.921875\n",
      "2016-12-01T15:59:19.635940: step 134, loss 0.689544, acc 0.78125\n",
      "2016-12-01T15:59:19.778555: step 135, loss 0.39076, acc 0.875\n",
      "2016-12-01T15:59:19.921636: step 136, loss 0.597632, acc 0.84375\n",
      "2016-12-01T15:59:20.073029: step 137, loss 0.306253, acc 0.90625\n",
      "2016-12-01T15:59:20.226228: step 138, loss 0.638616, acc 0.90625\n",
      "2016-12-01T15:59:20.367342: step 139, loss 0.414263, acc 0.859375\n",
      "2016-12-01T15:59:20.516556: step 140, loss 0.670241, acc 0.84375\n",
      "2016-12-01T15:59:20.659054: step 141, loss 0.225642, acc 0.9375\n",
      "2016-12-01T15:59:20.811399: step 142, loss 0.470454, acc 0.875\n",
      "2016-12-01T15:59:20.958574: step 143, loss 0.668016, acc 0.84375\n",
      "2016-12-01T15:59:21.100079: step 144, loss 0.785241, acc 0.875\n",
      "2016-12-01T15:59:21.247715: step 145, loss 0.500533, acc 0.890625\n",
      "2016-12-01T15:59:21.405340: step 146, loss 0.336415, acc 0.921875\n",
      "2016-12-01T15:59:21.558255: step 147, loss 0.468444, acc 0.875\n",
      "2016-12-01T15:59:21.726135: step 148, loss 0.645625, acc 0.875\n",
      "2016-12-01T15:59:21.892181: step 149, loss 0.472003, acc 0.859375\n",
      "2016-12-01T15:59:22.032555: step 150, loss 0.368969, acc 0.859375\n",
      "2016-12-01T15:59:22.184862: step 151, loss 0.392886, acc 0.859375\n",
      "2016-12-01T15:59:22.338250: step 152, loss 0.243562, acc 0.875\n",
      "2016-12-01T15:59:22.484497: step 153, loss 0.799321, acc 0.78125\n",
      "2016-12-01T15:59:22.637693: step 154, loss 0.337673, acc 0.90625\n",
      "2016-12-01T15:59:22.787134: step 155, loss 0.578485, acc 0.8125\n",
      "2016-12-01T15:59:22.932915: step 156, loss 0.280104, acc 0.921875\n",
      "2016-12-01T15:59:23.080032: step 157, loss 0.199296, acc 0.890625\n",
      "2016-12-01T15:59:23.345519: step 158, loss 0.439887, acc 0.890625\n",
      "2016-12-01T15:59:23.567056: step 159, loss 0.330952, acc 0.9375\n",
      "2016-12-01T15:59:23.729046: step 160, loss 0.428654, acc 0.890625\n",
      "2016-12-01T15:59:23.889754: step 161, loss 0.220161, acc 0.890625\n",
      "2016-12-01T15:59:24.100356: step 162, loss 0.206216, acc 0.953125\n",
      "2016-12-01T15:59:24.254836: step 163, loss 0.347778, acc 0.84375\n",
      "2016-12-01T15:59:24.397065: step 164, loss 0.45693, acc 0.921875\n",
      "2016-12-01T15:59:24.562103: step 165, loss 0.401169, acc 0.90625\n",
      "2016-12-01T15:59:24.733262: step 166, loss 0.491033, acc 0.890625\n",
      "2016-12-01T15:59:24.881132: step 167, loss 0.699155, acc 0.875\n",
      "2016-12-01T15:59:25.016882: step 168, loss 0.19181, acc 0.96875\n",
      "2016-12-01T15:59:25.316807: step 169, loss 0.38367, acc 0.890625\n",
      "2016-12-01T15:59:25.511647: step 170, loss 0.429808, acc 0.875\n",
      "2016-12-01T15:59:25.664445: step 171, loss 0.176535, acc 0.96875\n",
      "2016-12-01T15:59:25.811873: step 172, loss 0.966799, acc 0.796875\n",
      "2016-12-01T15:59:25.956981: step 173, loss 0.389749, acc 0.90625\n",
      "2016-12-01T15:59:26.107661: step 174, loss 0.387617, acc 0.90625\n",
      "2016-12-01T15:59:26.319023: step 175, loss 0.197228, acc 0.921875\n",
      "2016-12-01T15:59:26.519650: step 176, loss 0.312054, acc 0.9375\n",
      "2016-12-01T15:59:26.685170: step 177, loss 0.678033, acc 0.875\n",
      "2016-12-01T15:59:26.831956: step 178, loss 0.114123, acc 0.96875\n",
      "2016-12-01T15:59:26.980624: step 179, loss 0.419844, acc 0.90625\n",
      "2016-12-01T15:59:27.130574: step 180, loss 0.375148, acc 0.859375\n",
      "2016-12-01T15:59:27.277514: step 181, loss 0.131987, acc 0.9375\n",
      "2016-12-01T15:59:27.442671: step 182, loss 0.699993, acc 0.828125\n",
      "2016-12-01T15:59:27.714722: step 183, loss 0.444866, acc 0.90625\n",
      "2016-12-01T15:59:27.911885: step 184, loss 0.529276, acc 0.828125\n",
      "2016-12-01T15:59:28.236691: step 185, loss 0.486022, acc 0.875\n",
      "2016-12-01T15:59:28.394680: step 186, loss 0.430193, acc 0.890625\n",
      "2016-12-01T15:59:28.592177: step 187, loss 0.450482, acc 0.875\n",
      "2016-12-01T15:59:28.857612: step 188, loss 0.224353, acc 0.90625\n",
      "2016-12-01T15:59:29.008100: step 189, loss 0.316877, acc 0.890625\n",
      "2016-12-01T15:59:29.200157: step 190, loss 0.591393, acc 0.84375\n",
      "2016-12-01T15:59:29.357322: step 191, loss 0.227823, acc 0.9375\n",
      "2016-12-01T15:59:29.547149: step 192, loss 0.394077, acc 0.875\n",
      "2016-12-01T15:59:29.811766: step 193, loss 0.427242, acc 0.859375\n",
      "2016-12-01T15:59:29.985492: step 194, loss 0.254704, acc 0.875\n",
      "2016-12-01T15:59:30.181401: step 195, loss 0.353177, acc 0.890625\n",
      "2016-12-01T15:59:30.449990: step 196, loss 0.587546, acc 0.890625\n",
      "2016-12-01T15:59:30.651562: step 197, loss 0.333753, acc 0.890625\n",
      "2016-12-01T15:59:30.831208: step 198, loss 0.346457, acc 0.90625\n",
      "2016-12-01T15:59:30.980608: step 199, loss 0.48016, acc 0.875\n",
      "2016-12-01T15:59:31.139735: step 200, loss 0.436545, acc 0.90625\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1480625936/checkpoints/model-200\n",
      "\n",
      "2016-12-01T15:59:31.953698: step 201, loss 0.228557, acc 0.9375\n",
      "2016-12-01T15:59:32.242417: step 202, loss 0.346986, acc 0.890625\n",
      "2016-12-01T15:59:32.418818: step 203, loss 0.364734, acc 0.9375\n",
      "2016-12-01T15:59:32.598070: step 204, loss 0.518291, acc 0.84375\n",
      "2016-12-01T15:59:32.758284: step 205, loss 0.297988, acc 0.9375\n",
      "2016-12-01T15:59:32.946549: step 206, loss 0.707232, acc 0.84375\n",
      "2016-12-01T15:59:33.112110: step 207, loss 0.402297, acc 0.890625\n",
      "2016-12-01T15:59:33.279037: step 208, loss 0.510782, acc 0.859375\n",
      "2016-12-01T15:59:33.548122: step 209, loss 0.390255, acc 0.890625\n",
      "2016-12-01T15:59:33.733209: step 210, loss 0.195495, acc 0.953125\n",
      "2016-12-01T15:59:33.898591: step 211, loss 0.739922, acc 0.828125\n",
      "2016-12-01T15:59:34.051191: step 212, loss 0.336227, acc 0.859375\n",
      "2016-12-01T15:59:34.202879: step 213, loss 0.362089, acc 0.859375\n",
      "2016-12-01T15:59:34.514575: step 214, loss 0.27921, acc 0.90625\n",
      "2016-12-01T15:59:34.744301: step 215, loss 0.59076, acc 0.859375\n",
      "2016-12-01T15:59:35.055067: step 216, loss 0.331904, acc 0.84375\n",
      "2016-12-01T15:59:35.262948: step 217, loss 0.26223, acc 0.890625\n",
      "2016-12-01T15:59:35.446783: step 218, loss 0.476474, acc 0.875\n",
      "2016-12-01T15:59:35.625198: step 219, loss 0.563726, acc 0.859375\n",
      "2016-12-01T15:59:35.771697: step 220, loss 0.450526, acc 0.90625\n",
      "2016-12-01T15:59:36.021852: step 221, loss 0.363953, acc 0.875\n",
      "2016-12-01T15:59:36.163636: step 222, loss 0.254853, acc 0.890625\n",
      "2016-12-01T15:59:36.341143: step 223, loss 0.220363, acc 0.90625\n",
      "2016-12-01T15:59:36.490534: step 224, loss 0.399971, acc 0.890625\n",
      "2016-12-01T15:59:36.669146: step 225, loss 0.565813, acc 0.78125\n",
      "2016-12-01T15:59:36.829774: step 226, loss 0.439309, acc 0.875\n",
      "2016-12-01T15:59:36.973159: step 227, loss 0.196615, acc 0.921875\n",
      "2016-12-01T15:59:37.117757: step 228, loss 0.640118, acc 0.84375\n",
      "2016-12-01T15:59:37.298654: step 229, loss 0.575282, acc 0.859375\n",
      "2016-12-01T15:59:37.446254: step 230, loss 0.771584, acc 0.8125\n",
      "2016-12-01T15:59:37.599469: step 231, loss 0.685485, acc 0.8125\n",
      "2016-12-01T15:59:37.737922: step 232, loss 0.561629, acc 0.875\n",
      "2016-12-01T15:59:37.886625: step 233, loss 0.301527, acc 0.875\n",
      "2016-12-01T15:59:38.024204: step 234, loss 0.55604, acc 0.84375\n",
      "2016-12-01T15:59:38.180394: step 235, loss 0.437485, acc 0.828125\n",
      "2016-12-01T15:59:38.335242: step 236, loss 0.44396, acc 0.828125\n",
      "2016-12-01T15:59:38.487170: step 237, loss 0.358899, acc 0.859375\n",
      "2016-12-01T15:59:38.748597: step 238, loss 0.40667, acc 0.890625\n",
      "2016-12-01T15:59:38.898953: step 239, loss 0.573973, acc 0.8125\n",
      "2016-12-01T15:59:39.058225: step 240, loss 0.21453, acc 0.921875\n",
      "2016-12-01T15:59:39.203763: step 241, loss 0.601629, acc 0.84375\n",
      "2016-12-01T15:59:39.370974: step 242, loss 0.237541, acc 0.921875\n",
      "2016-12-01T15:59:39.508823: step 243, loss 0.331594, acc 0.921875\n",
      "2016-12-01T15:59:39.657773: step 244, loss 0.368983, acc 0.875\n",
      "2016-12-01T15:59:39.814140: step 245, loss 0.325294, acc 0.921875\n",
      "2016-12-01T15:59:39.982910: step 246, loss 0.3661, acc 0.90625\n",
      "2016-12-01T15:59:40.123060: step 247, loss 0.604978, acc 0.859375\n",
      "2016-12-01T15:59:40.288730: step 248, loss 0.32242, acc 0.90625\n",
      "2016-12-01T15:59:40.430441: step 249, loss 0.39991, acc 0.921875\n",
      "2016-12-01T15:59:40.580772: step 250, loss 0.298834, acc 0.875\n",
      "2016-12-01T15:59:40.733345: step 251, loss 0.384404, acc 0.875\n",
      "2016-12-01T15:59:40.885014: step 252, loss 0.419172, acc 0.890625\n",
      "2016-12-01T15:59:41.151833: step 253, loss 0.455329, acc 0.875\n",
      "2016-12-01T15:59:41.295864: step 254, loss 0.308035, acc 0.890625\n",
      "2016-12-01T15:59:41.458036: step 255, loss 0.670523, acc 0.859375\n",
      "2016-12-01T15:59:41.609993: step 256, loss 0.292645, acc 0.890625\n",
      "2016-12-01T15:59:41.867791: step 257, loss 0.202326, acc 0.921875\n",
      "2016-12-01T15:59:42.004459: step 258, loss 0.474202, acc 0.875\n",
      "2016-12-01T15:59:42.184735: step 259, loss 0.19161, acc 0.921875\n",
      "2016-12-01T15:59:42.342856: step 260, loss 0.542508, acc 0.90625\n",
      "2016-12-01T15:59:42.504358: step 261, loss 0.117553, acc 0.953125\n",
      "2016-12-01T15:59:42.649757: step 262, loss 0.250573, acc 0.90625\n",
      "2016-12-01T15:59:42.802830: step 263, loss 0.455271, acc 0.921875\n",
      "2016-12-01T15:59:42.950450: step 264, loss 0.45067, acc 0.875\n",
      "2016-12-01T15:59:43.092520: step 265, loss 0.295985, acc 0.921875\n",
      "2016-12-01T15:59:43.248584: step 266, loss 0.206382, acc 0.921875\n",
      "2016-12-01T15:59:43.507609: step 267, loss 0.417919, acc 0.875\n",
      "2016-12-01T15:59:43.646094: step 268, loss 0.492472, acc 0.875\n",
      "2016-12-01T15:59:43.804177: step 269, loss 0.183889, acc 0.953125\n",
      "2016-12-01T15:59:43.951742: step 270, loss 0.315397, acc 0.90625\n",
      "2016-12-01T15:59:44.112454: step 271, loss 0.224183, acc 0.9375\n",
      "2016-12-01T15:59:44.263921: step 272, loss 0.360272, acc 0.890625\n",
      "2016-12-01T15:59:44.403664: step 273, loss 0.388549, acc 0.890625\n",
      "2016-12-01T15:59:44.562700: step 274, loss 0.437281, acc 0.828125\n",
      "2016-12-01T15:59:44.738190: step 275, loss 0.215586, acc 0.9375\n",
      "2016-12-01T15:59:44.894354: step 276, loss 0.111533, acc 0.953125\n",
      "2016-12-01T15:59:45.079631: step 277, loss 0.558747, acc 0.875\n",
      "2016-12-01T15:59:45.227220: step 278, loss 0.18376, acc 0.953125\n",
      "2016-12-01T15:59:45.387665: step 279, loss 0.276825, acc 0.890625\n",
      "2016-12-01T15:59:45.538986: step 280, loss 0.328536, acc 0.921875\n",
      "2016-12-01T15:59:45.713881: step 281, loss 0.427774, acc 0.875\n",
      "2016-12-01T15:59:45.853591: step 282, loss 0.281142, acc 0.921875\n",
      "2016-12-01T15:59:46.012353: step 283, loss 0.298859, acc 0.890625\n",
      "2016-12-01T15:59:46.168163: step 284, loss 0.627041, acc 0.84375\n",
      "2016-12-01T15:59:46.335211: step 285, loss 0.416739, acc 0.875\n",
      "2016-12-01T15:59:46.595656: step 286, loss 0.499986, acc 0.875\n",
      "2016-12-01T15:59:46.754064: step 287, loss 0.324493, acc 0.875\n",
      "2016-12-01T15:59:46.920940: step 288, loss 0.251147, acc 0.921875\n",
      "2016-12-01T15:59:47.106811: step 289, loss 0.369048, acc 0.890625\n",
      "2016-12-01T15:59:47.304032: step 290, loss 0.14238, acc 0.96875\n",
      "2016-12-01T15:59:47.455823: step 291, loss 0.218685, acc 0.921875\n",
      "2016-12-01T15:59:47.618781: step 292, loss 0.246841, acc 0.921875\n",
      "2016-12-01T15:59:47.788521: step 293, loss 0.177111, acc 0.921875\n",
      "2016-12-01T15:59:47.969247: step 294, loss 0.583147, acc 0.84375\n",
      "2016-12-01T15:59:48.139310: step 295, loss 0.152176, acc 0.9375\n",
      "2016-12-01T15:59:48.303813: step 296, loss 0.464626, acc 0.890625\n",
      "2016-12-01T15:59:48.453027: step 297, loss 0.493772, acc 0.875\n",
      "2016-12-01T15:59:48.618431: step 298, loss 0.289502, acc 0.90625\n",
      "2016-12-01T15:59:48.761896: step 299, loss 0.335946, acc 0.859375\n",
      "2016-12-01T15:59:48.906786: step 300, loss 0.196252, acc 0.9375\n",
      "Saved model checkpoint to /Users/tcwong/Documents/NLP-project/6.864-project/runs/1480625936/checkpoints/model-300\n",
      "\n",
      "2016-12-01T15:59:49.659439: step 301, loss 0.576301, acc 0.890625\n",
      "2016-12-01T15:59:49.826428: step 302, loss 0.548624, acc 0.859375\n",
      "2016-12-01T15:59:49.975825: step 303, loss 0.350864, acc 0.921875\n",
      "2016-12-01T15:59:50.124452: step 304, loss 0.275917, acc 0.90625\n",
      "2016-12-01T15:59:50.321561: step 305, loss 0.374321, acc 0.90625\n",
      "2016-12-01T15:59:50.657206: step 306, loss 0.252618, acc 0.921875\n",
      "2016-12-01T15:59:50.865692: step 307, loss 0.538154, acc 0.859375\n",
      "2016-12-01T15:59:51.083349: step 308, loss 0.503748, acc 0.8125\n",
      "2016-12-01T15:59:51.295567: step 309, loss 0.233083, acc 0.953125\n",
      "2016-12-01T15:59:51.487509: step 310, loss 0.47171, acc 0.875\n",
      "2016-12-01T15:59:51.711152: step 311, loss 0.381049, acc 0.859375\n",
      "2016-12-01T15:59:52.013910: step 312, loss 0.365125, acc 0.890625\n",
      "2016-12-01T15:59:52.197121: step 313, loss 0.44202, acc 0.859375\n",
      "2016-12-01T15:59:52.488314: step 314, loss 0.395628, acc 0.875\n",
      "2016-12-01T15:59:52.671111: step 315, loss 0.20108, acc 0.9375\n",
      "2016-12-01T15:59:52.866410: step 316, loss 0.371647, acc 0.890625\n",
      "2016-12-01T15:59:53.066446: step 317, loss 0.527769, acc 0.859375\n",
      "2016-12-01T15:59:53.389160: step 318, loss 0.306804, acc 0.84375\n",
      "2016-12-01T15:59:53.562869: step 319, loss 0.38798, acc 0.84375\n",
      "2016-12-01T15:59:53.733059: step 320, loss 0.42159, acc 0.875\n",
      "2016-12-01T15:59:53.905102: step 321, loss 0.505673, acc 0.859375\n",
      "2016-12-01T15:59:54.238983: step 322, loss 0.54113, acc 0.875\n",
      "2016-12-01T15:59:54.399370: step 323, loss 0.124919, acc 0.953125\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x.shape[1],\n",
    "            num_classes=y.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss) # TODO check cnn.loss\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "#         Write vocabulary\n",
    "#         vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            # TODO: uncomment and add scores\n",
    "#             _, step, summaries, loss, accuracy, scores, predictions, temp, extracted, truth, correct, gold, precision, recall, f1 = sess.run(\n",
    "#                 [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions, cnn.temp, cnn.extracted, cnn.truth, cnn.correct, cnn.gold, cnn.precision, cnn.recall, cnn.f1],feed_dict)\n",
    "           # remove below afterwards  \n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "        \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            # TODO UNCOMMENT BELOW\n",
    "#             print(\"{}: step {}, loss {:g}, pre {:g}, rec {:g}, f1 {:g}\".format(time_str, step, loss, precision, recall, f1))\n",
    "            #temp\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "#             print \"type of scores: \", type(scores)\n",
    "#             print type(scores[0])\n",
    "#             print \"scores: \\n\", scores[0]\n",
    "#             print \" \"\n",
    "#             print \"type of predictions: \", type(predictions)\n",
    "#             print \"predictions: \\n\", predictions\n",
    "#             print type(temp)\n",
    "#             print \"temp: \\n\", temp[0]\n",
    "#             print \"gold: \\n\", gold[0]\n",
    "#             print \"extracted: \", extracted\n",
    "#             print \"extracted python: \", sum([sum(score) for score in scores])\n",
    "#             print \"truth: \", truth\n",
    "#             print \"truth python: \", sum([sum(x) for x in gold])\n",
    "#             print \"correct: \", correct\n",
    "#             print \"correct python: \", sum([sum(x) for x in temp])\n",
    "#             print \"precision: \", precision\n",
    "#             print \"recall: \", recall\n",
    "#             print \"f1: \", f1\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy, scores, predictions = sess.run(\n",
    "    \n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.scores, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x, y)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "#             if current_step % FLAGS.evaluate_every == 0:\n",
    "#                 print(\"\\nEvaluation:\")\n",
    "#                 dev_step(x_dev, y_dev, writer=dev_summary_writer) # from x_dev \n",
    "#                 print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
